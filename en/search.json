[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the SSP Cloud!",
    "section": "",
    "text": "An open instance of the Onyxia project, the SSP Cloud is a shared data processing platform. This “Datalab” is dedicated to experimenting with datascience methods based on open data. In this tutorial, we’ll take you on a guided tour of the Datalab, to help you get up and running quickly.\n\n\n\n\n\n\nWarning\n\n\n\nThe terms of use for the SSP Cloud can be found at this address. We remind you that the SSP Cloud is intended exclusively for the processing of public and non-sensitive data. Projects involving non-open data for experimentation can be carried out in consultation with the Onyxia team, subject to compliance with project-specific security rules.\n\n\n\n\nThe service catalog is at the core of the SSP Cloud’s usage. It offers a set of services for statistical data processing and complete management of data science projects.\n\n\n\nTo launch a service, simply click on the Launch button of the desired service.\nA page centered on the requested service will then open, offering several options:\n\nClick again on the Launch button to start the service with its default configuration.\nCustomize the name that the instance will have once the service is launched.\nUnfold a configuration menu to customize the service’s configuration before launching it.\nSave a customized configuration by clicking on the bookmark at the top right of the service.\n\n\nThe detailed configuration of services is an advanced usage and is not covered in this tutorial but in other pages of this documentation site.\n\n\n\n\nLaunching a service automatically takes you to the My Services page, where all active instances on the user’s account are listed.\n\nOnce the service is launched, an Open button appears, allowing access to the service. A password — and, depending on the services, a username — is generally required to use the service. This information is available in the service’s README, which can be accessed by clicking on the button of the same name.\n\n\n\nDeleting an instance of a service is done simply by clicking on the trash can icon below the instance.\n\n\n\n\n\n\nCaution\n\n\n\nFor some services, deleting an instance will also delete all associated data, and this action is irreversible. Therefore, it is essential to always read the README associated with the instance, which specifies the consequences of deleting the instance. In general, it is crucial to ensure that the data and code used are backed up before deleting the instance. Ideally, version your code with Git and regularly back up the data using MinIO (S3 storage system).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe resources made available for executing services are shared among different SSP Cloud users. Please make sure not to leave active services that you no longer use. We sometimes systematically remove inactive instances after a certain period to free up resources.\n\n\n\n\n\n\nSupport and assistance for using the SSP Cloud are provided through two communication channels:\n\nOn the Onyxia Slack in the #sspcloud channel. Any questions about using the SSP Cloud or suggestions for improvement are welcome there.\nIn a dedicated room of the French state inter-ministerial instant messaging service Tchap for French public agents."
  },
  {
    "objectID": "index.html#service-catalog",
    "href": "index.html#service-catalog",
    "title": "Welcome to the SSP Cloud!",
    "section": "",
    "text": "The service catalog is at the core of the SSP Cloud’s usage. It offers a set of services for statistical data processing and complete management of data science projects.\n\n\n\nTo launch a service, simply click on the Launch button of the desired service.\nA page centered on the requested service will then open, offering several options:\n\nClick again on the Launch button to start the service with its default configuration.\nCustomize the name that the instance will have once the service is launched.\nUnfold a configuration menu to customize the service’s configuration before launching it.\nSave a customized configuration by clicking on the bookmark at the top right of the service.\n\n\nThe detailed configuration of services is an advanced usage and is not covered in this tutorial but in other pages of this documentation site.\n\n\n\n\nLaunching a service automatically takes you to the My Services page, where all active instances on the user’s account are listed.\n\nOnce the service is launched, an Open button appears, allowing access to the service. A password — and, depending on the services, a username — is generally required to use the service. This information is available in the service’s README, which can be accessed by clicking on the button of the same name.\n\n\n\nDeleting an instance of a service is done simply by clicking on the trash can icon below the instance.\n\n\n\n\n\n\nCaution\n\n\n\nFor some services, deleting an instance will also delete all associated data, and this action is irreversible. Therefore, it is essential to always read the README associated with the instance, which specifies the consequences of deleting the instance. In general, it is crucial to ensure that the data and code used are backed up before deleting the instance. Ideally, version your code with Git and regularly back up the data using MinIO (S3 storage system).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe resources made available for executing services are shared among different SSP Cloud users. Please make sure not to leave active services that you no longer use. We sometimes systematically remove inactive instances after a certain period to free up resources."
  },
  {
    "objectID": "index.html#support-1",
    "href": "index.html#support-1",
    "title": "Welcome to the SSP Cloud!",
    "section": "",
    "text": "Support and assistance for using the SSP Cloud are provided through two communication channels:\n\nOn the Onyxia Slack in the #sspcloud channel. Any questions about using the SSP Cloud or suggestions for improvement are welcome there.\nIn a dedicated room of the French state inter-ministerial instant messaging service Tchap for French public agents."
  },
  {
    "objectID": "content/storage.html",
    "href": "content/storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "The file storage solution associated with Datalab is MinIO, an object storage system based on the cloud, compatible with Amazon’s S3 API. In practice, this has several advantages:\n\nStored files are easily accessible from anywhere: a file can be accessed directly via a simple URL, which can be shared.\nIt is possible to access the stored files directly within the data science services (R, Python, etc.) offered on Datalab, without the need to copy the files locally beforehand, greatly improving the reproducibility of analyses.\n\n\n\n\nMinIO Schema\n\n\n\n\n\n\n\nThe My Files page in Datalab takes the form of a file explorer showing the different buckets (repositories) to which the user has access.\nEach user has a personal bucket by default to store their files. Within this bucket, two options are possible:\n\n“Create a directory”: Creates a directory in the current bucket/directory hierarchically, similar to a traditional file system.\n“Upload a file”: Uploads one or multiple files to the current directory.\n\n\n\n\n\n\n\nNote\n\n\n\nThe graphical interface for data storage in Datalab is still under construction. As such, it may experience responsiveness issues. For frequent operations on file storage, it may be preferable to interact with MinIO via the terminal.\n\n\n\n\n\nBy clicking on a file in their personal bucket, the user can access its characteristics page. On this page, it is possible to change the distribution status of the file. Changing the status of the file from “private” to “public” generates a distribution link, which can then be shared for downloading the file. The “public” status only grants read rights to other users, and modifying or deleting other users’ personal files is not possible.\nTo simplify the sharing of multiple files for, say, a training session, it is possible to create a “distribution” folder in the user’s personal bucket. By default, all files present in this folder have a public distribution status.\n\n\n\n\n\n\nNote\n\n\n\nFor collaborative projects, it can be beneficial for different participants to have access to a shared storage space. It is possible to create shared buckets on MinIO for this purpose. Feel free to contact us via the channels specified on the “First Use” page if you wish to work on open data projects on Datalab.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn accordance with the terms of use, only open data-type or non-sensitive data can be stored on Datalab. Having a file with a “private” distribution status does not guarantee perfect confidentiality.\n\n\n\n\n\n\nThe access credentials needed to access data on MinIO are pre-configured in the various Datalab services, accessible in the form of environment variables. This greatly facilitates importing and exporting files from the services.\n\n\n\nRPythonmc\n\n\nIn R, interaction with an S3-compatible file system is made possible by the aws.s3 library.\nlibrary(aws.s3)\n\n\nIn Python, interaction with an S3-compatible file system is made possible by two libraries:\n\nBoto3, a library created and maintained by Amazon.\nS3Fs, a library that allows interaction with stored files similar to a classic filesystem.\n\nFor this reason, and because S3Fs is used by default by the pandas library to manage S3 connections, we will present how to manage storage on MinIO via Python using this library.\nimport os\nimport s3fs\n\n# Create filesystem object\nS3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n\n\nMinIO offers a command-line client (mc) that allows interaction with the storage system in a manner similar to a classic UNIX filesystem. This client is installed by default and accessible via a terminal in the various Datalab services.\nThe MinIO client offers basic UNIX commands such as ls, cat, cp, etc. The complete list is available in the client documentation.\n\n\n\n\n\n\n\nRPythonmc\n\n\naws.s3::get_bucket(\"donnees-insee\", region = \"\")\n\n\nfs.ls(\"donnees-insee\")\n\n\nThe Datalab storage is accessible via the alias s3. For example, to list the files in the bucket donnees-insee:\nmc ls s3/donnees-insee\n\n\n\n\n\n\n\nRPythonmc\n\n\nBUCKET &lt;- \"donnees-insee\"\nFILE_KEY_S3 &lt;- \"diffusion/BPE/2019/BPE_ENS.csv\"\n\ndf &lt;- \n  aws.s3::s3read_using(\n    FUN = readr::read_delim,\n    # Put FUN options here\n    delim = \";\",\n    object = FILE_KEY_S3,\n    bucket = BUCKET,\n    opts = list(\"region\" = \"\")\n  )\n\n\nThe S3Fs package allows you to interact with files stored on MinIO as if they were local files. The syntax is therefore very familiar to Python users. For example, to import/export tabular data via pandas:\nimport pandas as pd\n\nBUCKET = \"donnees-insee\"\nFILE_KEY_S3 = \"diffusion/BPE/2019/BPE_ENS.csv\"\nFILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n\nwith fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n    df_bpe = pd.read_csv(file_in, sep=\";\")\n\n\nTo copy data from a MinIO bucket to the local service:\nmc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv\n\n\n\n\n\n\nWarning\n\n\n\nCopying files to the local service is generally not a good practice: it limits the reproducibility of analyses and becomes quickly impossible with large volumes of data. Therefore, it is preferable to get into the habit of importing data directly into R/Python.\n\n\n\n\n\n\n\n\n\nRPythonmc\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\n\naws.s3::s3write_using(\n    df,\n    FUN = readr::write_csv,\n    object = FILE_KEY_OUT_S3,\n    bucket = BUCKET_OUT,\n    opts = list(\"region\" = \"\")\n)\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\nFILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n\nwith fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n    df_bpe.to_csv(file_out)\n\n\nTo copy data from the local service to a bucket on MinIO:\nmc cp local/path/to/my/file.csv s3/&lt;my_bucket&gt;/remote/path/to/my/file.csv\n\n\n\n\n\n\nAccess to MinIO storage is possible via a personal access token, which is valid for 7 days and automatically regenerated at regular intervals on SSP Cloud. When a token has expired, services created before the expiration date (using the previous token) can no longer access storage, and the affected service will appear in red on the My Services page. In this case, there are two options:\n\nOpen a new service on Datalab, which will have a default, up-to-date token.\nManually replace expired tokens with new ones. Scripts indicating how to do this for different MinIO uses (R/Python/mc) are available here. Simply choose the relevant script and execute it in your current working environment.\n\n\n\n\n\n\n\nFor security reasons, the authentication to MinIO used by default in the interactive services of the SSP Cloud relies on a temporary access token. In the context of projects involving periodic processing or the deployment of applications, a more permanent access to MinIO data may be required.\nIn this case, a service account is used, which is an account tied to a specific project or application rather than an individual. Technically, instead of authenticating to MinIO via a triplet (access key id, secret access key, and session token), a pair (access key id, secret access key) will be used, granting read/write permissions to a specific project bucket.\nThe procedure for creating a service account is described below.\n\nGraphical InterfaceTerminal (mc)\n\n\n\nOpen the MinIO console\nOpen the Access Keys tab\nThe service account information is pre-generated. It is possible to modify the access key to give it a simpler name.\nThe policy specifying the rights is also pre-generated. Ideally, the policy should be restricted to only cover the project bucket(s).\nOnce the service account is generated, the access key and secret access key can be used to authenticate the services/applications to the specified bucket.\n\n\n\n\nCreate a service on the SSP Cloud with up-to-date MinIO access. Confirm that the connection works with:\n\nmc ls s3/&lt;username&gt;\n\nGenerate a policy.json file with the following content, replacing project-&lt;my_project&gt; with the name of the relevant bucket (twice):\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n     {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n       \"s3:*\"\n      ],\n      \"Resource\": [\n       \"arn:aws:s3:::projet-&lt;my_project&gt;\",\n       \"arn:aws:s3:::projet-&lt;my_project&gt;/*\"\n      ]\n     }\n    ]\n  }\n\nIn a terminal, generate the service account with the following command:\n\nmc admin user svcacct add s3 $AWS_ACCESS_KEY_ID --access-key=\"&lt;access-key&gt;\" --secret-key=\"&lt;secret-key&gt;\" --policy=\"policy.json\"\nreplacing &lt;access-key&gt; and &lt;secret-key&gt; with names of your choice. Ideally, give a simple name for the access key (e.g., sa-project-projectname) but a complex key for the secret access key, which can be generated, for example, with the gpg client:\ngpg --gen-random --armor 1 16\n\nYou can now use the access key and secret access key to authenticate the services/applications to the specified bucket\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that the generated authentication information appears only once. They can then be stored in a password manager, a secret storage service like Vault, or via the Onyxia project settings feature, which allows importing the service account directly into services at the time of their configuration."
  },
  {
    "objectID": "content/storage.html#principles",
    "href": "content/storage.html#principles",
    "title": "Data Storage",
    "section": "",
    "text": "The file storage solution associated with Datalab is MinIO, an object storage system based on the cloud, compatible with Amazon’s S3 API. In practice, this has several advantages:\n\nStored files are easily accessible from anywhere: a file can be accessed directly via a simple URL, which can be shared.\nIt is possible to access the stored files directly within the data science services (R, Python, etc.) offered on Datalab, without the need to copy the files locally beforehand, greatly improving the reproducibility of analyses.\n\n\n\n\nMinIO Schema"
  },
  {
    "objectID": "content/storage.html#managing-your-data",
    "href": "content/storage.html#managing-your-data",
    "title": "Data Storage",
    "section": "",
    "text": "The My Files page in Datalab takes the form of a file explorer showing the different buckets (repositories) to which the user has access.\nEach user has a personal bucket by default to store their files. Within this bucket, two options are possible:\n\n“Create a directory”: Creates a directory in the current bucket/directory hierarchically, similar to a traditional file system.\n“Upload a file”: Uploads one or multiple files to the current directory.\n\n\n\n\n\n\n\nNote\n\n\n\nThe graphical interface for data storage in Datalab is still under construction. As such, it may experience responsiveness issues. For frequent operations on file storage, it may be preferable to interact with MinIO via the terminal.\n\n\n\n\n\nBy clicking on a file in their personal bucket, the user can access its characteristics page. On this page, it is possible to change the distribution status of the file. Changing the status of the file from “private” to “public” generates a distribution link, which can then be shared for downloading the file. The “public” status only grants read rights to other users, and modifying or deleting other users’ personal files is not possible.\nTo simplify the sharing of multiple files for, say, a training session, it is possible to create a “distribution” folder in the user’s personal bucket. By default, all files present in this folder have a public distribution status.\n\n\n\n\n\n\nNote\n\n\n\nFor collaborative projects, it can be beneficial for different participants to have access to a shared storage space. It is possible to create shared buckets on MinIO for this purpose. Feel free to contact us via the channels specified on the “First Use” page if you wish to work on open data projects on Datalab.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn accordance with the terms of use, only open data-type or non-sensitive data can be stored on Datalab. Having a file with a “private” distribution status does not guarantee perfect confidentiality."
  },
  {
    "objectID": "content/storage.html#using-data-stored-on-minio",
    "href": "content/storage.html#using-data-stored-on-minio",
    "title": "Data Storage",
    "section": "",
    "text": "The access credentials needed to access data on MinIO are pre-configured in the various Datalab services, accessible in the form of environment variables. This greatly facilitates importing and exporting files from the services.\n\n\n\nRPythonmc\n\n\nIn R, interaction with an S3-compatible file system is made possible by the aws.s3 library.\nlibrary(aws.s3)\n\n\nIn Python, interaction with an S3-compatible file system is made possible by two libraries:\n\nBoto3, a library created and maintained by Amazon.\nS3Fs, a library that allows interaction with stored files similar to a classic filesystem.\n\nFor this reason, and because S3Fs is used by default by the pandas library to manage S3 connections, we will present how to manage storage on MinIO via Python using this library.\nimport os\nimport s3fs\n\n# Create filesystem object\nS3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n\n\nMinIO offers a command-line client (mc) that allows interaction with the storage system in a manner similar to a classic UNIX filesystem. This client is installed by default and accessible via a terminal in the various Datalab services.\nThe MinIO client offers basic UNIX commands such as ls, cat, cp, etc. The complete list is available in the client documentation.\n\n\n\n\n\n\n\nRPythonmc\n\n\naws.s3::get_bucket(\"donnees-insee\", region = \"\")\n\n\nfs.ls(\"donnees-insee\")\n\n\nThe Datalab storage is accessible via the alias s3. For example, to list the files in the bucket donnees-insee:\nmc ls s3/donnees-insee\n\n\n\n\n\n\n\nRPythonmc\n\n\nBUCKET &lt;- \"donnees-insee\"\nFILE_KEY_S3 &lt;- \"diffusion/BPE/2019/BPE_ENS.csv\"\n\ndf &lt;- \n  aws.s3::s3read_using(\n    FUN = readr::read_delim,\n    # Put FUN options here\n    delim = \";\",\n    object = FILE_KEY_S3,\n    bucket = BUCKET,\n    opts = list(\"region\" = \"\")\n  )\n\n\nThe S3Fs package allows you to interact with files stored on MinIO as if they were local files. The syntax is therefore very familiar to Python users. For example, to import/export tabular data via pandas:\nimport pandas as pd\n\nBUCKET = \"donnees-insee\"\nFILE_KEY_S3 = \"diffusion/BPE/2019/BPE_ENS.csv\"\nFILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n\nwith fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n    df_bpe = pd.read_csv(file_in, sep=\";\")\n\n\nTo copy data from a MinIO bucket to the local service:\nmc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv\n\n\n\n\n\n\nWarning\n\n\n\nCopying files to the local service is generally not a good practice: it limits the reproducibility of analyses and becomes quickly impossible with large volumes of data. Therefore, it is preferable to get into the habit of importing data directly into R/Python.\n\n\n\n\n\n\n\n\n\nRPythonmc\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\n\naws.s3::s3write_using(\n    df,\n    FUN = readr::write_csv,\n    object = FILE_KEY_OUT_S3,\n    bucket = BUCKET_OUT,\n    opts = list(\"region\" = \"\")\n)\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\nFILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n\nwith fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n    df_bpe.to_csv(file_out)\n\n\nTo copy data from the local service to a bucket on MinIO:\nmc cp local/path/to/my/file.csv s3/&lt;my_bucket&gt;/remote/path/to/my/file.csv\n\n\n\n\n\n\nAccess to MinIO storage is possible via a personal access token, which is valid for 7 days and automatically regenerated at regular intervals on SSP Cloud. When a token has expired, services created before the expiration date (using the previous token) can no longer access storage, and the affected service will appear in red on the My Services page. In this case, there are two options:\n\nOpen a new service on Datalab, which will have a default, up-to-date token.\nManually replace expired tokens with new ones. Scripts indicating how to do this for different MinIO uses (R/Python/mc) are available here. Simply choose the relevant script and execute it in your current working environment."
  },
  {
    "objectID": "content/storage.html#advanced-usage",
    "href": "content/storage.html#advanced-usage",
    "title": "Data Storage",
    "section": "",
    "text": "For security reasons, the authentication to MinIO used by default in the interactive services of the SSP Cloud relies on a temporary access token. In the context of projects involving periodic processing or the deployment of applications, a more permanent access to MinIO data may be required.\nIn this case, a service account is used, which is an account tied to a specific project or application rather than an individual. Technically, instead of authenticating to MinIO via a triplet (access key id, secret access key, and session token), a pair (access key id, secret access key) will be used, granting read/write permissions to a specific project bucket.\nThe procedure for creating a service account is described below.\n\nGraphical InterfaceTerminal (mc)\n\n\n\nOpen the MinIO console\nOpen the Access Keys tab\nThe service account information is pre-generated. It is possible to modify the access key to give it a simpler name.\nThe policy specifying the rights is also pre-generated. Ideally, the policy should be restricted to only cover the project bucket(s).\nOnce the service account is generated, the access key and secret access key can be used to authenticate the services/applications to the specified bucket.\n\n\n\n\nCreate a service on the SSP Cloud with up-to-date MinIO access. Confirm that the connection works with:\n\nmc ls s3/&lt;username&gt;\n\nGenerate a policy.json file with the following content, replacing project-&lt;my_project&gt; with the name of the relevant bucket (twice):\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n     {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n       \"s3:*\"\n      ],\n      \"Resource\": [\n       \"arn:aws:s3:::projet-&lt;my_project&gt;\",\n       \"arn:aws:s3:::projet-&lt;my_project&gt;/*\"\n      ]\n     }\n    ]\n  }\n\nIn a terminal, generate the service account with the following command:\n\nmc admin user svcacct add s3 $AWS_ACCESS_KEY_ID --access-key=\"&lt;access-key&gt;\" --secret-key=\"&lt;secret-key&gt;\" --policy=\"policy.json\"\nreplacing &lt;access-key&gt; and &lt;secret-key&gt; with names of your choice. Ideally, give a simple name for the access key (e.g., sa-project-projectname) but a complex key for the secret access key, which can be generated, for example, with the gpg client:\ngpg --gen-random --armor 1 16\n\nYou can now use the access key and secret access key to authenticate the services/applications to the specified bucket\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that the generated authentication information appears only once. They can then be stored in a password manager, a secret storage service like Vault, or via the Onyxia project settings feature, which allows importing the service account directly into services at the time of their configuration."
  },
  {
    "objectID": "content/secrets.html",
    "href": "content/secrets.html",
    "title": "Secrets Management",
    "section": "",
    "text": "Secrets Management\n\nEnvironment Variables\nSometimes, certain pieces of information need to be made available to a large number of applications, or they should not be directly embedded in your code (access tokens, passwords, etc.). The use of environment variables allows accessing this information from any service.\nWhen a service is launched, several environment variables are automatically injected—such as access tokens for GitHub and MinIO.\n\n\n\nCreation and Management of Secrets\nOn the platform, environment variables are treated as secrets stored in Vault (the Datalab’s safe) and are encrypted. This enables you to store tokens, credentials, and passwords securely. The My Secrets page is designed like a file explorer, allowing you to sort and organize your variables into folders.\n\nGetting Started:\n\nCreate a new folder with + New folder.\nThen, within this folder, create a new secret with + New secret.\nOpen your secret.\n\n\nEach secret can contain multiple variables, consisting of key-value pairs.\n\n+ Add a variable\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe keys (variable names) always begin with $ and contain only letters, numbers, and the underscore character (_). By convention, keys are written in UPPERCASE.\n\n\nFill in the name of the key and its value.\n\n\n\nConverting Secrets into Environment Variables\nOnce your secret is edited, along with its different variables, you are ready to use it in your service.\n\nCopy the secret’s path by clicking on the Use in a service button.\nThen, during the configuration of your service, go to the Vault tab and paste the secret’s path in the dedicated field.\n\n\n\nCreate and open your service.\n\nTo verify that your environment variables have been successfully created, you can run the following commands in the service terminal:\n# List all available environment variables\nenv\n\n# Display the value of an environment variable\necho $MY_VARIABLE\n\n# Find all environment variables containing a given pattern\nenv | grep -i \"&lt;PATTERN&gt;\""
  },
  {
    "objectID": "content/principles.html",
    "href": "content/principles.html",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Onyxia project stems from the recognition of common difficulties faced by data scientists in the public sector:\n\nOften isolated agents, due to the relative scarcity of data skills in administration.\nInadequate infrastructures in terms of resources and technologies, which hinder innovation.\nDifficulty transitioning from experimentation to production due to multiple separations (physical separation, development languages, work methods) between business units and IT production.\n\nIn response to these challenges, Datalab SSP Cloud was built to offer a collaborative platform at multiple levels:\n\nSharing a modern infrastructure centered around deploying services through containers, specifically designed for data science applications.\nSharing methods through the mutualization of data science services offered, to which everyone can contribute.\nSharing knowledge through Datalab-related training and the formation of user communities focused on its utilization.\n\n\n\n\n\n\n\nNote\n\n\n\nOnyxia, Datalab SSP Cloud: What are the differences?\nOnyxia is an open-source project that provides a web application for accessing data science services. The Datalab SSP Cloud is an instance of the Onyxia project hosted at Insee.\n\n\n\n\n\nThe architecture of Datalab is based on a set of fundamental principles:\n\nData science-oriented production by providing an infrastructure suitable for most use cases and a service catalog covering the entire data project lifecycle.\nChoices that promote user autonomy by avoiding proprietary lock-ins and enabling access to the lower layers of the infrastructure to address advanced and specific needs.\nA project that is 100% cloud-native and also cloud-agnostic, allowing for easy deployment on any infrastructure.\nA completely open-source project, both in terms of its constituent components and its distribution (MIT license).\n\n\n\n\nDatalab can be accessed through a modern and responsive user interface, focused on providing a great user experience. It serves as the technical connection between the different components of Onyxia:\n\nOpen-source technologies that represent the state of the art in container deployment and orchestration, storage, and security.\nA service and tool catalog to support data science projects.\nA training and documentation platform to facilitate onboarding onto the offered technologies.\n\n\n\n\nFundamental building blocks of Datalab Onyxia\n\n\nThe service catalog is designed to accommodate the essential needs of data scientists, from self-service development to production deployment of processes or applications. The entire data project lifecycle is covered, and the catalog of services is regularly expanded to meet users’ new requirements.\n\n\n\nA comprehensive service catalog for data science projects\n\n\n\n\n\nThe Datalab Onyxia project is resolutely open at multiple levels:\n\nThe Datalab is accessible through its web interface to all public service agents (via AgentConnect or a gouv.fr email address) as well as students from statistical schools linked to Insee (Cefil, Ensai, Ensae).\nThe open-source code and modularity of the project allow for the deployment of a customized Onyxia instance on any Kubernetes-based infrastructure cluster.\nThe project is open to external contributions, whether related to the service catalog, the graphical interface, or the arrangement of the software components it comprises."
  },
  {
    "objectID": "content/principles.html#a-platform-for-collaboration",
    "href": "content/principles.html#a-platform-for-collaboration",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Onyxia project stems from the recognition of common difficulties faced by data scientists in the public sector:\n\nOften isolated agents, due to the relative scarcity of data skills in administration.\nInadequate infrastructures in terms of resources and technologies, which hinder innovation.\nDifficulty transitioning from experimentation to production due to multiple separations (physical separation, development languages, work methods) between business units and IT production.\n\nIn response to these challenges, Datalab SSP Cloud was built to offer a collaborative platform at multiple levels:\n\nSharing a modern infrastructure centered around deploying services through containers, specifically designed for data science applications.\nSharing methods through the mutualization of data science services offered, to which everyone can contribute.\nSharing knowledge through Datalab-related training and the formation of user communities focused on its utilization.\n\n\n\n\n\n\n\nNote\n\n\n\nOnyxia, Datalab SSP Cloud: What are the differences?\nOnyxia is an open-source project that provides a web application for accessing data science services. The Datalab SSP Cloud is an instance of the Onyxia project hosted at Insee."
  },
  {
    "objectID": "content/principles.html#fundamental-principles",
    "href": "content/principles.html#fundamental-principles",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The architecture of Datalab is based on a set of fundamental principles:\n\nData science-oriented production by providing an infrastructure suitable for most use cases and a service catalog covering the entire data project lifecycle.\nChoices that promote user autonomy by avoiding proprietary lock-ins and enabling access to the lower layers of the infrastructure to address advanced and specific needs.\nA project that is 100% cloud-native and also cloud-agnostic, allowing for easy deployment on any infrastructure.\nA completely open-source project, both in terms of its constituent components and its distribution (MIT license)."
  },
  {
    "objectID": "content/principles.html#service-offering",
    "href": "content/principles.html#service-offering",
    "title": "Principles of Datalab",
    "section": "",
    "text": "Datalab can be accessed through a modern and responsive user interface, focused on providing a great user experience. It serves as the technical connection between the different components of Onyxia:\n\nOpen-source technologies that represent the state of the art in container deployment and orchestration, storage, and security.\nA service and tool catalog to support data science projects.\nA training and documentation platform to facilitate onboarding onto the offered technologies.\n\n\n\n\nFundamental building blocks of Datalab Onyxia\n\n\nThe service catalog is designed to accommodate the essential needs of data scientists, from self-service development to production deployment of processes or applications. The entire data project lifecycle is covered, and the catalog of services is regularly expanded to meet users’ new requirements.\n\n\n\nA comprehensive service catalog for data science projects"
  },
  {
    "objectID": "content/principles.html#an-open-project",
    "href": "content/principles.html#an-open-project",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Datalab Onyxia project is resolutely open at multiple levels:\n\nThe Datalab is accessible through its web interface to all public service agents (via AgentConnect or a gouv.fr email address) as well as students from statistical schools linked to Insee (Cefil, Ensai, Ensae).\nThe open-source code and modularity of the project allow for the deployment of a customized Onyxia instance on any Kubernetes-based infrastructure cluster.\nThe project is open to external contributions, whether related to the service catalog, the graphical interface, or the arrangement of the software components it comprises."
  },
  {
    "objectID": "content/services-configuration.html",
    "href": "content/services-configuration.html",
    "title": "Service Configuration",
    "section": "",
    "text": "After clicking on “New service” &gt; “RStudio/Jupyter-python/VScode-python” &gt; “Launch”\n\n\nTo recognize the service and/or the configuration if saved by clicking on the bookmark symbol at the top right. If the name already exists among the saved configurations, saving will overwrite the old configuration.\nConvenient for distinguishing different services of the same type (RStudio, Jupyter…).\n\n\n\nIt is possible to share a service with a group of people by checking the “Share the service” box when opening the service. Other members of the group will see the service and can use it. Creating groups can be done by writing to administrators on Tchap (privately) or by email at innovation@insee.fr, providing the group name, usernames of the members, and whether or not a associated storage space is needed on MinIO.\n\nFor occasional needs, it is also possible to share a service that you have created with another person. Simply provide them with the URL (e.g., https://user-aaaaaaaaaaaaaa-xxxxxxx-x.user.lab.sspcloud.fr/) and the service password. The username remains Onyxia. Please note, it is recommended to change the service password during its launch (in the Security tab) to avoid any leaks. You should also uncheck Enable IP protection and Enable network policy in the Security tab. Only one person at a time can connect to an RStudio service.\n\n\n\n\n\n\n\n\n\n\n\n\nA link to a shell script (sequence of Linux commands) that is executed right after the service is launched. Convenient for automating the setup of certain configurations.\nThis script link must be accessible on the internet, for example, on https://git.lab.sspcloud.fr/ with a public project or on S3 storage with a public file.\nExample of an initialization script that clones a project from a private Gitlab instance, configures global RStudio options, automatically opens the cloned RStudio project, installs and selects French spelling correction, and customizes code snippets (snippets).\n\n\n\n\n\n\nWarning\n\n\n\nThe script is executed as a superuser (Root), and the files it creates become the property of the superuser. This leads to errors when these files are called, for example, RStudio configuration files. To give normal user rights (named onyxia) to their personal folder\n\n\nchown -R ${USERNAME}:${GROUPNAME} ${HOME}\n\n\n\nOptions to pass to the initialization script, separated by spaces and can be subsequently called with $1, $2, etc.\nFor example, if you enter file1.txt file2.txt in the PersonalInitArgs field and use this initialization script:\n#!/bin/bash\ntouch $1\ntouch $2\nThe script will create two files, file1.txt and file2.txt, using the touch command.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is the password to enter when opening a service, which is provided by “Copy the password” on the service page. It is supplied by the general parameter “Password for your services,” which can be found in “My Account” &gt; “Account Information,” unless a specific one is defined at the service level.\n\n\n\nIf checked, the service is only accessible from a single IP. Uncheck it if you wish to work from different locations.\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page.\n\n\n\n\n\n\nWarning\n\n\n\nIt is not possible to automatically clone a private project from a private instance (i.e., other than gitlab.com and github.com). To do so, you will need to use a shell script as indicated here.\n\n\n\n\nIf checked, configure Git and attempt a clone when the service starts.\n\n\n\nThe name that will appear in the commits (not the Gitlab or Github account username).\n\n\n\nThe email address that will appear in the commits (not necessarily the email associated with the Gitlab or Github account).\n\n\n\n\n\n\nAccess token defined on the platform used (Gitlab, Github…).\n\n\n\nThe URL obtained from the platform used (Gitlab, Github…) by clicking on “Clone” &gt; HTTPS.\nIn the format:\nhttps://github.com/InseeFrLab/docs.sspcloud.fr.git\n\n\n\n\n\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page.\n:::"
  },
  {
    "objectID": "content/services-configuration.html#custom-name",
    "href": "content/services-configuration.html#custom-name",
    "title": "Service Configuration",
    "section": "",
    "text": "To recognize the service and/or the configuration if saved by clicking on the bookmark symbol at the top right. If the name already exists among the saved configurations, saving will overwrite the old configuration.\nConvenient for distinguishing different services of the same type (RStudio, Jupyter…)."
  },
  {
    "objectID": "content/services-configuration.html#share-the-service",
    "href": "content/services-configuration.html#share-the-service",
    "title": "Service Configuration",
    "section": "",
    "text": "It is possible to share a service with a group of people by checking the “Share the service” box when opening the service. Other members of the group will see the service and can use it. Creating groups can be done by writing to administrators on Tchap (privately) or by email at innovation@insee.fr, providing the group name, usernames of the members, and whether or not a associated storage space is needed on MinIO.\n\nFor occasional needs, it is also possible to share a service that you have created with another person. Simply provide them with the URL (e.g., https://user-aaaaaaaaaaaaaa-xxxxxxx-x.user.lab.sspcloud.fr/) and the service password. The username remains Onyxia. Please note, it is recommended to change the service password during its launch (in the Security tab) to avoid any leaks. You should also uncheck Enable IP protection and Enable network policy in the Security tab. Only one person at a time can connect to an RStudio service."
  },
  {
    "objectID": "content/services-configuration.html#init-1",
    "href": "content/services-configuration.html#init-1",
    "title": "Service Configuration",
    "section": "",
    "text": "A link to a shell script (sequence of Linux commands) that is executed right after the service is launched. Convenient for automating the setup of certain configurations.\nThis script link must be accessible on the internet, for example, on https://git.lab.sspcloud.fr/ with a public project or on S3 storage with a public file.\nExample of an initialization script that clones a project from a private Gitlab instance, configures global RStudio options, automatically opens the cloned RStudio project, installs and selects French spelling correction, and customizes code snippets (snippets).\n\n\n\n\n\n\nWarning\n\n\n\nThe script is executed as a superuser (Root), and the files it creates become the property of the superuser. This leads to errors when these files are called, for example, RStudio configuration files. To give normal user rights (named onyxia) to their personal folder\n\n\nchown -R ${USERNAME}:${GROUPNAME} ${HOME}\n\n\n\nOptions to pass to the initialization script, separated by spaces and can be subsequently called with $1, $2, etc.\nFor example, if you enter file1.txt file2.txt in the PersonalInitArgs field and use this initialization script:\n#!/bin/bash\ntouch $1\ntouch $2\nThe script will create two files, file1.txt and file2.txt, using the touch command."
  },
  {
    "objectID": "content/services-configuration.html#security-1",
    "href": "content/services-configuration.html#security-1",
    "title": "Service Configuration",
    "section": "",
    "text": "This is the password to enter when opening a service, which is provided by “Copy the password” on the service page. It is supplied by the general parameter “Password for your services,” which can be found in “My Account” &gt; “Account Information,” unless a specific one is defined at the service level.\n\n\n\nIf checked, the service is only accessible from a single IP. Uncheck it if you wish to work from different locations."
  },
  {
    "objectID": "content/services-configuration.html#git-1",
    "href": "content/services-configuration.html#git-1",
    "title": "Service Configuration",
    "section": "",
    "text": "To learn how to use this tab, see the dedicated page.\n\n\n\n\n\n\nWarning\n\n\n\nIt is not possible to automatically clone a private project from a private instance (i.e., other than gitlab.com and github.com). To do so, you will need to use a shell script as indicated here.\n\n\n\n\nIf checked, configure Git and attempt a clone when the service starts.\n\n\n\nThe name that will appear in the commits (not the Gitlab or Github account username).\n\n\n\nThe email address that will appear in the commits (not necessarily the email associated with the Gitlab or Github account).\n\n\n\n\n\n\nAccess token defined on the platform used (Gitlab, Github…).\n\n\n\nThe URL obtained from the platform used (Gitlab, Github…) by clicking on “Clone” &gt; HTTPS.\nIn the format:\nhttps://github.com/InseeFrLab/docs.sspcloud.fr.git\n\n\n\n\n\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page.\n:::"
  },
  {
    "objectID": "content/version-control.html",
    "href": "content/version-control.html",
    "title": "Version Control",
    "section": "",
    "text": "The Datalab is a shared platform where resources used by services are shared among different users. As such, Datalab services operate on the model of ephemeral containers: in a standard usage, the user launches a service, performs data processing, saves the code that was used to perform these processes, and then deletes the service instance. This code backup is greatly facilitated by the use of version control.\nHowever, this performance consideration should not be seen as a constraint: version control is an essential best practice in development. The benefits are numerous, both individually:\n\nThe local project is synchronized with a remote server, making the loss of code almost impossible.\nThe complete history of choices and modifications made on the project is preserved.\nThe user can browse this history to search for modifications that may have caused errors and decide at any time to revert to a previous version of the project or specific files.\n\nand in the context of collaborative projects:\n\nSimultaneous work on the same project is possible without the risk of loss.\nThe user can share their modifications while benefiting from the changes made by others.\nIt becomes possible to contribute to open-source projects, for which the use of Git is widely standard.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial aims to present how version control can be easily implemented using the tools available on the Datalab. It does not explain the workings of Git and presupposes a certain familiarity with the tool. Many online resources can serve as an introduction; for example, R users can consult this training program, and Python users can refer to this chapter of the course.\n\n\n\n\n\n\n\nAlthough offline use of Git is possible, the real benefit of version control lies in synchronizing the local copy of a project (clone) with a remote repository (remote). Various software forge services allow this synchronization of Git projects, the most well-known of which are GitHub and GitLab. Since GitHub has much more visibility today — for example, the repositories of Insee, InseeFr, and InseeFrLab, are on GitHub — Datalab offers an integration with GitHub, which we present in this tutorial.\n\n\n\n\n\n\nWarning\n\n\n\nThe rest of the tutorial requires having a GitHub account.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile using Datalab with the GitHub platform is facilitated, it is by no means mandatory: it is still possible to use the software forge of your choice for project synchronization. A forge based on GitLab is also available to Datalab users.\n\n\n\n\n\nSynchronization with a remote repository requires authentication with GitHub. This is done using a personal access token, which must be generated from the user’s GitHub account. The generation service is accessible at this address. The GitHub documentation provides illustrations to guide the process.\nTo generate a token, it is necessary to choose a token name, an expiration date, and access rights (scope). It is recommended to choose a short expiration date (30 days) and limited access (repo only) to reduce security risks in case the token is maliciously exposed.\n\n\n\nRecommended configuration for generating a GitHub access token\n\n\nOnce the token is generated, it will be displayed on the screen. A token can only be viewed once; if lost, a new one must be generated.\n\n\n\nIt is recommended to add access tokens to a password manager. Alternatively, the token can be added to the “External Services” configuration of the user account on Datalab, which allows the token to be directly accessible within the services offered on the platform.\n\n\n\nAdding a GitHub access token to a user account on Datalab\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to use the email address associated with your GitHub account in the “Account Information.” It is this address that effectively links the commits you make to your GitHub account.\n\n\n\n\n\n\nGit is pre-configured to work natively with the relevant Datalab services. When opening a service, it is possible to configure certain elements. If you have added a GitHub access token to your Datalab account, it will be pre-configured. It is also possible to indicate the complete URL of a Git repository (e.g., https://github.com/InseeFrLab/onyxia), which will then be cloned into the instance’s workspace during initialization.\n\n\n\nConfiguring Git when opening a service\n\n\n\n\nThe GitHub access token is available in the terminals of different services via the environment variable $GIT_PERSONAL_ACCESS_TOKEN. To avoid having to authenticate for every operation involving the remote repository (clone, push, and pull), it is recommended to clone it, including the access token in the HTTPS link, using the following command:\ngit clone https://${GIT_PERSONAL_ACCESS_TOKEN}@github.com/&lt;owner&gt;/&lt;repo&gt;.git\nwhere  and  should be replaced with the GitHub username and repository name, respectively.\n\n\n\nThe main code production services available on Datalab have a graphical interface to facilitate the use of Git:\n\nRStudio: RStudio provides a native and fairly comprehensive graphical interface for Git. The utilitR documentation presents its operation in detail.\nJupyter: the jupyterlab-git plugin provides a (fairly basic) interface between Jupyter and Git.\nVSCode: VSCode natively offers a well-integrated graphical interface with Git and GitHub. A detailed documentation presents the possibilities of the tool.\n\n\n\n\n\n\n\nWarning\n\n\n\nGraphical interfaces make it easier to get started with Git but never completely replace the use of the tool via a terminal due to necessarily imperfect integration. Therefore, it is useful to familiarize yourself with using Git via the terminal as early as possible."
  },
  {
    "objectID": "content/version-control.html#why-use-version-control",
    "href": "content/version-control.html#why-use-version-control",
    "title": "Version Control",
    "section": "",
    "text": "The Datalab is a shared platform where resources used by services are shared among different users. As such, Datalab services operate on the model of ephemeral containers: in a standard usage, the user launches a service, performs data processing, saves the code that was used to perform these processes, and then deletes the service instance. This code backup is greatly facilitated by the use of version control.\nHowever, this performance consideration should not be seen as a constraint: version control is an essential best practice in development. The benefits are numerous, both individually:\n\nThe local project is synchronized with a remote server, making the loss of code almost impossible.\nThe complete history of choices and modifications made on the project is preserved.\nThe user can browse this history to search for modifications that may have caused errors and decide at any time to revert to a previous version of the project or specific files.\n\nand in the context of collaborative projects:\n\nSimultaneous work on the same project is possible without the risk of loss.\nThe user can share their modifications while benefiting from the changes made by others.\nIt becomes possible to contribute to open-source projects, for which the use of Git is widely standard.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial aims to present how version control can be easily implemented using the tools available on the Datalab. It does not explain the workings of Git and presupposes a certain familiarity with the tool. Many online resources can serve as an introduction; for example, R users can consult this training program, and Python users can refer to this chapter of the course."
  },
  {
    "objectID": "content/version-control.html#integrating-github-with-datalab",
    "href": "content/version-control.html#integrating-github-with-datalab",
    "title": "Version Control",
    "section": "",
    "text": "Although offline use of Git is possible, the real benefit of version control lies in synchronizing the local copy of a project (clone) with a remote repository (remote). Various software forge services allow this synchronization of Git projects, the most well-known of which are GitHub and GitLab. Since GitHub has much more visibility today — for example, the repositories of Insee, InseeFr, and InseeFrLab, are on GitHub — Datalab offers an integration with GitHub, which we present in this tutorial.\n\n\n\n\n\n\nWarning\n\n\n\nThe rest of the tutorial requires having a GitHub account.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile using Datalab with the GitHub platform is facilitated, it is by no means mandatory: it is still possible to use the software forge of your choice for project synchronization. A forge based on GitLab is also available to Datalab users.\n\n\n\n\n\nSynchronization with a remote repository requires authentication with GitHub. This is done using a personal access token, which must be generated from the user’s GitHub account. The generation service is accessible at this address. The GitHub documentation provides illustrations to guide the process.\nTo generate a token, it is necessary to choose a token name, an expiration date, and access rights (scope). It is recommended to choose a short expiration date (30 days) and limited access (repo only) to reduce security risks in case the token is maliciously exposed.\n\n\n\nRecommended configuration for generating a GitHub access token\n\n\nOnce the token is generated, it will be displayed on the screen. A token can only be viewed once; if lost, a new one must be generated.\n\n\n\nIt is recommended to add access tokens to a password manager. Alternatively, the token can be added to the “External Services” configuration of the user account on Datalab, which allows the token to be directly accessible within the services offered on the platform.\n\n\n\nAdding a GitHub access token to a user account on Datalab\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to use the email address associated with your GitHub account in the “Account Information.” It is this address that effectively links the commits you make to your GitHub account."
  },
  {
    "objectID": "content/version-control.html#using-git-with-datalab-services",
    "href": "content/version-control.html#using-git-with-datalab-services",
    "title": "Version Control",
    "section": "",
    "text": "Git is pre-configured to work natively with the relevant Datalab services. When opening a service, it is possible to configure certain elements. If you have added a GitHub access token to your Datalab account, it will be pre-configured. It is also possible to indicate the complete URL of a Git repository (e.g., https://github.com/InseeFrLab/onyxia), which will then be cloned into the instance’s workspace during initialization.\n\n\n\nConfiguring Git when opening a service\n\n\n\n\nThe GitHub access token is available in the terminals of different services via the environment variable $GIT_PERSONAL_ACCESS_TOKEN. To avoid having to authenticate for every operation involving the remote repository (clone, push, and pull), it is recommended to clone it, including the access token in the HTTPS link, using the following command:\ngit clone https://${GIT_PERSONAL_ACCESS_TOKEN}@github.com/&lt;owner&gt;/&lt;repo&gt;.git\nwhere  and  should be replaced with the GitHub username and repository name, respectively.\n\n\n\nThe main code production services available on Datalab have a graphical interface to facilitate the use of Git:\n\nRStudio: RStudio provides a native and fairly comprehensive graphical interface for Git. The utilitR documentation presents its operation in detail.\nJupyter: the jupyterlab-git plugin provides a (fairly basic) interface between Jupyter and Git.\nVSCode: VSCode natively offers a well-integrated graphical interface with Git and GitHub. A detailed documentation presents the possibilities of the tool.\n\n\n\n\n\n\n\nWarning\n\n\n\nGraphical interfaces make it easier to get started with Git but never completely replace the use of the tool via a terminal due to necessarily imperfect integration. Therefore, it is useful to familiarize yourself with using Git via the terminal as early as possible."
  }
]