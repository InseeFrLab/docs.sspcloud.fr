[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the SSP Cloud!",
    "section": "",
    "text": "An open instance of the Onyxia project, the SSP Cloud is a shared data processing platform. This “Datalab” is dedicated to experimenting with datascience methods based on open data. In this tutorial, we’ll take you on a guided tour of the Datalab, to help you get up and running quickly.\n\n\n\n\n\n\nWarning\n\n\n\nThe terms of use for the SSP Cloud can be found at this address. We remind you that the SSP Cloud is intended exclusively for the processing of public and non-sensitive data. Projects involving non-open data for experimentation can be carried out in consultation with the Onyxia team, subject to compliance with project-specific security rules.\n\n\n\n\nThe service catalog is at the core of the SSP Cloud’s usage. It offers a set of services for statistical data processing and complete management of data science projects.\n\n\n\nTo launch a service, simply click on the Launch button of the desired service.\nA page centered on the requested service will then open, offering several options:\n\nClick again on the Launch button to start the service with its default configuration.\nCustomize the name that the instance will have once the service is launched.\nUnfold a configuration menu to customize the service’s configuration before launching it.\nSave a customized configuration by clicking on the bookmark at the top right of the service.\n\n\nThe detailed configuration of services is an advanced usage and is not covered in this tutorial but in other pages of this documentation site.\n\n\n\n\nLaunching a service automatically takes you to the My Services page, where all active instances on the user’s account are listed.\n\nOnce the service is launched, an Open button appears, allowing access to the service. A password — and, depending on the services, a username — is generally required to use the service. This information is available in the service’s README, which can be accessed by clicking on the button of the same name.\n\n\n\nDeleting an instance of a service is done simply by clicking on the trash can icon below the instance.\n\n\n\n\n\n\nCaution\n\n\n\nFor some services, deleting an instance will also delete all associated data, and this action is irreversible. Therefore, it is essential to always read the README associated with the instance, which specifies the consequences of deleting the instance. In general, it is crucial to ensure that the data and code used are backed up before deleting the instance. Ideally, version your code with Git and regularly back up the data using MinIO (S3 storage system).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe resources made available for executing services are shared among different SSP Cloud users. Please make sure not to leave active services that you no longer use. We sometimes systematically remove inactive instances after a certain period to free up resources.\n\n\n\n\n\n\nSupport and assistance for using the SSP Cloud are provided through two communication channels:\n\nOn the Onyxia Slack in the #sspcloud channel. Any questions about using the SSP Cloud or suggestions for improvement are welcome there.\nIn a dedicated room of the French state inter-ministerial instant messaging service Tchap for French public agents."
  },
  {
    "objectID": "index.html#service-catalog",
    "href": "index.html#service-catalog",
    "title": "Welcome to the SSP Cloud!",
    "section": "",
    "text": "The service catalog is at the core of the SSP Cloud’s usage. It offers a set of services for statistical data processing and complete management of data science projects.\n\n\n\nTo launch a service, simply click on the Launch button of the desired service.\nA page centered on the requested service will then open, offering several options:\n\nClick again on the Launch button to start the service with its default configuration.\nCustomize the name that the instance will have once the service is launched.\nUnfold a configuration menu to customize the service’s configuration before launching it.\nSave a customized configuration by clicking on the bookmark at the top right of the service.\n\n\nThe detailed configuration of services is an advanced usage and is not covered in this tutorial but in other pages of this documentation site.\n\n\n\n\nLaunching a service automatically takes you to the My Services page, where all active instances on the user’s account are listed.\n\nOnce the service is launched, an Open button appears, allowing access to the service. A password — and, depending on the services, a username — is generally required to use the service. This information is available in the service’s README, which can be accessed by clicking on the button of the same name.\n\n\n\nDeleting an instance of a service is done simply by clicking on the trash can icon below the instance.\n\n\n\n\n\n\nCaution\n\n\n\nFor some services, deleting an instance will also delete all associated data, and this action is irreversible. Therefore, it is essential to always read the README associated with the instance, which specifies the consequences of deleting the instance. In general, it is crucial to ensure that the data and code used are backed up before deleting the instance. Ideally, version your code with Git and regularly back up the data using MinIO (S3 storage system).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe resources made available for executing services are shared among different SSP Cloud users. Please make sure not to leave active services that you no longer use. We sometimes systematically remove inactive instances after a certain period to free up resources."
  },
  {
    "objectID": "index.html#support-1",
    "href": "index.html#support-1",
    "title": "Welcome to the SSP Cloud!",
    "section": "",
    "text": "Support and assistance for using the SSP Cloud are provided through two communication channels:\n\nOn the Onyxia Slack in the #sspcloud channel. Any questions about using the SSP Cloud or suggestions for improvement are welcome there.\nIn a dedicated room of the French state inter-ministerial instant messaging service Tchap for French public agents."
  },
  {
    "objectID": "content/tutorials/set-up-environment.html",
    "href": "content/tutorials/set-up-environment.html",
    "title": "Setting Up an Environment Using the SSP Cloud Platform",
    "section": "",
    "text": "With SSP Cloud, you have the flexibility to create a custom, shareable link that launches a pre-configured service for learners, or, for an even smoother experience, add a link directly within tutorials referential. These options provide an easy path for starting a training session.\n\nGenerate a link that contains all the necessary configurations for your chosen environment, and share this link with apprentices. They can then access the environment instantly, with everything they need already set up.\nShare this link with the learners. You can create your own tutorial referential by publishing them through a GitHub repository, using a static website, or adding them to the Tutorials Section on the SSP Cloud Platform. By choosing the latter, you can create a custom “launch” button within SSP Cloud’s interface, mapped to your specific training environment. This makes it even easier for learners to access—just one click opens the configured environment directly.\n\nThe possibilities for sharing your links are endless. Choose the one that best fits your needs! ✨\n\n\nThe setup link is a powerful feature in SSP Cloud, allowing you to define all aspects of the training environment. In order to get your link, feel free to use the SSP Cloud platform and configure a service. Then click on the Copy auto launch URL button. By clicking the link, all learners launch the exact same environment, fully configured with the specified configuration and ready to use. This uniformity is a huge benefit in training scenarios: everyone works with the same setup, eliminating any inconsistencies that might arise from varied installations.\nThe link can include : - service selected to decide on the language and framework for the training - the resources to define the amount of computational resources required, including memory, CPU, and storage. This ensures that the environment matches the needs of the course content, even for intensive machine learning tasks - an initialization script to ensure all learners start with the same setup. See here for initialization scripts examples. This script can: - Clone the training repository with all relevant notebooks and materials. - Install any necessary libraries, dependencies… so there’s nothing to install locally. - and many more …\nHere’s an example link: https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=python-initiation&init.personalInit=%C2%ABhttps://raw.githubusercontent.com/InseeFrLab/formation-python-initiation/main/utils/init_onyxia.sh%C2%BB&init.personalInitArgs=%C2%ABen%20fundamentals%20types-variables%C2%BB&security.allowlist.enabled=false\nThis link automatically launches a Jupyter Notebook environment - you can change it to Vscode if prefered - on the sspcloud and configures it with pre-installed libraries and scripts, thanks to the init.personalInit argument. It sets up a predefined training module, in this case, “Python Fundamentals: Types and Variables”. All learners will start with this exact configuration. This eliminates the common issues of dependency mismatches and local installation errors, so learners can jump directly into the material.\n\n\n\nIn order to add a tutorial on the [SSP Cloud dedicated place] (https://www.sspcloud.fr/formation) you can submit a PR on github.\nThere, you’ll have the possibility to define the name of your course, fill an abstract to inform the users of its content, specify the authors, the date of creation of the course, to add tags and keywords… To set an image to represent the course, the number of expected hours to follow the course and an article URL if you want to direct the users to an external resource to explain the content of the course. Finally give the URL to launch the preconfigured service. 🚀️\nTo organize the course effectively, you can divide it into different parts. Use this feature to group related tutorials within a course folder, allowing learners to follow a structured, progressive format.\nFor example, you can follow this pattern\n\n{\n    \"name\": {\n        \"fr\": \"Nom d'un dossier contenant des cours\",\n        \"en\": \"Name of a folder training\",\n    },\n    \"abstract\": {\n        \"fr\": \"Description de ce que contient le dossier.\",\n        \"en\": \"Description of the content of the folder.\",\n    },\n    \"imageUrl\": anImgUrl,\n    \"tags\": [\"learn\", \"discover\"],\n    \"parts\": [\n        {\n            \"name\": {\n                \"fr\": \"1. Un titre descriptif pour la première partie de la formation\",\n                \"en\": \"1. A descriptive title for the first part of the course\",\n            },\n            \"abstract\": {\n                \"fr\": \"Ici se trouve une description de la première partie de la formation\",\n                \"en\": \"Here lies a description of the first part of the course\",\n            },\n            \"authors\": [\"Inseefrlab\"],\n            \"types\": [\"Notebook Python\"],\n            \"tags\": [\"discover\", \"learn\"],\n            \"category\": \"Discover the platform\",\n            \"imageUrl\": anImgUrl,\n            \"timeRequired\": 1,\n            \"articleUrl\": anArticleUrl,\n            \"deploymentUrl\": {\n                \"fr\": \"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&init.personalInit=«https://raw.githubusercontent.com/InseeFrLab/myinitscript\",\n                \"en\": \"https://datalab.sspcloud.fr/launcher/ide/jupyter-pyspark?autoLaunch=true&init.personalInit=«https://raw.githubusercontent.com/InseeFrLab/InseeFrLab/myinitscript\",\n            },\n        },\n        {\n            \"name\": {\n                \"fr\": \"2. Un titre descriptif pour la seconde partie de la formation\",\n                \"en\": \"2. A descriptive title for the second part of the course\",\n            },\n            \"abstract\": {\n                \"fr\": \"Ici se trouve une description de la deuxième partie de la formation\",\n                \"en\": \"Here lies a description of the second part of the course\",\n            },\n            \"authors\": [\"Inseefrlab\"],\n            \"types\": [\"Notebook Python\"],\n            \"tags\": [\"discover\", \"learn\"],\n            \"category\": \"Discover the platform\",\n            \"imageUrl\": anImgUrl,\n            \"timeRequired\": 1,\n            \"deploymentUrl\": {\n                \"fr\": \"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&init.personalInit=«https://raw.githubusercontent.com/InseeFrLab/myinitscript\",\n                \"en\": \"https://datalab.sspcloud.fr/launcher/ide/jupyter-pyspark?autoLaunch=true&init.personalInit=«https://raw.githubusercontent.com/InseeFrLab/InseeFrLab/myinitscript\",\n            },\n        }\n    ]\n}\n\n\n\nIf your course requires data, you can make it accessible either by sharing a public link (allowing open access) or by placing the data in a “diffusion” folder within your bucket on the platform. This second option automatically makes the data available to all SSP Cloud users, providing seamless integration within the platform environment and consequently reduce the network traffic.\nClick here for more information\n\n\n\nThe SSP Cloud combined with Notebooks creates an ideal environment for teaching and learning programming or data science. The combination of instant, cloud-based environments with the interactive nature of notebooks allows for engaging, reproducible, and scalable training sessions. By leveraging these tools, educators can focus on what matters most, eg teaching and learning, without worrying about setup issues or inconsistent environments.\nFor your next training session, try setting up your environment using the SSP Cloud, and see how they can transform the learning experience! 👩‍🎓️"
  },
  {
    "objectID": "content/tutorials/set-up-environment.html#how-the-link-works",
    "href": "content/tutorials/set-up-environment.html#how-the-link-works",
    "title": "Setting Up an Environment Using the SSP Cloud Platform",
    "section": "",
    "text": "The setup link is a powerful feature in SSP Cloud, allowing you to define all aspects of the training environment. In order to get your link, feel free to use the SSP Cloud platform and configure a service. Then click on the Copy auto launch URL button. By clicking the link, all learners launch the exact same environment, fully configured with the specified configuration and ready to use. This uniformity is a huge benefit in training scenarios: everyone works with the same setup, eliminating any inconsistencies that might arise from varied installations.\nThe link can include : - service selected to decide on the language and framework for the training - the resources to define the amount of computational resources required, including memory, CPU, and storage. This ensures that the environment matches the needs of the course content, even for intensive machine learning tasks - an initialization script to ensure all learners start with the same setup. See here for initialization scripts examples. This script can: - Clone the training repository with all relevant notebooks and materials. - Install any necessary libraries, dependencies… so there’s nothing to install locally. - and many more …\nHere’s an example link: https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&name=python-initiation&init.personalInit=%C2%ABhttps://raw.githubusercontent.com/InseeFrLab/formation-python-initiation/main/utils/init_onyxia.sh%C2%BB&init.personalInitArgs=%C2%ABen%20fundamentals%20types-variables%C2%BB&security.allowlist.enabled=false\nThis link automatically launches a Jupyter Notebook environment - you can change it to Vscode if prefered - on the sspcloud and configures it with pre-installed libraries and scripts, thanks to the init.personalInit argument. It sets up a predefined training module, in this case, “Python Fundamentals: Types and Variables”. All learners will start with this exact configuration. This eliminates the common issues of dependency mismatches and local installation errors, so learners can jump directly into the material."
  },
  {
    "objectID": "content/tutorials/set-up-environment.html#publish-your-training-course-in-the-tutorials-section-on-the-ssp-cloud-platform",
    "href": "content/tutorials/set-up-environment.html#publish-your-training-course-in-the-tutorials-section-on-the-ssp-cloud-platform",
    "title": "Setting Up an Environment Using the SSP Cloud Platform",
    "section": "",
    "text": "In order to add a tutorial on the [SSP Cloud dedicated place] (https://www.sspcloud.fr/formation) you can submit a PR on github.\nThere, you’ll have the possibility to define the name of your course, fill an abstract to inform the users of its content, specify the authors, the date of creation of the course, to add tags and keywords… To set an image to represent the course, the number of expected hours to follow the course and an article URL if you want to direct the users to an external resource to explain the content of the course. Finally give the URL to launch the preconfigured service. 🚀️\nTo organize the course effectively, you can divide it into different parts. Use this feature to group related tutorials within a course folder, allowing learners to follow a structured, progressive format.\nFor example, you can follow this pattern\n\n{\n    \"name\": {\n        \"fr\": \"Nom d'un dossier contenant des cours\",\n        \"en\": \"Name of a folder training\",\n    },\n    \"abstract\": {\n        \"fr\": \"Description de ce que contient le dossier.\",\n        \"en\": \"Description of the content of the folder.\",\n    },\n    \"imageUrl\": anImgUrl,\n    \"tags\": [\"learn\", \"discover\"],\n    \"parts\": [\n        {\n            \"name\": {\n                \"fr\": \"1. Un titre descriptif pour la première partie de la formation\",\n                \"en\": \"1. A descriptive title for the first part of the course\",\n            },\n            \"abstract\": {\n                \"fr\": \"Ici se trouve une description de la première partie de la formation\",\n                \"en\": \"Here lies a description of the first part of the course\",\n            },\n            \"authors\": [\"Inseefrlab\"],\n            \"types\": [\"Notebook Python\"],\n            \"tags\": [\"discover\", \"learn\"],\n            \"category\": \"Discover the platform\",\n            \"imageUrl\": anImgUrl,\n            \"timeRequired\": 1,\n            \"articleUrl\": anArticleUrl,\n            \"deploymentUrl\": {\n                \"fr\": \"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&init.personalInit=«https://raw.githubusercontent.com/InseeFrLab/myinitscript\",\n                \"en\": \"https://datalab.sspcloud.fr/launcher/ide/jupyter-pyspark?autoLaunch=true&init.personalInit=«https://raw.githubusercontent.com/InseeFrLab/InseeFrLab/myinitscript\",\n            },\n        },\n        {\n            \"name\": {\n                \"fr\": \"2. Un titre descriptif pour la seconde partie de la formation\",\n                \"en\": \"2. A descriptive title for the second part of the course\",\n            },\n            \"abstract\": {\n                \"fr\": \"Ici se trouve une description de la deuxième partie de la formation\",\n                \"en\": \"Here lies a description of the second part of the course\",\n            },\n            \"authors\": [\"Inseefrlab\"],\n            \"types\": [\"Notebook Python\"],\n            \"tags\": [\"discover\", \"learn\"],\n            \"category\": \"Discover the platform\",\n            \"imageUrl\": anImgUrl,\n            \"timeRequired\": 1,\n            \"deploymentUrl\": {\n                \"fr\": \"https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=true&init.personalInit=«https://raw.githubusercontent.com/InseeFrLab/myinitscript\",\n                \"en\": \"https://datalab.sspcloud.fr/launcher/ide/jupyter-pyspark?autoLaunch=true&init.personalInit=«https://raw.githubusercontent.com/InseeFrLab/InseeFrLab/myinitscript\",\n            },\n        }\n    ]\n}"
  },
  {
    "objectID": "content/tutorials/set-up-environment.html#make-your-data-available-to-everyone",
    "href": "content/tutorials/set-up-environment.html#make-your-data-available-to-everyone",
    "title": "Setting Up an Environment Using the SSP Cloud Platform",
    "section": "",
    "text": "If your course requires data, you can make it accessible either by sharing a public link (allowing open access) or by placing the data in a “diffusion” folder within your bucket on the platform. This second option automatically makes the data available to all SSP Cloud users, providing seamless integration within the platform environment and consequently reduce the network traffic.\nClick here for more information"
  },
  {
    "objectID": "content/tutorials/set-up-environment.html#conclusion",
    "href": "content/tutorials/set-up-environment.html#conclusion",
    "title": "Setting Up an Environment Using the SSP Cloud Platform",
    "section": "",
    "text": "The SSP Cloud combined with Notebooks creates an ideal environment for teaching and learning programming or data science. The combination of instant, cloud-based environments with the interactive nature of notebooks allows for engaging, reproducible, and scalable training sessions. By leveraging these tools, educators can focus on what matters most, eg teaching and learning, without worrying about setup issues or inconsistent environments.\nFor your next training session, try setting up your environment using the SSP Cloud, and see how they can transform the learning experience! 👩‍🎓️"
  },
  {
    "objectID": "content/tutorials/course-structure.html",
    "href": "content/tutorials/course-structure.html",
    "title": "How to Structure an Effective Training Course with SSP Cloud notebooks",
    "section": "",
    "text": "How to Structure an Effective Training Course with SSP Cloud notebooks\n\nClear Objectives: 👨‍🎓 Start with clear learning objectives. For example, a Python basics course could focus on data types, control structures, and basic data manipulation using libraries like Pandas.\nPre-configured Environment: Create an SSP Cloud environment with all necessary libraries included see the next tab in order to configure the environment. This allows learners to focus on learning instead of dealing with setup problems.\nModular Notebooks: 📚 Break the course into several Notebooks, each covering a key concept or topic. Each notebook should have a combination of:\n\nExplanatory text: To describe the theory and concepts behind the code.\nCode cells: For learners to run, modify, and experiment with.\nExercises: Challenge students with problems to solve using the concepts they’ve learned.\n\nFor example:\n\nNotebook 1: Introduction to Python and data types.\nNotebook 2: Control flow (loops, conditionals).\nNotebook 3: Working with files and data.\n\nStep-by-Step Walkthroughs: 👣 Each notebook should contain step-by-step instructions that learners can follow, starting with simple examples and building up to more complex exercises. Use markdown cells to explain what each piece of code does.\nInteractive Exercises: Include exercises at the end of each module. Encourage learners to modify the code, test different inputs, and debug any errors they encounter.\n\nYou can display solutions for exercises in two ways: either by creating separate files - one for exercises and another one for solutions - or by embedding the solution directly within the course using the following snippet of code:\n&lt;details&gt;\n\n&lt;summary&gt;\n\nShow solution\n\n&lt;/summary&gt;\n\nContent of the solution\n\n&lt;/details&gt;\n\nVisualizations and Data Exploration: 🔎 Make use of the visualization libraries to create engaging plots and graphs. This is especially useful when teaching data analysis, as it allows learners to immediately see the impact of their code on data.\nProgressive Difficulty: 📈 Start with basic concepts and gradually introduce more advanced topics, allowing learners to build their skills step by step. This scaffolding approach ensures that students aren’t overwhelmed but are constantly challenged."
  },
  {
    "objectID": "content/storage.html",
    "href": "content/storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "The file storage solution associated with Datalab is MinIO, an object storage system based on the cloud, compatible with Amazon’s S3 API. In practice, this has several advantages:\n\nStored files are easily accessible from anywhere: a file can be accessed directly via a simple URL, which can be shared.\nIt is possible to access the stored files directly within the data science services (R, Python, etc.) offered on Datalab, without the need to copy the files locally beforehand, greatly improving the reproducibility of analyses.\n\n\n\n\nMinIO Schema\n\n\n\n\n\n\n\nThe My Files page in Datalab takes the form of a file explorer showing the different buckets (repositories) to which the user has access.\nEach user has a personal bucket by default to store their files. Within this bucket, two options are possible:\n\n“Create a directory”: Creates a directory in the current bucket/directory hierarchically, similar to a traditional file system.\n“Upload a file”: Uploads one or multiple files to the current directory.\n\n\n\n\n\n\n\nNote\n\n\n\nThe graphical interface for data storage in Datalab is still under construction. As such, it may experience responsiveness issues. For frequent operations on file storage, it may be preferable to interact with MinIO via the terminal.\n\n\n\n\n\nThe default access policy of the S3 storage forbids all access to a bucket by any third party users of the SSPCLoud. The only exception is the diffusion folder located directly at the root of each bucket for which other users have read-only access by default.\nTherefore a straightforward way to share files for, say, a training session, is to create a diffusion folder in the user’s personal bucket and use it to store all resources meant to be shared with other users of the plateform.\nBy clicking on a file in their personal bucket, the user can access its characteristics page. On this page, it is also possible to manually change the diffusion status of the file. Changing the status of the file from “private” to “public” generates a diffusion link, which can then be shared for downloading the file. The “public” status only grants read-only access rights to other users, and modifying or deleting other users’ personal files is not possible.\n\n\n\n\n\n\nNote\n\n\n\nFor collaborative projects, it can be beneficial for different participants to have access to a shared storage space. It is possible to create shared buckets on MinIO for this purpose. Feel free to contact us via the channels specified on the “First Use” page if you wish to work on open data projects on the SSPCloud Datalab.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn accordance with the terms of use, only non-sensitive data (e.g. open data) may be stored on the SSPCloud Datalab. Setting a file’s diffusion status to “private” does not guarantee its confidentiality.\n\n\n\n\n\n\nThe access credentials needed to access data on MinIO are pre-configured in the various Datalab services, accessible in the form of environment variables. This greatly facilitates importing and exporting files from the services.\n\n\n\nRPythonmc\n\n\nIn R, interaction with an S3-compatible file system is made possible by the aws.s3 library.\nlibrary(aws.s3)\n\n\nIn Python, interaction with an S3-compatible file system is made possible by two libraries:\n\nBoto3, a library created and maintained by Amazon.\nS3Fs, a library that allows interaction with stored files similar to a classic filesystem.\n\nFor this reason, and because S3Fs is used by default by the pandas library to manage S3 connections, we will present how to manage storage on MinIO via Python using this library.\nimport os\nimport s3fs\n\n# Create filesystem object\nS3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n\n\nMinIO offers a command-line client (mc) that allows interaction with the storage system in a manner similar to a classic UNIX filesystem. This client is installed by default and accessible via a terminal in the various Datalab services.\nThe MinIO client offers basic UNIX commands such as ls, cat, cp, etc. The complete list is available in the client documentation.\n\n\n\n\n\n\n\nRPythonmc\n\n\naws.s3::get_bucket(\"donnees-insee\", region = \"\")\n\n\nfs.ls(\"donnees-insee\")\n\n\nThe Datalab storage is accessible via the alias s3. For example, to list the files in the bucket donnees-insee:\nmc ls s3/donnees-insee\n\n\n\n\n\n\n\nRPythonmc\n\n\nBUCKET &lt;- \"donnees-insee\"\nFILE_KEY_S3 &lt;- \"diffusion/BPE/2019/BPE_ENS.csv\"\n\ndf &lt;-\n  aws.s3::s3read_using(\n    FUN = readr::read_delim,\n    # Put FUN options here\n    delim = \";\",\n    object = FILE_KEY_S3,\n    bucket = BUCKET,\n    opts = list(\"region\" = \"\")\n  )\n\n\nThe S3Fs package allows you to interact with files stored on MinIO as if they were local files. The syntax is therefore very familiar to Python users. For example, to import/export tabular data via pandas:\nimport pandas as pd\n\nBUCKET = \"donnees-insee\"\nFILE_KEY_S3 = \"diffusion/BPE/2019/BPE_ENS.csv\"\nFILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n\nwith fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n    df_bpe = pd.read_csv(file_in, sep=\";\")\n\n\nTo copy data from a MinIO bucket to the local service:\nmc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv\n\n\n\n\n\n\nWarning\n\n\n\nCopying files to the local service is generally not a good practice: it limits the reproducibility of analyses and becomes quickly impossible with large volumes of data. Therefore, it is preferable to get into the habit of importing data directly into R/Python.\n\n\n\n\n\n\n\n\n\nRPythonmc\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\n\naws.s3::s3write_using(\n    df,\n    FUN = readr::write_csv,\n    object = FILE_KEY_OUT_S3,\n    bucket = BUCKET_OUT,\n    opts = list(\"region\" = \"\")\n)\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\nFILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n\nwith fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n    df_bpe.to_csv(file_out)\n\n\nTo copy data from the local service to a bucket on MinIO:\nmc cp local/path/to/my/file.csv s3/&lt;my_bucket&gt;/remote/path/to/my/file.csv\n\n\n\n\n\n\nAccess to MinIO storage is possible via a personal access token, which is valid for 7 days and automatically regenerated at regular intervals on SSP Cloud. When a token has expired, services created before the expiration date (using the previous token) can no longer access storage, and the affected service will appear in red on the My Services page. In this case, there are two options:\n\nOpen a new service on Datalab, which will have a default, up-to-date token.\nManually replace expired tokens with new ones. Scripts indicating how to do this for different MinIO uses (R/Python/mc) are available here. Simply choose the relevant script and execute it in your current working environment.\n\n\n\n\n\n\n\nFor security reasons, the authentication to MinIO used by default in the interactive services of the SSP Cloud relies on a temporary access token. In the context of projects involving periodic processing or the deployment of applications, a more permanent access to MinIO data may be required.\nIn this case, a service account is used, which is an account tied to a specific project or application rather than an individual. Technically, instead of authenticating to MinIO via a triplet (access key id, secret access key, and session token), a pair (access key id, secret access key) will be used, granting read/write permissions to a specific project bucket.\nThe procedure for creating a service account is described below.\n\nGraphical InterfaceTerminal (mc)\n\n\n\nOpen the MinIO console\nOpen the Access Keys tab\nThe service account information is pre-generated. It is possible to modify the access key to give it a simpler name.\nThe policy specifying the rights is also pre-generated. Ideally, the policy should be restricted to only cover the project bucket(s).\nOnce the service account is generated, the access key and secret access key can be used to authenticate the services/applications to the specified bucket.\n\n\n\n\nCreate a service on the SSP Cloud with up-to-date MinIO access. Confirm that the connection works with:\n\nmc ls s3/&lt;username&gt;\n\nGenerate a policy.json file with the following content, replacing project-&lt;my_project&gt; with the name of the relevant bucket (twice):\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n     {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n       \"s3:*\"\n      ],\n      \"Resource\": [\n       \"arn:aws:s3:::projet-&lt;my_project&gt;\",\n       \"arn:aws:s3:::projet-&lt;my_project&gt;/*\"\n      ]\n     }\n    ]\n  }\n\nIn a terminal, generate the service account with the following command:\n\nmc admin user svcacct add s3 $AWS_ACCESS_KEY_ID --access-key=\"&lt;access-key&gt;\" --secret-key=\"&lt;secret-key&gt;\" --policy=\"policy.json\"\nreplacing &lt;access-key&gt; and &lt;secret-key&gt; with names of your choice. Ideally, give a simple name for the access key (e.g., sa-project-projectname) but a complex key for the secret access key, which can be generated, for example, with the gpg client:\ngpg --gen-random --armor 1 16\n\nYou can now use the access key and secret access key to authenticate the services/applications to the specified bucket\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that the generated authentication information appears only once. They can then be stored in a password manager, a secret storage service like Vault, or via the Onyxia project settings feature, which allows importing the service account directly into services at the time of their configuration."
  },
  {
    "objectID": "content/storage.html#principles",
    "href": "content/storage.html#principles",
    "title": "Data Storage",
    "section": "",
    "text": "The file storage solution associated with Datalab is MinIO, an object storage system based on the cloud, compatible with Amazon’s S3 API. In practice, this has several advantages:\n\nStored files are easily accessible from anywhere: a file can be accessed directly via a simple URL, which can be shared.\nIt is possible to access the stored files directly within the data science services (R, Python, etc.) offered on Datalab, without the need to copy the files locally beforehand, greatly improving the reproducibility of analyses.\n\n\n\n\nMinIO Schema"
  },
  {
    "objectID": "content/storage.html#managing-your-data",
    "href": "content/storage.html#managing-your-data",
    "title": "Data Storage",
    "section": "",
    "text": "The My Files page in Datalab takes the form of a file explorer showing the different buckets (repositories) to which the user has access.\nEach user has a personal bucket by default to store their files. Within this bucket, two options are possible:\n\n“Create a directory”: Creates a directory in the current bucket/directory hierarchically, similar to a traditional file system.\n“Upload a file”: Uploads one or multiple files to the current directory.\n\n\n\n\n\n\n\nNote\n\n\n\nThe graphical interface for data storage in Datalab is still under construction. As such, it may experience responsiveness issues. For frequent operations on file storage, it may be preferable to interact with MinIO via the terminal.\n\n\n\n\n\nThe default access policy of the S3 storage forbids all access to a bucket by any third party users of the SSPCLoud. The only exception is the diffusion folder located directly at the root of each bucket for which other users have read-only access by default.\nTherefore a straightforward way to share files for, say, a training session, is to create a diffusion folder in the user’s personal bucket and use it to store all resources meant to be shared with other users of the plateform.\nBy clicking on a file in their personal bucket, the user can access its characteristics page. On this page, it is also possible to manually change the diffusion status of the file. Changing the status of the file from “private” to “public” generates a diffusion link, which can then be shared for downloading the file. The “public” status only grants read-only access rights to other users, and modifying or deleting other users’ personal files is not possible.\n\n\n\n\n\n\nNote\n\n\n\nFor collaborative projects, it can be beneficial for different participants to have access to a shared storage space. It is possible to create shared buckets on MinIO for this purpose. Feel free to contact us via the channels specified on the “First Use” page if you wish to work on open data projects on the SSPCloud Datalab.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIn accordance with the terms of use, only non-sensitive data (e.g. open data) may be stored on the SSPCloud Datalab. Setting a file’s diffusion status to “private” does not guarantee its confidentiality."
  },
  {
    "objectID": "content/storage.html#using-data-stored-on-minio",
    "href": "content/storage.html#using-data-stored-on-minio",
    "title": "Data Storage",
    "section": "",
    "text": "The access credentials needed to access data on MinIO are pre-configured in the various Datalab services, accessible in the form of environment variables. This greatly facilitates importing and exporting files from the services.\n\n\n\nRPythonmc\n\n\nIn R, interaction with an S3-compatible file system is made possible by the aws.s3 library.\nlibrary(aws.s3)\n\n\nIn Python, interaction with an S3-compatible file system is made possible by two libraries:\n\nBoto3, a library created and maintained by Amazon.\nS3Fs, a library that allows interaction with stored files similar to a classic filesystem.\n\nFor this reason, and because S3Fs is used by default by the pandas library to manage S3 connections, we will present how to manage storage on MinIO via Python using this library.\nimport os\nimport s3fs\n\n# Create filesystem object\nS3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n\n\nMinIO offers a command-line client (mc) that allows interaction with the storage system in a manner similar to a classic UNIX filesystem. This client is installed by default and accessible via a terminal in the various Datalab services.\nThe MinIO client offers basic UNIX commands such as ls, cat, cp, etc. The complete list is available in the client documentation.\n\n\n\n\n\n\n\nRPythonmc\n\n\naws.s3::get_bucket(\"donnees-insee\", region = \"\")\n\n\nfs.ls(\"donnees-insee\")\n\n\nThe Datalab storage is accessible via the alias s3. For example, to list the files in the bucket donnees-insee:\nmc ls s3/donnees-insee\n\n\n\n\n\n\n\nRPythonmc\n\n\nBUCKET &lt;- \"donnees-insee\"\nFILE_KEY_S3 &lt;- \"diffusion/BPE/2019/BPE_ENS.csv\"\n\ndf &lt;-\n  aws.s3::s3read_using(\n    FUN = readr::read_delim,\n    # Put FUN options here\n    delim = \";\",\n    object = FILE_KEY_S3,\n    bucket = BUCKET,\n    opts = list(\"region\" = \"\")\n  )\n\n\nThe S3Fs package allows you to interact with files stored on MinIO as if they were local files. The syntax is therefore very familiar to Python users. For example, to import/export tabular data via pandas:\nimport pandas as pd\n\nBUCKET = \"donnees-insee\"\nFILE_KEY_S3 = \"diffusion/BPE/2019/BPE_ENS.csv\"\nFILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n\nwith fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n    df_bpe = pd.read_csv(file_in, sep=\";\")\n\n\nTo copy data from a MinIO bucket to the local service:\nmc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv\n\n\n\n\n\n\nWarning\n\n\n\nCopying files to the local service is generally not a good practice: it limits the reproducibility of analyses and becomes quickly impossible with large volumes of data. Therefore, it is preferable to get into the habit of importing data directly into R/Python.\n\n\n\n\n\n\n\n\n\nRPythonmc\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\n\naws.s3::s3write_using(\n    df,\n    FUN = readr::write_csv,\n    object = FILE_KEY_OUT_S3,\n    bucket = BUCKET_OUT,\n    opts = list(\"region\" = \"\")\n)\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\nFILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n\nwith fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n    df_bpe.to_csv(file_out)\n\n\nTo copy data from the local service to a bucket on MinIO:\nmc cp local/path/to/my/file.csv s3/&lt;my_bucket&gt;/remote/path/to/my/file.csv\n\n\n\n\n\n\nAccess to MinIO storage is possible via a personal access token, which is valid for 7 days and automatically regenerated at regular intervals on SSP Cloud. When a token has expired, services created before the expiration date (using the previous token) can no longer access storage, and the affected service will appear in red on the My Services page. In this case, there are two options:\n\nOpen a new service on Datalab, which will have a default, up-to-date token.\nManually replace expired tokens with new ones. Scripts indicating how to do this for different MinIO uses (R/Python/mc) are available here. Simply choose the relevant script and execute it in your current working environment."
  },
  {
    "objectID": "content/storage.html#advanced-usage",
    "href": "content/storage.html#advanced-usage",
    "title": "Data Storage",
    "section": "",
    "text": "For security reasons, the authentication to MinIO used by default in the interactive services of the SSP Cloud relies on a temporary access token. In the context of projects involving periodic processing or the deployment of applications, a more permanent access to MinIO data may be required.\nIn this case, a service account is used, which is an account tied to a specific project or application rather than an individual. Technically, instead of authenticating to MinIO via a triplet (access key id, secret access key, and session token), a pair (access key id, secret access key) will be used, granting read/write permissions to a specific project bucket.\nThe procedure for creating a service account is described below.\n\nGraphical InterfaceTerminal (mc)\n\n\n\nOpen the MinIO console\nOpen the Access Keys tab\nThe service account information is pre-generated. It is possible to modify the access key to give it a simpler name.\nThe policy specifying the rights is also pre-generated. Ideally, the policy should be restricted to only cover the project bucket(s).\nOnce the service account is generated, the access key and secret access key can be used to authenticate the services/applications to the specified bucket.\n\n\n\n\nCreate a service on the SSP Cloud with up-to-date MinIO access. Confirm that the connection works with:\n\nmc ls s3/&lt;username&gt;\n\nGenerate a policy.json file with the following content, replacing project-&lt;my_project&gt; with the name of the relevant bucket (twice):\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n     {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n       \"s3:*\"\n      ],\n      \"Resource\": [\n       \"arn:aws:s3:::projet-&lt;my_project&gt;\",\n       \"arn:aws:s3:::projet-&lt;my_project&gt;/*\"\n      ]\n     }\n    ]\n  }\n\nIn a terminal, generate the service account with the following command:\n\nmc admin user svcacct add s3 $AWS_ACCESS_KEY_ID --access-key=\"&lt;access-key&gt;\" --secret-key=\"&lt;secret-key&gt;\" --policy=\"policy.json\"\nreplacing &lt;access-key&gt; and &lt;secret-key&gt; with names of your choice. Ideally, give a simple name for the access key (e.g., sa-project-projectname) but a complex key for the secret access key, which can be generated, for example, with the gpg client:\ngpg --gen-random --armor 1 16\n\nYou can now use the access key and secret access key to authenticate the services/applications to the specified bucket\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that the generated authentication information appears only once. They can then be stored in a password manager, a secret storage service like Vault, or via the Onyxia project settings feature, which allows importing the service account directly into services at the time of their configuration."
  },
  {
    "objectID": "content/secrets.html",
    "href": "content/secrets.html",
    "title": "Secrets Management",
    "section": "",
    "text": "Secrets Management\n\nEnvironment Variables\nSometimes, certain pieces of information need to be made available to a large number of applications, or they should not be directly embedded in your code (access tokens, passwords, etc.). The use of environment variables allows accessing this information from any service.\nWhen a service is launched, several environment variables are automatically injected—such as access tokens for GitHub and MinIO.\n\n\n\nCreation and Management of Secrets\nOn the platform, environment variables are treated as secrets stored in Vault (the Datalab’s safe) and are encrypted. This enables you to store tokens, credentials, and passwords securely. The My Secrets page is designed like a file explorer, allowing you to sort and organize your variables into folders.\n\nGetting Started:\n\nCreate a new folder with + New folder.\nThen, within this folder, create a new secret with + New secret.\nOpen your secret.\n\n\nEach secret can contain multiple variables, consisting of key-value pairs.\n\n+ Add a variable\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe keys (variable names) always begin with $ and contain only letters, numbers, and the underscore character (_). By convention, keys are written in UPPERCASE.\n\n\nFill in the name of the key and its value.\n\n\n\nConverting Secrets into Environment Variables\nOnce your secret is edited, along with its different variables, you are ready to use it in your service.\n\nCopy the secret’s path by clicking on the Use in a service button.\nThen, during the configuration of your service, go to the Vault tab and paste the secret’s path in the dedicated field.\n\n\n\nCreate and open your service.\n\nTo verify that your environment variables have been successfully created, you can run the following commands in the service terminal:\n# List all available environment variables\nenv\n\n# Display the value of an environment variable\necho $MY_VARIABLE\n\n# Find all environment variables containing a given pattern\nenv | grep -i \"&lt;PATTERN&gt;\""
  },
  {
    "objectID": "content/principles.html",
    "href": "content/principles.html",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Onyxia project stems from the recognition of common difficulties faced by data scientists in the public sector:\n\nOften isolated agents, due to the relative scarcity of data skills in administration.\nInadequate infrastructures in terms of resources and technologies, which hinder innovation.\nDifficulty transitioning from experimentation to production due to multiple separations (physical separation, development languages, work methods) between business units and IT production.\n\nIn response to these challenges, Datalab SSP Cloud was built to offer a collaborative platform at multiple levels:\n\nSharing a modern infrastructure centered around deploying services through containers, specifically designed for data science applications.\nSharing methods through the mutualization of data science services offered, to which everyone can contribute.\nSharing knowledge through Datalab-related training and the formation of user communities focused on its utilization.\n\n\n\n\n\n\n\nNote\n\n\n\nOnyxia, Datalab SSP Cloud: What are the differences?\nOnyxia is an open-source project that provides a web application for accessing data science services. The Datalab SSP Cloud is an instance of the Onyxia project hosted at Insee.\n\n\n\n\n\nThe architecture of Datalab is based on a set of fundamental principles:\n\nData science-oriented production by providing an infrastructure suitable for most use cases and a service catalog covering the entire data project lifecycle.\nChoices that promote user autonomy by avoiding proprietary lock-ins and enabling access to the lower layers of the infrastructure to address advanced and specific needs.\nA project that is 100% cloud-native and also cloud-agnostic, allowing for easy deployment on any infrastructure.\nA completely open-source project, both in terms of its constituent components and its distribution (MIT license).\n\n\n\n\nDatalab can be accessed through a modern and responsive user interface, focused on providing a great user experience. It serves as the technical connection between the different components of Onyxia:\n\nOpen-source technologies that represent the state of the art in container deployment and orchestration, storage, and security.\nA service and tool catalog to support data science projects.\nA training and documentation platform to facilitate onboarding onto the offered technologies.\n\n\n\n\nFundamental building blocks of Datalab Onyxia\n\n\nThe service catalog is designed to accommodate the essential needs of data scientists, from self-service development to production deployment of processes or applications. The entire data project lifecycle is covered, and the catalog of services is regularly expanded to meet users’ new requirements.\n\n\n\nA comprehensive service catalog for data science projects\n\n\n\n\n\nThe Datalab Onyxia project is resolutely open at multiple levels:\n\nThe Datalab is accessible through its web interface to all public service agents (via AgentConnect or a gouv.fr email address) as well as students from statistical schools linked to Insee (Cefil, Ensai, Ensae).\nThe open-source code and modularity of the project allow for the deployment of a customized Onyxia instance on any Kubernetes-based infrastructure cluster.\nThe project is open to external contributions, whether related to the service catalog, the graphical interface, or the arrangement of the software components it comprises."
  },
  {
    "objectID": "content/principles.html#a-platform-for-collaboration",
    "href": "content/principles.html#a-platform-for-collaboration",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Onyxia project stems from the recognition of common difficulties faced by data scientists in the public sector:\n\nOften isolated agents, due to the relative scarcity of data skills in administration.\nInadequate infrastructures in terms of resources and technologies, which hinder innovation.\nDifficulty transitioning from experimentation to production due to multiple separations (physical separation, development languages, work methods) between business units and IT production.\n\nIn response to these challenges, Datalab SSP Cloud was built to offer a collaborative platform at multiple levels:\n\nSharing a modern infrastructure centered around deploying services through containers, specifically designed for data science applications.\nSharing methods through the mutualization of data science services offered, to which everyone can contribute.\nSharing knowledge through Datalab-related training and the formation of user communities focused on its utilization.\n\n\n\n\n\n\n\nNote\n\n\n\nOnyxia, Datalab SSP Cloud: What are the differences?\nOnyxia is an open-source project that provides a web application for accessing data science services. The Datalab SSP Cloud is an instance of the Onyxia project hosted at Insee."
  },
  {
    "objectID": "content/principles.html#fundamental-principles",
    "href": "content/principles.html#fundamental-principles",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The architecture of Datalab is based on a set of fundamental principles:\n\nData science-oriented production by providing an infrastructure suitable for most use cases and a service catalog covering the entire data project lifecycle.\nChoices that promote user autonomy by avoiding proprietary lock-ins and enabling access to the lower layers of the infrastructure to address advanced and specific needs.\nA project that is 100% cloud-native and also cloud-agnostic, allowing for easy deployment on any infrastructure.\nA completely open-source project, both in terms of its constituent components and its distribution (MIT license)."
  },
  {
    "objectID": "content/principles.html#service-offering",
    "href": "content/principles.html#service-offering",
    "title": "Principles of Datalab",
    "section": "",
    "text": "Datalab can be accessed through a modern and responsive user interface, focused on providing a great user experience. It serves as the technical connection between the different components of Onyxia:\n\nOpen-source technologies that represent the state of the art in container deployment and orchestration, storage, and security.\nA service and tool catalog to support data science projects.\nA training and documentation platform to facilitate onboarding onto the offered technologies.\n\n\n\n\nFundamental building blocks of Datalab Onyxia\n\n\nThe service catalog is designed to accommodate the essential needs of data scientists, from self-service development to production deployment of processes or applications. The entire data project lifecycle is covered, and the catalog of services is regularly expanded to meet users’ new requirements.\n\n\n\nA comprehensive service catalog for data science projects"
  },
  {
    "objectID": "content/principles.html#an-open-project",
    "href": "content/principles.html#an-open-project",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Datalab Onyxia project is resolutely open at multiple levels:\n\nThe Datalab is accessible through its web interface to all public service agents (via AgentConnect or a gouv.fr email address) as well as students from statistical schools linked to Insee (Cefil, Ensai, Ensae).\nThe open-source code and modularity of the project allow for the deployment of a customized Onyxia instance on any Kubernetes-based infrastructure cluster.\nThe project is open to external contributions, whether related to the service catalog, the graphical interface, or the arrangement of the software components it comprises."
  },
  {
    "objectID": "content/services-configuration.html",
    "href": "content/services-configuration.html",
    "title": "Service Configuration",
    "section": "",
    "text": "After clicking on “New service” &gt; “RStudio/Jupyter-python/VScode-python” &gt; “Launch”\n\n\nTo recognize the service and/or the configuration if saved by clicking on the bookmark symbol at the top right. If the name already exists among the saved configurations, saving will overwrite the old configuration.\nConvenient for distinguishing different services of the same type (RStudio, Jupyter…).\n\n\n\nIt is possible to share a service with a group of people by checking the “Share the service” box when opening the service. Other members of the group will see the service and can use it. Creating groups can be done by writing to administrators on Tchap (privately) or by email at innovation@insee.fr, providing the group name, usernames of the members, and whether or not a associated storage space is needed on MinIO.\n\nFor occasional needs, it is also possible to share a service that you have created with another person. Simply provide them with the URL (e.g., https://user-aaaaaaaaaaaaaa-xxxxxxx-x.user.lab.sspcloud.fr/) and the service password. The username remains Onyxia. Please note, it is recommended to change the service password during its launch (in the Security tab) to avoid any leaks. You should also uncheck Enable IP protection and Enable network policy in the Security tab. Only one person at a time can connect to an RStudio service.\n\n\n\n\n\n\n\n\n\n\n\n\nA link to a shell script (sequence of Linux commands) that is executed right after the service is launched. Convenient for automating the setup of certain configurations.\nThis script link must be accessible on the internet, for example, on https://git.lab.sspcloud.fr/ with a public project or on S3 storage with a public file.\nExample of an initialization script that clones a project from a private Gitlab instance, configures global RStudio options, automatically opens the cloned RStudio project, installs and selects French spelling correction, and customizes code snippets (snippets).\nYou can also find some initialization scripts on our dedicated repo on github.\n\n\n\n\n\n\nWarning\n\n\n\nThe script is executed as a superuser (Root), and the files it creates become the property of the superuser. This leads to errors when these files are called, for example, RStudio configuration files. To give normal user rights (named onyxia) to their personal folder\n\n\nchown -R ${USERNAME}:${GROUPNAME} ${HOME}\n\n\n\nOptions to pass to the initialization script, separated by spaces and can be subsequently called with $1, $2, etc.\nFor example, if you enter file1.txt file2.txt in the PersonalInitArgs field and use this initialization script:\n#!/bin/bash\ntouch $1\ntouch $2\nThe script will create two files, file1.txt and file2.txt, using the touch command.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is the password to enter when opening a service, which is provided by “Copy the password” on the service page. It is supplied by the general parameter “Password for your services,” which can be found in “My Account” &gt; “Account Information,” unless a specific one is defined at the service level.\n\n\n\nIf checked, the service is only accessible from a single IP. Uncheck it if you wish to work from different locations.\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page.\n\n\n\n\n\n\nWarning\n\n\n\nIt is not possible to automatically clone a private project from a private instance (i.e., other than gitlab.com and github.com). To do so, you will need to use a shell script as indicated here.\n\n\n\n\nIf checked, configure Git and attempt a clone when the service starts.\n\n\n\nThe name that will appear in the commits (not the Gitlab or Github account username).\n\n\n\nThe email address that will appear in the commits (not necessarily the email associated with the Gitlab or Github account).\n\n\n\n\n\n\nAccess token defined on the platform used (Gitlab, Github…).\n\n\n\nThe URL obtained from the platform used (Gitlab, Github…) by clicking on “Clone” &gt; HTTPS.\nIn the format:\nhttps://github.com/InseeFrLab/docs.sspcloud.fr.git\n\n\n\n\n\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page.\n:::"
  },
  {
    "objectID": "content/services-configuration.html#custom-name",
    "href": "content/services-configuration.html#custom-name",
    "title": "Service Configuration",
    "section": "",
    "text": "To recognize the service and/or the configuration if saved by clicking on the bookmark symbol at the top right. If the name already exists among the saved configurations, saving will overwrite the old configuration.\nConvenient for distinguishing different services of the same type (RStudio, Jupyter…)."
  },
  {
    "objectID": "content/services-configuration.html#share-the-service",
    "href": "content/services-configuration.html#share-the-service",
    "title": "Service Configuration",
    "section": "",
    "text": "It is possible to share a service with a group of people by checking the “Share the service” box when opening the service. Other members of the group will see the service and can use it. Creating groups can be done by writing to administrators on Tchap (privately) or by email at innovation@insee.fr, providing the group name, usernames of the members, and whether or not a associated storage space is needed on MinIO.\n\nFor occasional needs, it is also possible to share a service that you have created with another person. Simply provide them with the URL (e.g., https://user-aaaaaaaaaaaaaa-xxxxxxx-x.user.lab.sspcloud.fr/) and the service password. The username remains Onyxia. Please note, it is recommended to change the service password during its launch (in the Security tab) to avoid any leaks. You should also uncheck Enable IP protection and Enable network policy in the Security tab. Only one person at a time can connect to an RStudio service."
  },
  {
    "objectID": "content/services-configuration.html#init-1",
    "href": "content/services-configuration.html#init-1",
    "title": "Service Configuration",
    "section": "",
    "text": "A link to a shell script (sequence of Linux commands) that is executed right after the service is launched. Convenient for automating the setup of certain configurations.\nThis script link must be accessible on the internet, for example, on https://git.lab.sspcloud.fr/ with a public project or on S3 storage with a public file.\nExample of an initialization script that clones a project from a private Gitlab instance, configures global RStudio options, automatically opens the cloned RStudio project, installs and selects French spelling correction, and customizes code snippets (snippets).\nYou can also find some initialization scripts on our dedicated repo on github.\n\n\n\n\n\n\nWarning\n\n\n\nThe script is executed as a superuser (Root), and the files it creates become the property of the superuser. This leads to errors when these files are called, for example, RStudio configuration files. To give normal user rights (named onyxia) to their personal folder\n\n\nchown -R ${USERNAME}:${GROUPNAME} ${HOME}\n\n\n\nOptions to pass to the initialization script, separated by spaces and can be subsequently called with $1, $2, etc.\nFor example, if you enter file1.txt file2.txt in the PersonalInitArgs field and use this initialization script:\n#!/bin/bash\ntouch $1\ntouch $2\nThe script will create two files, file1.txt and file2.txt, using the touch command."
  },
  {
    "objectID": "content/services-configuration.html#security-1",
    "href": "content/services-configuration.html#security-1",
    "title": "Service Configuration",
    "section": "",
    "text": "This is the password to enter when opening a service, which is provided by “Copy the password” on the service page. It is supplied by the general parameter “Password for your services,” which can be found in “My Account” &gt; “Account Information,” unless a specific one is defined at the service level.\n\n\n\nIf checked, the service is only accessible from a single IP. Uncheck it if you wish to work from different locations."
  },
  {
    "objectID": "content/services-configuration.html#git-1",
    "href": "content/services-configuration.html#git-1",
    "title": "Service Configuration",
    "section": "",
    "text": "To learn how to use this tab, see the dedicated page.\n\n\n\n\n\n\nWarning\n\n\n\nIt is not possible to automatically clone a private project from a private instance (i.e., other than gitlab.com and github.com). To do so, you will need to use a shell script as indicated here.\n\n\n\n\nIf checked, configure Git and attempt a clone when the service starts.\n\n\n\nThe name that will appear in the commits (not the Gitlab or Github account username).\n\n\n\nThe email address that will appear in the commits (not necessarily the email associated with the Gitlab or Github account).\n\n\n\n\n\n\nAccess token defined on the platform used (Gitlab, Github…).\n\n\n\nThe URL obtained from the platform used (Gitlab, Github…) by clicking on “Clone” &gt; HTTPS.\nIn the format:\nhttps://github.com/InseeFrLab/docs.sspcloud.fr.git\n\n\n\n\n\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page.\n:::"
  },
  {
    "objectID": "content/tutorials/choose-materials.html",
    "href": "content/tutorials/choose-materials.html",
    "title": "Which materials should be used for Trainings?",
    "section": "",
    "text": "When designing effective training materials, it’s crucial to select formats that engage learners and enhance comprehension. Even though PDFs, slide decks, and videos all offer valuable ways to present information, we will focus on interactive environments as they are ideal for practical training. Interactive environments allow learners to apply concepts immediately, test code in real time, and actively participate in their learning journey which promotes deeper understanding and skill retention.\nIn the following, we’ll compare different interactive environments according to the language used. Will also be shown a way to deploy a static site on the platform using quarto.\n\n\n🐍 In python, Jupyter Notebooks, also available in Vscode, stand out as the go-to choice. With support for rich media, widgets, and numerous extensions, it allows the integration of interactive graphs, data tables, and even forms for exercises.\n\n\n\nExemple with jupyter\n\n\n\n\n\nExemple with vscode\n\n\n\n\n\n®️ In R, we can either use Rmarkdown Notebooks or LearnR which offers interactive tutorials embedded within R Markdown documents. Both of them combine explanation, code and visualization in an interactive document. However, even if learnR allows to generate interactive and more complexe elements such as quizzes making it ideal for beginner tutorials, it requires the deployment of a Shiny server which can be costly. Moreover, in learnR cells do not communicate across the entire environment hence defining global variables or managing state becomes challenging. This restriction limits the effectiveness of learnr for complex tutorials where a connected, evolving state is necessary to build on previous steps.\nOverall, while learnR provides an interactive environment for R that’s suitable for simple exercises, Rmarkdown Notebooks offer a more robust, flexible, and widely-used solution. For R, traditional R Markdown or local interactive sessions are often more practical for comprehensive training sessions. This is why, for sophisticated training materials that require smooth progression and state management, notebooks remains the preferred choice for Python or R tutorials.\n\n\n\nExemple with rstudio\n\n\n\n\n\n\n\n\nWarning\n\n\n\nHere will lie some explanation on how to deploy a static site using quarto and CI to automate its deployment. It may be moved to another tab."
  },
  {
    "objectID": "content/tutorials/choose-materials.html#using-python",
    "href": "content/tutorials/choose-materials.html#using-python",
    "title": "Which materials should be used for Trainings?",
    "section": "",
    "text": "🐍 In python, Jupyter Notebooks, also available in Vscode, stand out as the go-to choice. With support for rich media, widgets, and numerous extensions, it allows the integration of interactive graphs, data tables, and even forms for exercises.\n\n\n\nExemple with jupyter\n\n\n\n\n\nExemple with vscode"
  },
  {
    "objectID": "content/tutorials/choose-materials.html#using-r",
    "href": "content/tutorials/choose-materials.html#using-r",
    "title": "Which materials should be used for Trainings?",
    "section": "",
    "text": "®️ In R, we can either use Rmarkdown Notebooks or LearnR which offers interactive tutorials embedded within R Markdown documents. Both of them combine explanation, code and visualization in an interactive document. However, even if learnR allows to generate interactive and more complexe elements such as quizzes making it ideal for beginner tutorials, it requires the deployment of a Shiny server which can be costly. Moreover, in learnR cells do not communicate across the entire environment hence defining global variables or managing state becomes challenging. This restriction limits the effectiveness of learnr for complex tutorials where a connected, evolving state is necessary to build on previous steps.\nOverall, while learnR provides an interactive environment for R that’s suitable for simple exercises, Rmarkdown Notebooks offer a more robust, flexible, and widely-used solution. For R, traditional R Markdown or local interactive sessions are often more practical for comprehensive training sessions. This is why, for sophisticated training materials that require smooth progression and state management, notebooks remains the preferred choice for Python or R tutorials.\n\n\n\nExemple with rstudio\n\n\n\n\n\n\n\n\nWarning\n\n\n\nHere will lie some explanation on how to deploy a static site using quarto and CI to automate its deployment. It may be moved to another tab."
  },
  {
    "objectID": "content/tutorials/introduction.html",
    "href": "content/tutorials/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nIn the world of data science and programming, having a flexible and reproducible training environment is essential. For educators and students alike, the ability to quickly set up consistent, ready-to-use environments is a game-changer. Onyxia excels at providing such environments. With just a simple link, users can launch pre-configured training environments, making it ideal for hands-on learning. This tutorial explains why SSP Cloud and notebooks are powerful tools for training, and how to structure and deploy an effective course using these technologies.\n\n\nWhy use SSP Cloud for Training?\nSSP Cloud allows users to deploy cloud-based workspaces easily, without the hassle of local installation or complex configuration. Here’s why it’s particularly useful for training:\n\nInstant Setup: Instructors can share a link that configures everything—environment, software, and even datasets—so learners don’t need to install or configure anything locally.\nConsistency Across Learners: By providing a reproducible environment, everyone works with the same configuration, minimizing setup issues that typically arise with different operating systems and dependencies.\nScalability: SSP Cloud can scale with your needs, offering powerful computing resources for large datasets or compute-intensive tasks. This allows it to support a wide range of training types, from beginner-level Python tutorials to advanced machine learning workflows.\n\n\n\n\n\n\n\nWarning\n\n\n\nSSPCloud does not provide formal service level agreements (SLAs) or guarantees regarding service availability. This means that users should be prepared for potential service interruptions or downtimes."
  },
  {
    "objectID": "content/version-control.html",
    "href": "content/version-control.html",
    "title": "Version Control",
    "section": "",
    "text": "The Datalab is a shared platform where resources used by services are shared among different users. As such, Datalab services operate on the model of ephemeral containers: in a standard usage, the user launches a service, performs data processing, saves the code that was used to perform these processes, and then deletes the service instance. This code backup is greatly facilitated by the use of version control.\nHowever, this performance consideration should not be seen as a constraint: version control is an essential best practice in development. The benefits are numerous, both individually:\n\nThe local project is synchronized with a remote server, making the loss of code almost impossible.\nThe complete history of choices and modifications made on the project is preserved.\nThe user can browse this history to search for modifications that may have caused errors and decide at any time to revert to a previous version of the project or specific files.\n\nand in the context of collaborative projects:\n\nSimultaneous work on the same project is possible without the risk of loss.\nThe user can share their modifications while benefiting from the changes made by others.\nIt becomes possible to contribute to open-source projects, for which the use of Git is widely standard.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial aims to present how version control can be easily implemented using the tools available on the Datalab. It does not explain the workings of Git and presupposes a certain familiarity with the tool. Many online resources can serve as an introduction; for example, R users can consult this training program, and Python users can refer to this chapter of the course.\n\n\n\n\n\n\n\nAlthough offline use of Git is possible, the real benefit of version control lies in synchronizing the local copy of a project (clone) with a remote repository (remote). Various software forge services allow this synchronization of Git projects, the most well-known of which are GitHub and GitLab. Since GitHub has much more visibility today — for example, the repositories of Insee, InseeFr, and InseeFrLab, are on GitHub — Datalab offers an integration with GitHub, which we present in this tutorial.\n\n\n\n\n\n\nWarning\n\n\n\nThe rest of the tutorial requires having a GitHub account.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile using Datalab with the GitHub platform is facilitated, it is by no means mandatory: it is still possible to use the software forge of your choice for project synchronization. A forge based on GitLab is also available to Datalab users.\n\n\n\n\n\nSynchronization with a remote repository requires authentication with GitHub. This is done using a personal access token, which must be generated from the user’s GitHub account. The generation service is accessible at this address. The GitHub documentation provides illustrations to guide the process.\nTo generate a token, it is necessary to choose a token name, an expiration date, and access rights (scope). It is recommended to choose a short expiration date (30 days) and limited access (repo only) to reduce security risks in case the token is maliciously exposed.\n\n\n\nRecommended configuration for generating a GitHub access token\n\n\nOnce the token is generated, it will be displayed on the screen. A token can only be viewed once; if lost, a new one must be generated.\n\n\n\nIt is recommended to add access tokens to a password manager. Alternatively, the token can be added to the “External Services” configuration of the user account on Datalab, which allows the token to be directly accessible within the services offered on the platform.\n\n\n\nAdding a GitHub access token to a user account on Datalab\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to use the email address associated with your GitHub account in the “Account Information.” It is this address that effectively links the commits you make to your GitHub account.\n\n\n\n\n\n\nGit is pre-configured to work natively with the relevant Datalab services. When opening a service, it is possible to configure certain elements. If you have added a GitHub access token to your Datalab account, it will be pre-configured. It is also possible to indicate the complete URL of a Git repository (e.g., https://github.com/InseeFrLab/onyxia), which will then be cloned into the instance’s workspace during initialization.\n\n\n\nConfiguring Git when opening a service\n\n\n\n\nThe GitHub access token is available in the terminals of different services via the environment variable $GIT_PERSONAL_ACCESS_TOKEN. To avoid having to authenticate for every operation involving the remote repository (clone, push, and pull), it is recommended to clone it, including the access token in the HTTPS link, using the following command:\ngit clone https://${GIT_PERSONAL_ACCESS_TOKEN}@github.com/&lt;owner&gt;/&lt;repo&gt;.git\nwhere  and  should be replaced with the GitHub username and repository name, respectively.\n\n\n\nThe main code production services available on Datalab have a graphical interface to facilitate the use of Git:\n\nRStudio: RStudio provides a native and fairly comprehensive graphical interface for Git. The utilitR documentation presents its operation in detail.\nJupyter: the jupyterlab-git plugin provides a (fairly basic) interface between Jupyter and Git.\nVSCode: VSCode natively offers a well-integrated graphical interface with Git and GitHub. A detailed documentation presents the possibilities of the tool.\n\n\n\n\n\n\n\nWarning\n\n\n\nGraphical interfaces make it easier to get started with Git but never completely replace the use of the tool via a terminal due to necessarily imperfect integration. Therefore, it is useful to familiarize yourself with using Git via the terminal as early as possible."
  },
  {
    "objectID": "content/version-control.html#why-use-version-control",
    "href": "content/version-control.html#why-use-version-control",
    "title": "Version Control",
    "section": "",
    "text": "The Datalab is a shared platform where resources used by services are shared among different users. As such, Datalab services operate on the model of ephemeral containers: in a standard usage, the user launches a service, performs data processing, saves the code that was used to perform these processes, and then deletes the service instance. This code backup is greatly facilitated by the use of version control.\nHowever, this performance consideration should not be seen as a constraint: version control is an essential best practice in development. The benefits are numerous, both individually:\n\nThe local project is synchronized with a remote server, making the loss of code almost impossible.\nThe complete history of choices and modifications made on the project is preserved.\nThe user can browse this history to search for modifications that may have caused errors and decide at any time to revert to a previous version of the project or specific files.\n\nand in the context of collaborative projects:\n\nSimultaneous work on the same project is possible without the risk of loss.\nThe user can share their modifications while benefiting from the changes made by others.\nIt becomes possible to contribute to open-source projects, for which the use of Git is widely standard.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial aims to present how version control can be easily implemented using the tools available on the Datalab. It does not explain the workings of Git and presupposes a certain familiarity with the tool. Many online resources can serve as an introduction; for example, R users can consult this training program, and Python users can refer to this chapter of the course."
  },
  {
    "objectID": "content/version-control.html#integrating-github-with-datalab",
    "href": "content/version-control.html#integrating-github-with-datalab",
    "title": "Version Control",
    "section": "",
    "text": "Although offline use of Git is possible, the real benefit of version control lies in synchronizing the local copy of a project (clone) with a remote repository (remote). Various software forge services allow this synchronization of Git projects, the most well-known of which are GitHub and GitLab. Since GitHub has much more visibility today — for example, the repositories of Insee, InseeFr, and InseeFrLab, are on GitHub — Datalab offers an integration with GitHub, which we present in this tutorial.\n\n\n\n\n\n\nWarning\n\n\n\nThe rest of the tutorial requires having a GitHub account.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile using Datalab with the GitHub platform is facilitated, it is by no means mandatory: it is still possible to use the software forge of your choice for project synchronization. A forge based on GitLab is also available to Datalab users.\n\n\n\n\n\nSynchronization with a remote repository requires authentication with GitHub. This is done using a personal access token, which must be generated from the user’s GitHub account. The generation service is accessible at this address. The GitHub documentation provides illustrations to guide the process.\nTo generate a token, it is necessary to choose a token name, an expiration date, and access rights (scope). It is recommended to choose a short expiration date (30 days) and limited access (repo only) to reduce security risks in case the token is maliciously exposed.\n\n\n\nRecommended configuration for generating a GitHub access token\n\n\nOnce the token is generated, it will be displayed on the screen. A token can only be viewed once; if lost, a new one must be generated.\n\n\n\nIt is recommended to add access tokens to a password manager. Alternatively, the token can be added to the “External Services” configuration of the user account on Datalab, which allows the token to be directly accessible within the services offered on the platform.\n\n\n\nAdding a GitHub access token to a user account on Datalab\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to use the email address associated with your GitHub account in the “Account Information.” It is this address that effectively links the commits you make to your GitHub account."
  },
  {
    "objectID": "content/version-control.html#using-git-with-datalab-services",
    "href": "content/version-control.html#using-git-with-datalab-services",
    "title": "Version Control",
    "section": "",
    "text": "Git is pre-configured to work natively with the relevant Datalab services. When opening a service, it is possible to configure certain elements. If you have added a GitHub access token to your Datalab account, it will be pre-configured. It is also possible to indicate the complete URL of a Git repository (e.g., https://github.com/InseeFrLab/onyxia), which will then be cloned into the instance’s workspace during initialization.\n\n\n\nConfiguring Git when opening a service\n\n\n\n\nThe GitHub access token is available in the terminals of different services via the environment variable $GIT_PERSONAL_ACCESS_TOKEN. To avoid having to authenticate for every operation involving the remote repository (clone, push, and pull), it is recommended to clone it, including the access token in the HTTPS link, using the following command:\ngit clone https://${GIT_PERSONAL_ACCESS_TOKEN}@github.com/&lt;owner&gt;/&lt;repo&gt;.git\nwhere  and  should be replaced with the GitHub username and repository name, respectively.\n\n\n\nThe main code production services available on Datalab have a graphical interface to facilitate the use of Git:\n\nRStudio: RStudio provides a native and fairly comprehensive graphical interface for Git. The utilitR documentation presents its operation in detail.\nJupyter: the jupyterlab-git plugin provides a (fairly basic) interface between Jupyter and Git.\nVSCode: VSCode natively offers a well-integrated graphical interface with Git and GitHub. A detailed documentation presents the possibilities of the tool.\n\n\n\n\n\n\n\nWarning\n\n\n\nGraphical interfaces make it easier to get started with Git but never completely replace the use of the tool via a terminal due to necessarily imperfect integration. Therefore, it is useful to familiarize yourself with using Git via the terminal as early as possible."
  }
]