# Data Storage

## Principles

The file storage solution associated with Datalab is [MinIO](https://min.io), an object storage system based on the cloud, compatible with Amazon's S3 API. In practice, this has several advantages:

* Stored files are easily accessible from anywhere: a file can be accessed directly via a simple URL, which can be shared.
* It is possible to access the stored files directly within the data science services (R, Python, etc.) offered on Datalab, without the need to copy the files locally beforehand, greatly improving the reproducibility of analyses.

## Managing Your Data

### Importing Data

The [My Files](https://datalab.sspcloud.fr/mes-fichiers/) page in Datalab takes the form of a file explorer showing the different buckets (repositories) to which the user has access.

Each user has a personal bucket by default to store their files. Within this bucket, two options are possible:

* "**Create a directory**": Creates a directory in the current bucket/directory hierarchically, similar to a traditional file system.
* "**Upload a file**": Uploads one or multiple files to the current directory.

::: {.callout-note} 
The graphical interface for data storage in Datalab is still under construction. As such, it may experience responsiveness issues. For frequent operations on file storage, it may be preferable to interact with MinIO via the terminal. 
:::

### Sharing Data

By clicking on a file in their personal bucket, the user can access its characteristics page. On this page, it is possible to **change the distribution status of the file**. Changing the status of the file from "private" to "public" generates a **distribution link**, which can then be shared for downloading the file. The "public" status only grants read rights to other users, and modifying or deleting other users' personal files is not possible.

To simplify the sharing of multiple files for, say, a training session, it is possible to create a **"distribution" folder** in the user's personal bucket. By default, all files present in this folder have a public distribution status.

::: 
{.callout-note} For collaborative projects, it can be beneficial for different participants to have access to a shared storage space. It is possible to create shared buckets on MinIO for this purpose. Feel free to contact us via the channels specified on the "[First Use](premiere-utilisation.md)" page if you wish to work on open data projects on Datalab. 
:::

::: {.callout-warning} 
In accordance with the [terms of use](https://www.sspcloud.fr/tos_fr.md), only *open data*\-type or non-sensitive data can be stored on Datalab. Having a file with a "private" distribution status does not guarantee perfect confidentiality. 
:::

## Using Data Stored on MinIO

The access credentials needed to access data on MinIO are pre-configured in the various Datalab services, accessible in the form of [environment variables](gestion-des-secrets.md). This greatly facilitates importing and exporting files from the services.

### Configuration

::: {.panel-tabset}

#### R

In R, interaction with an S3-compatible file system is made possible by the `aws.s3` library.

```r
library(aws.s3)
```

#### Python

In Python, interaction with an S3-compatible file system is made possible by two libraries:

* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), a library created and maintained by Amazon.
* [S3Fs](https://s3fs.readthedocs.io/en/latest/), a library that allows interaction with stored files similar to a classic filesystem.

For this reason, and because S3Fs is used by default by the [pandas](https://pandas.pydata.org) library to manage S3 connections, we will present how to manage storage on MinIO via Python using this library.

```python
import os
import s3fs

# Create filesystem object
S3_ENDPOINT_URL = "https://" + os.environ["AWS_S3_ENDPOINT"]
fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})
```

#### mc

MinIO offers a command-line client (`mc`) that allows interaction with the storage system in a manner similar to a classic UNIX filesystem. This client is installed by default and accessible via a terminal in the various Datalab services.

The MinIO client offers basic UNIX commands such as ls, cat, cp, etc. The complete list is available in the [client documentation](https://docs.min.io/docs/minio-client-complete-guide.html).

:::

**Listing the files in a bucket**

::: {.panel-tabset}

#### R

```r
aws.s3::get_bucket("donnees-insee", region = "")
```

#### Python

```python
fs.ls("donnees-insee")
```

#### mc

The Datalab storage is accessible via the alias `s3`. For example, to list the files in the bucket `donnees-insee`:

```bash
mc ls s3/donnees-insee
```

:::

**Importing Data**

::: {.panel-tabset}

#### R

```r
BUCKET <- "donnees-insee"
FILE_KEY_S3 <- "diffusion/BPE/2019/BPE_ENS.csv"

df <- 
  aws.s3::s3read_using(
    FUN = readr::read_delim,
    # Put FUN options here
    delim = ";",
    object = FILE_KEY_S3,
    bucket = BUCKET,
    opts = list("region" = "")
  )
```

#### Python

The S3Fs package allows you to interact with files stored on MinIO as if they were local files. The syntax is therefore very familiar to Python users. For example, to import/export tabular data via `pandas`:

```python
import pandas as pd

BUCKET = "donnees-insee"
FILE_KEY_S3 = "diffusion/BPE/2019/BPE_ENS.csv"
FILE_PATH_S3 = BUCKET + "/" + FILE_KEY_S3

with fs.open(FILE_PATH_S3, mode="rb") as file_in:
    df_bpe = pd.read_csv(file_in, sep=";")
```

#### mc

To copy data from a MinIO bucket to the local service:

```bash
mc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv
```

::: {.callout-warning} 
**Copying files to the local service is generally not a good practice**: it limits the reproducibility of analyses and becomes quickly impossible with large volumes of data. Therefore, it is preferable to get into the habit of importing data directly into `R`/`Python`. 
:::

:::

**Exporting Data to MinIO**

::: {.panel-tabset}

#### R

```r
BUCKET_OUT = "<my_bucket>"
FILE_KEY_OUT_S3 = "my_folder/BPE_ENS.csv"

aws.s3::s3write_using(
    df,
    FUN = readr::write_csv,
    object = FILE_KEY_OUT_S3,
    bucket = BUCKET_OUT,
    opts = list("region" = "")
)
```

#### Python

```python
BUCKET_OUT = "<my_bucket>"
FILE_KEY_OUT_S3 = "my_folder/BPE_ENS.csv"
FILE_PATH_OUT_S3 = BUCKET_OUT + "/" + FILE_KEY_OUT_S3

with fs.open(FILE_PATH_OUT_S3, 'w') as file_out:
    df_bpe.to_csv(file_out)
```

#### mc

To copy data from the local service to a bucket on MinIO:

```bash
mc cp local/path/to/my/file.csv s3/<my_bucket>/remote/path/to/my/file.csv
```

:::

**Renewing Expired Access Tokens**

Access to MinIO storage is possible via a personal access token, which is valid for 24 hours and automatically regenerated at regular intervals on SSP Cloud. When a token has expired, services created before the expiration date (using the previous token) can no longer access storage, and the affected service will appear in red on the [My Services](https://datalab.sspcloud.fr/my-services) page. In this case, there are two options:

* Open a new service on Datalab, which will have a default, up-to-date token.
* Manually replace expired tokens with new ones. Scripts indicating how to do this for different MinIO uses (`R`/`Python`/`mc`) are available [here](https://datalab.sspcloud.fr/account/storage). Simply choose the relevant script and execute it in your current working environment.
