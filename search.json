[
  {
    "objectID": "docs/fr/version-control.html",
    "href": "docs/fr/version-control.html",
    "title": "Contrôle de version",
    "section": "",
    "text": "Le Datalab est une plateforme mutualisée : les ressources utilisées par les services sont partagées entre les différents utilisateurs. A ce titre, les services du Datalab fonctionnent sur le modèle des conteneurs éphémères : dans un usage standard, l’utilisateur lance un service, réalise des traitements de données, sauvegarde le code qui a permis de réaliser ces traitements, et supprime l’instance du service. Cette sauvegarde du code est grandement facilitée par l’usage du contrôle de version.\nCette considération de performance ne doit cependant pas être vue comme une contrainte : le contrôle de version est une bonne pratique essentielle de développement. Les bénéfices sont nombreux, aussi bien à titre individuel :\n\nle projet local est synchronisé avec un serveur distant, rendant la perte de code quasi impossible ;\nl’historique complet des choix et modifications effectuées sur le projet est conservé ;\nl’utilisateur peut parcourir cet historique pour rechercher les modifications qui ont pu créer des erreurs, et décider à tout moment de revenir à une version précédente du projet, ou bien de certains fichiers.\n\nque dans le cadre de projets collaboratifs :\n\nle travail simultané sur un même projet est possible, sans risque de perte ;\nl’utilisateur peut partager ses modifications tout en bénéficiant de celles des autres ;\nil devient possible de contribuer à des projets open-source, pour lesquels l’usage de Git est très largement standard.\n\n\n\n\n\n\n\nWarning\n\n\n\nCe tutoriel vise à présenter comment le contrôle de version peut être facilement implémenté grâce aux outils présents sur le Datalab. Il ne présente pas le fonctionnement de Git et présuppose donc une certaine familiarité avec l’outil. De nombreuses ressources en ligne peuvent servir d’introduction ; l’utilisateur de R pourra par exemple consulter ce guide et l’utilisateur de Python ce chapitre de cours. Une formation complète à Git sera bientôt proposée dans l’espace formationdu Datalab.\n\n\n\n\n\n\n\nBien qu’une utilisation hors-ligne de Git soit possible, tout l’intérêt du contrôle de version réside dans la synchronisation de la copie locale d’un projet (clone) avec un dépôt distant (remote). Différents services de forge logicielle permettent cette synchronisation des projets Git, dont les plus connus sont GitHubet GitLab. Dans la mesure où le premier dispose aujourd’hui de beaucoup plus de visibilité — par exemple, les dépôts de l’Insee, InseeFret InseeFrLab, sont sur GitHub — le Datalab propose une intégration facilitée avec GitHub, que nous présentons à travers ce tutoriel.\n\n\n\n\n\n\nWarning\n\n\n\nLa suite du tutoriel nécessite de disposer d’un compte GitHub.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi l’utilisation du Datalab avec la plateforme GitHub est facilitée, elle n’est en aucun cas obligatoire : il reste tout à fait possible d’utiliser la forge logicielle de son choix pour la synchronisation des projets. Une forge basée sur GitLabest notamment mise à disposition des utilisateurs du Datalab.\n\n\n\n\n\nLa synchronisation avec un dépôt distant nécessite une authentification auprès de GitHub. Celle-ci s’effectue à l’aide d’un jeton d’accès personnel, qui doit être généré à partir du compte GitHub de l’utilisateur. Le service de génération est accessible à cette adresse. La documentation GitHub(en Anglais) propose des illustrations pour guider le processus.\nPour générer un jeton, il est nécessaire de choisir un nom de jeton, un délai d’expiration et des droits d’accès (scope). Il est recommandé de choisir un délai court (30 jours) et un accès restreint (repo seulement) afin de limiter les risques de sécurité en cas de diffusion malveillante du jeton.\n\n\n\nConfiguration recommandée pour la génération d’un jeton d’accès GitHub\n\n\nUne fois le jeton généré, ce dernier apparaît à l’écran. Un jeton ne peut être visualisé qu’une seule fois ; en cas de perte, il faudra en générer un nouveau.\n\n\n\nIl est recommandé d’ajouter ses jetons d’accès à un gestionnaire de mots de passe. Le jeton peut également être ajouté à la configuration “Services externes” du compte utilisateur sur le Datalab, ce qui permet au jeton d’être directement accessible au sein des services proposés sur la plateforme.\n\n\n\nAjouter un jeton d’accès GitHub à un compte utilisateur sur le Datalab\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAttention à bien utiliser dans les “Informations du compte” l’adresse mail associée à votre compte GitHub, c’est elle qui permet de lier effectivement les commits que vous effectuerez à votre compte GitHub.\n\n\n\n\n\n\nGit est préconfiguré pour fonctionner nativement avec les différents services pertinents du Datalab. A l’ouverture d’un service, il est possible de configurer certains éléments. Si l’on a ajouté un jeton d’accès GitHub à son compte sur le Datalab, ce dernier est pré-configuré. Il est par ailleurs possible d’indiquer l’URL complète d’un Repository Git (ex : https://github.com/InseeFrLab/onyxia), qui sera alors cloné à l’initialisation dans l’espace de travail de l’instance.\n\n\n\nConfiguration de Git à l’ouverture d’un service\n\n\n\n\nLe jeton d’accès GitHub est disponible dans le terminal des différents services via la variable d’environnement $GIT_PERSONAL_ACCESS_TOKEN. Afin d’éviter de devoir s’authentifier à chaque opération impliquant le dépôt distant (clone, push & pull), il est recommandé de cloner celui-ci en incluant le jeton d’accès dans le lien HTTPS, à l’aide de la commande suivante :\ngit clone https://${GIT_PERSONAL_ACCESS_TOKEN}@github.com/&lt;owner&gt;/&lt;repo&gt;.git\noù &lt;owner&gt; et &lt;repo&gt; sont à remplacer respectivement par le nom d’utilisateur et le nom du dépôt GitHub.\n\n\n\nLes principaux services de production de code disponibles sur le Datalab disposent d’une interface graphique pour faciliter l’utilisation de Git :\n\nRStudio : RStudio propose une interface graphique pour Git native et assez complète. La documentation utilitR présente son fonctionnement en détail ;\nJupyter : le plugin jupyterlab-gitpermet un interfaçage (assez sommaire) de Jupyter avec Git ;\nVSCode : VSCode propose nativement une interface graphique très bien intégrée avec Git et GitHub. Une documentation détaillée(en Anglais) présente les possibilités de l’outil.\n\n\n\n\n\n\n\nWarning\n\n\n\nLes interfaces graphiques facilitent la prise en main de Git, mais ne remplacent jamais complètement l’usage de l’outil via un terminal du fait d’une intégration nécessairement imparfaite. Il est donc utile de se familiariser avec l’usage de Git via le terminal le plus tôt possible."
  },
  {
    "objectID": "docs/fr/version-control.html#pourquoi-utiliser-le-contrôle-de-version",
    "href": "docs/fr/version-control.html#pourquoi-utiliser-le-contrôle-de-version",
    "title": "Contrôle de version",
    "section": "",
    "text": "Le Datalab est une plateforme mutualisée : les ressources utilisées par les services sont partagées entre les différents utilisateurs. A ce titre, les services du Datalab fonctionnent sur le modèle des conteneurs éphémères : dans un usage standard, l’utilisateur lance un service, réalise des traitements de données, sauvegarde le code qui a permis de réaliser ces traitements, et supprime l’instance du service. Cette sauvegarde du code est grandement facilitée par l’usage du contrôle de version.\nCette considération de performance ne doit cependant pas être vue comme une contrainte : le contrôle de version est une bonne pratique essentielle de développement. Les bénéfices sont nombreux, aussi bien à titre individuel :\n\nle projet local est synchronisé avec un serveur distant, rendant la perte de code quasi impossible ;\nl’historique complet des choix et modifications effectuées sur le projet est conservé ;\nl’utilisateur peut parcourir cet historique pour rechercher les modifications qui ont pu créer des erreurs, et décider à tout moment de revenir à une version précédente du projet, ou bien de certains fichiers.\n\nque dans le cadre de projets collaboratifs :\n\nle travail simultané sur un même projet est possible, sans risque de perte ;\nl’utilisateur peut partager ses modifications tout en bénéficiant de celles des autres ;\nil devient possible de contribuer à des projets open-source, pour lesquels l’usage de Git est très largement standard.\n\n\n\n\n\n\n\nWarning\n\n\n\nCe tutoriel vise à présenter comment le contrôle de version peut être facilement implémenté grâce aux outils présents sur le Datalab. Il ne présente pas le fonctionnement de Git et présuppose donc une certaine familiarité avec l’outil. De nombreuses ressources en ligne peuvent servir d’introduction ; l’utilisateur de R pourra par exemple consulter ce guide et l’utilisateur de Python ce chapitre de cours. Une formation complète à Git sera bientôt proposée dans l’espace formationdu Datalab."
  },
  {
    "objectID": "docs/fr/version-control.html#intégration-de-github-avec-le-datalab",
    "href": "docs/fr/version-control.html#intégration-de-github-avec-le-datalab",
    "title": "Contrôle de version",
    "section": "",
    "text": "Bien qu’une utilisation hors-ligne de Git soit possible, tout l’intérêt du contrôle de version réside dans la synchronisation de la copie locale d’un projet (clone) avec un dépôt distant (remote). Différents services de forge logicielle permettent cette synchronisation des projets Git, dont les plus connus sont GitHubet GitLab. Dans la mesure où le premier dispose aujourd’hui de beaucoup plus de visibilité — par exemple, les dépôts de l’Insee, InseeFret InseeFrLab, sont sur GitHub — le Datalab propose une intégration facilitée avec GitHub, que nous présentons à travers ce tutoriel.\n\n\n\n\n\n\nWarning\n\n\n\nLa suite du tutoriel nécessite de disposer d’un compte GitHub.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSi l’utilisation du Datalab avec la plateforme GitHub est facilitée, elle n’est en aucun cas obligatoire : il reste tout à fait possible d’utiliser la forge logicielle de son choix pour la synchronisation des projets. Une forge basée sur GitLabest notamment mise à disposition des utilisateurs du Datalab.\n\n\n\n\n\nLa synchronisation avec un dépôt distant nécessite une authentification auprès de GitHub. Celle-ci s’effectue à l’aide d’un jeton d’accès personnel, qui doit être généré à partir du compte GitHub de l’utilisateur. Le service de génération est accessible à cette adresse. La documentation GitHub(en Anglais) propose des illustrations pour guider le processus.\nPour générer un jeton, il est nécessaire de choisir un nom de jeton, un délai d’expiration et des droits d’accès (scope). Il est recommandé de choisir un délai court (30 jours) et un accès restreint (repo seulement) afin de limiter les risques de sécurité en cas de diffusion malveillante du jeton.\n\n\n\nConfiguration recommandée pour la génération d’un jeton d’accès GitHub\n\n\nUne fois le jeton généré, ce dernier apparaît à l’écran. Un jeton ne peut être visualisé qu’une seule fois ; en cas de perte, il faudra en générer un nouveau.\n\n\n\nIl est recommandé d’ajouter ses jetons d’accès à un gestionnaire de mots de passe. Le jeton peut également être ajouté à la configuration “Services externes” du compte utilisateur sur le Datalab, ce qui permet au jeton d’être directement accessible au sein des services proposés sur la plateforme.\n\n\n\nAjouter un jeton d’accès GitHub à un compte utilisateur sur le Datalab\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAttention à bien utiliser dans les “Informations du compte” l’adresse mail associée à votre compte GitHub, c’est elle qui permet de lier effectivement les commits que vous effectuerez à votre compte GitHub."
  },
  {
    "objectID": "docs/fr/version-control.html#utiliser-git-avec-les-services-du-datalab",
    "href": "docs/fr/version-control.html#utiliser-git-avec-les-services-du-datalab",
    "title": "Contrôle de version",
    "section": "",
    "text": "Git est préconfiguré pour fonctionner nativement avec les différents services pertinents du Datalab. A l’ouverture d’un service, il est possible de configurer certains éléments. Si l’on a ajouté un jeton d’accès GitHub à son compte sur le Datalab, ce dernier est pré-configuré. Il est par ailleurs possible d’indiquer l’URL complète d’un Repository Git (ex : https://github.com/InseeFrLab/onyxia), qui sera alors cloné à l’initialisation dans l’espace de travail de l’instance.\n\n\n\nConfiguration de Git à l’ouverture d’un service\n\n\n\n\nLe jeton d’accès GitHub est disponible dans le terminal des différents services via la variable d’environnement $GIT_PERSONAL_ACCESS_TOKEN. Afin d’éviter de devoir s’authentifier à chaque opération impliquant le dépôt distant (clone, push & pull), il est recommandé de cloner celui-ci en incluant le jeton d’accès dans le lien HTTPS, à l’aide de la commande suivante :\ngit clone https://${GIT_PERSONAL_ACCESS_TOKEN}@github.com/&lt;owner&gt;/&lt;repo&gt;.git\noù &lt;owner&gt; et &lt;repo&gt; sont à remplacer respectivement par le nom d’utilisateur et le nom du dépôt GitHub.\n\n\n\nLes principaux services de production de code disponibles sur le Datalab disposent d’une interface graphique pour faciliter l’utilisation de Git :\n\nRStudio : RStudio propose une interface graphique pour Git native et assez complète. La documentation utilitR présente son fonctionnement en détail ;\nJupyter : le plugin jupyterlab-gitpermet un interfaçage (assez sommaire) de Jupyter avec Git ;\nVSCode : VSCode propose nativement une interface graphique très bien intégrée avec Git et GitHub. Une documentation détaillée(en Anglais) présente les possibilités de l’outil.\n\n\n\n\n\n\n\nWarning\n\n\n\nLes interfaces graphiques facilitent la prise en main de Git, mais ne remplacent jamais complètement l’usage de l’outil via un terminal du fait d’une intégration nécessairement imparfaite. Il est donc utile de se familiariser avec l’usage de Git via le terminal le plus tôt possible."
  },
  {
    "objectID": "docs/fr/secrets.html",
    "href": "docs/fr/secrets.html",
    "title": "Gestion des secrets",
    "section": "",
    "text": "Gestion des secrets\n\nLes variables d’environnement;\nIl arrive que certaines informations doivent être mise à disposition d’un grand nombre d’applications, ou ne doivent pas figurer en clair dans votre code (jetons d’accès, mots de passe, etc.). L’utilisation de variables d’environnement permet de pouvoir accéder à ces informations depuis n’importe quel service.\nAu lancement d’un service, plusieurs variables d’environnement sont déjà injectées automatiquement — par exemple, les tokens d’accès à ``\n\n\n\nCréation et gestion de secrets\nSur la plateforme, les variables d’environnement sont des secrets écrits dans Vault (le coffre fort du Datalab) et sont chiffrées. Cela vous permet d’y stocker des jetons, des identifiants et des mots de passe. La page Mes secrets prends la forme d’un explorateur de fichiers où vous pouvez trier et hiérarchiser vos variables dans des dossiers.\n\nPour commencer :\n\nCréez un nouveau dossier + Nouveau dossier\nPuis dans ce dossier, créez un nouveau secret + Nouveau secret\nOuvrez votre secret\n\n\nChaque secret peut contenir plusieurs variables, composés de paires de clés-valeurs.\n\n+ Ajouter une variable\n\n\n\n\n\n\n\n\nNote\n\n\n\nLes clés (nom de la variable) commencent toujours par$et contiennent uniquement des lettres, des chiffres et le caractère de soulignement (_). Par convention, les clefs s’écrivent en MAJUSCULE.\n\n\nRemplissez le champ du nom de la clef puis sa valeur.\n\n\n\nConvertir des secrets en variables d’environnement\nUne fois votre secret édité, avec ses différentes variables, vous êtes prêt à l’utiliser dans votre service.\n\nCopiez le chemin du secret en cliquant sur le bouton Utiliser dans un service\nPuis au moment de la configuration de votre service, allez dans l’onglet Vaultet collez le chemin du secret dans le champ dédié\n\n\n\nCréez et ouvrez votre service\n\nPour vérifier que vos variables d’environnement ont bien été crées, vous pouvez lancer les commandes suivantes dans le terminal du service :\n# Lister toutes les variables d'environnement disponibles\nenv \n\n# Afficher la valeur d'une variable d'environnement\necho $MA_VARIABLE \n\n# Trouver toutes les variables d'environnement qui contiennent un pattern donné\nenv | grep -i \"&lt;PATTERN&gt;\""
  },
  {
    "objectID": "docs/fr/discover.html",
    "href": "docs/fr/discover.html",
    "title": "Première utilisation",
    "section": "",
    "text": "Bienvenue sur le Datalab Onyxia, plateforme libre service mutualisée de traitement de données, destinée aux statisticiens et data scientists de l’Etat. Ce tutoriel propose une visite guidée du Datalab pour être rapidement opérationnel dans l’utilisation de ses services.\n\n\n\n\n\n\nWarning\n\n\n\nLes conditions d’utilisation du Datalab sont consultables à cette adresse. Nous rappelons que le Datalab est destiné exclusivement au traitement de données publiques et non-sensibles. Des projets d’expérimentation mobilisant des données non ouvertes peuvent être menés en concertation avec l’équipe du Datalab, sous réserve de se conformer aux règles de sécurité spécifiques au projet.\n\n\n\n\nLe catalogue de services est au centre de l’utilisation du Datalab. Il propose un ensemble de services destinés aux traitements statistiques de données ainsi qu’à la gestion complète des projets de data science.\n\n\n\nPour lancer un service, il suffit de cliquer sur le bouton Lancer du service désiré\nUne page centrée sur le service demandé s’ouvre alors, qui offre plusieurs possibilités :\n\ncliquer à nouveau sur le bouton Lancer pour lancer le service avec sa configuration par défaut ;\npersonnaliser le nom que portera l’instance une fois le service lancé ;\ndérouler un menu de configuration afin de personnaliser la configuration du service avant de le lancer ;\nsauvegarder une configuration personnalisée en cliquant sur le signet en haut à droite du service.\n\n\nLa configuration précise des services du Datalab constitue un usage avancé et n’est donc pas traité dans ce tutoriel, mais dans d’autres pages de ce site documentaire.\n\n\n\n\nL’action de lancer un service amène automatiquement sur la page Mes Services, où sont listées toutes les instances en activité sur le compte de l’utilisateur.\n\nUne fois le service lancé, un bouton Ouvrir apparaît qui permet l’accès au service. Un mot de passe — et, selon les services, un nom d’utilisateur — est généralement requis pour pouvoir utiliser le service. Ces informations sont disponibles dans le README associé au service, auquel on accède en cliquant sur le bouton du même nom.\n\n\n\nSupprimer une instance d’un service s’effectue simplement en cliquant sur l’icône en forme de poubelle en dessous de l’instance.\n\n\n\n\n\n\nCaution\n\n\n\nPour certains services, la suppression d’une instance entraîne la suppression de toutes les données associées, et cette action est irrémédiable. Il est donc nécessaire de toujours bien lire le README associé à l’instance, qui précise les conséquences d’une suppression de l’instance. De manière générale, il est très important de s’assurer que les données ainsi que le code utilisés sont sauvegardés avant de supprimer l’instance. L’idéal est de versionner son code avec Git et de procéder à des sauvegardes régulières des données à l’aide du système de stockage S3.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nLes ressources mises à disposition pour l’execution des services sont partagées entre les différents utilisateurs du Datalab. Veuillez à ne pas laisser en cours des services dont vous ne faites plus l’usage. Nous procédons parfois à une suppression systématique des instances inactives depuis un certain temps, afin de libérer des ressources.\n\n\n\n\n\n\nLe support et l’aide à l’utilisation du Datalab sont effectuées sur un salon dédié du service de messagerie instantanée interministériel Tchap. Toute question sur l’utilisation du Datalab ou suggestion d’amélioration y sont les bienvenues.\nPour les agents n’utilisant pas Tchap, il est également possible de nous contacter par mail sur la BAL innovation@insee.fr."
  },
  {
    "objectID": "docs/fr/discover.html#le-catalogue-de-services",
    "href": "docs/fr/discover.html#le-catalogue-de-services",
    "title": "Première utilisation",
    "section": "",
    "text": "Le catalogue de services est au centre de l’utilisation du Datalab. Il propose un ensemble de services destinés aux traitements statistiques de données ainsi qu’à la gestion complète des projets de data science.\n\n\n\nPour lancer un service, il suffit de cliquer sur le bouton Lancer du service désiré\nUne page centrée sur le service demandé s’ouvre alors, qui offre plusieurs possibilités :\n\ncliquer à nouveau sur le bouton Lancer pour lancer le service avec sa configuration par défaut ;\npersonnaliser le nom que portera l’instance une fois le service lancé ;\ndérouler un menu de configuration afin de personnaliser la configuration du service avant de le lancer ;\nsauvegarder une configuration personnalisée en cliquant sur le signet en haut à droite du service.\n\n\nLa configuration précise des services du Datalab constitue un usage avancé et n’est donc pas traité dans ce tutoriel, mais dans d’autres pages de ce site documentaire.\n\n\n\n\nL’action de lancer un service amène automatiquement sur la page Mes Services, où sont listées toutes les instances en activité sur le compte de l’utilisateur.\n\nUne fois le service lancé, un bouton Ouvrir apparaît qui permet l’accès au service. Un mot de passe — et, selon les services, un nom d’utilisateur — est généralement requis pour pouvoir utiliser le service. Ces informations sont disponibles dans le README associé au service, auquel on accède en cliquant sur le bouton du même nom.\n\n\n\nSupprimer une instance d’un service s’effectue simplement en cliquant sur l’icône en forme de poubelle en dessous de l’instance.\n\n\n\n\n\n\nCaution\n\n\n\nPour certains services, la suppression d’une instance entraîne la suppression de toutes les données associées, et cette action est irrémédiable. Il est donc nécessaire de toujours bien lire le README associé à l’instance, qui précise les conséquences d’une suppression de l’instance. De manière générale, il est très important de s’assurer que les données ainsi que le code utilisés sont sauvegardés avant de supprimer l’instance. L’idéal est de versionner son code avec Git et de procéder à des sauvegardes régulières des données à l’aide du système de stockage S3.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nLes ressources mises à disposition pour l’execution des services sont partagées entre les différents utilisateurs du Datalab. Veuillez à ne pas laisser en cours des services dont vous ne faites plus l’usage. Nous procédons parfois à une suppression systématique des instances inactives depuis un certain temps, afin de libérer des ressources."
  },
  {
    "objectID": "docs/fr/discover.html#support",
    "href": "docs/fr/discover.html#support",
    "title": "Première utilisation",
    "section": "",
    "text": "Le support et l’aide à l’utilisation du Datalab sont effectuées sur un salon dédié du service de messagerie instantanée interministériel Tchap. Toute question sur l’utilisation du Datalab ou suggestion d’amélioration y sont les bienvenues.\nPour les agents n’utilisant pas Tchap, il est également possible de nous contacter par mail sur la BAL innovation@insee.fr."
  },
  {
    "objectID": "docs/en/version-control.html",
    "href": "docs/en/version-control.html",
    "title": "Version Control",
    "section": "",
    "text": "The Datalab is a shared platform where resources used by services are shared among different users. As such, Datalab services operate on the model of ephemeral containers: in a standard usage, the user launches a service, performs data processing, saves the code that was used to perform these processes, and then deletes the service instance. This code backup is greatly facilitated by the use of version control.\nHowever, this performance consideration should not be seen as a constraint: version control is an essential best practice in development. The benefits are numerous, both individually:\n\nThe local project is synchronized with a remote server, making the loss of code almost impossible.\nThe complete history of choices and modifications made on the project is preserved.\nThe user can browse this history to search for modifications that may have caused errors and decide at any time to revert to a previous version of the project or specific files.\n\nand in the context of collaborative projects:\n\nSimultaneous work on the same project is possible without the risk of loss.\nThe user can share their modifications while benefiting from the changes made by others.\nIt becomes possible to contribute to open-source projects, for which the use of Git is widely standard.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial aims to present how version control can be easily implemented using the tools available on the Datalab. It does not explain the workings of Git and presupposes a certain familiarity with the tool. Many online resources can serve as an introduction; for example, R users can consult this guide, and Python users can refer to this chapter of the course. A comprehensive Git training will soon be offered in the training space of the Datalab.\n\n\n\n\n\n\n\nAlthough offline use of Git is possible, the real benefit of version control lies in synchronizing the local copy of a project (clone) with a remote repository (remote). Various software forge services allow this synchronization of Git projects, the most well-known of which are GitHub and GitLab. Since GitHub has much more visibility today — for example, the repositories of Insee, InseeFr, and InseeFrLab, are on GitHub — Datalab offers an integration with GitHub, which we present in this tutorial.\n\n\n\n\n\n\nWarning\n\n\n\nThe rest of the tutorial requires having a GitHub account.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile using Datalab with the GitHub platform is facilitated, it is by no means mandatory: it is still possible to use the software forge of your choice for project synchronization. A forge based on GitLab is also available to Datalab users.\n\n\n\n\n\nSynchronization with a remote repository requires authentication with GitHub. This is done using a personal access token, which must be generated from the user’s GitHub account. The generation service is accessible at this address. The GitHub documentation provides illustrations to guide the process.\nTo generate a token, it is necessary to choose a token name, an expiration date, and access rights (scope). It is recommended to choose a short expiration date (30 days) and limited access (repo only) to reduce security risks in case the token is maliciously exposed.\n\n\n\nRecommended configuration for generating a GitHub access token\n\n\nOnce the token is generated, it will be displayed on the screen. A token can only be viewed once; if lost, a new one must be generated.\n\n\n\nIt is recommended to add access tokens to a password manager. Alternatively, the token can be added to the “External Services” configuration of the user account on Datalab, which allows the token to be directly accessible within the services offered on the platform.\n\n\n\nAdding a GitHub access token to a user account on Datalab\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to use the email address associated with your GitHub account in the “Account Information.” It is this address that effectively links the commits you make to your GitHub account.\n\n\n\n\n\n\nGit is pre-configured to work natively with the relevant Datalab services. When opening a service, it is possible to configure certain elements. If you have added a GitHub access token to your Datalab account, it will be pre-configured. It is also possible to indicate the complete URL of a Git repository (e.g., https://github.com/InseeFrLab/onyxia), which will then be cloned into the instance’s workspace during initialization.\n\n\n\nConfiguring Git when opening a service\n\n\n\n\nThe GitHub access token is available in the terminals of different services via the environment variable $GIT_PERSONAL_ACCESS_TOKEN. To avoid having to authenticate for every operation involving the remote repository (clone, push, and pull), it is recommended to clone it, including the access token in the HTTPS link, using the following command:\ngit clone https://${GIT_PERSONAL_ACCESS_TOKEN}@github.com/&lt;owner&gt;/&lt;repo&gt;.git\nwhere  and  should be replaced with the GitHub username and repository name, respectively.\n\n\n\nThe main code production services available on Datalab have a graphical interface to facilitate the use of Git:\n\nRStudio: RStudio provides a native and fairly comprehensive graphical interface for Git. The utilitR documentation presents its operation in detail.\nJupyter: the jupyterlab-git plugin provides a (fairly basic) interface between Jupyter and Git.\nVSCode: VSCode natively offers a well-integrated graphical interface with Git and GitHub. A detailed documentation presents the possibilities of the tool.\n\n\n\n\n\n\n\nWarning\n\n\n\nGraphical interfaces make it easier to get started with Git but never completely replace the use of the tool via a terminal due to necessarily imperfect integration. Therefore, it is useful to familiarize yourself with using Git via the terminal as early as possible."
  },
  {
    "objectID": "docs/en/version-control.html#why-use-version-control",
    "href": "docs/en/version-control.html#why-use-version-control",
    "title": "Version Control",
    "section": "",
    "text": "The Datalab is a shared platform where resources used by services are shared among different users. As such, Datalab services operate on the model of ephemeral containers: in a standard usage, the user launches a service, performs data processing, saves the code that was used to perform these processes, and then deletes the service instance. This code backup is greatly facilitated by the use of version control.\nHowever, this performance consideration should not be seen as a constraint: version control is an essential best practice in development. The benefits are numerous, both individually:\n\nThe local project is synchronized with a remote server, making the loss of code almost impossible.\nThe complete history of choices and modifications made on the project is preserved.\nThe user can browse this history to search for modifications that may have caused errors and decide at any time to revert to a previous version of the project or specific files.\n\nand in the context of collaborative projects:\n\nSimultaneous work on the same project is possible without the risk of loss.\nThe user can share their modifications while benefiting from the changes made by others.\nIt becomes possible to contribute to open-source projects, for which the use of Git is widely standard.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial aims to present how version control can be easily implemented using the tools available on the Datalab. It does not explain the workings of Git and presupposes a certain familiarity with the tool. Many online resources can serve as an introduction; for example, R users can consult this guide, and Python users can refer to this chapter of the course. A comprehensive Git training will soon be offered in the training space of the Datalab."
  },
  {
    "objectID": "docs/en/version-control.html#integrating-github-with-datalab",
    "href": "docs/en/version-control.html#integrating-github-with-datalab",
    "title": "Version Control",
    "section": "",
    "text": "Although offline use of Git is possible, the real benefit of version control lies in synchronizing the local copy of a project (clone) with a remote repository (remote). Various software forge services allow this synchronization of Git projects, the most well-known of which are GitHub and GitLab. Since GitHub has much more visibility today — for example, the repositories of Insee, InseeFr, and InseeFrLab, are on GitHub — Datalab offers an integration with GitHub, which we present in this tutorial.\n\n\n\n\n\n\nWarning\n\n\n\nThe rest of the tutorial requires having a GitHub account.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile using Datalab with the GitHub platform is facilitated, it is by no means mandatory: it is still possible to use the software forge of your choice for project synchronization. A forge based on GitLab is also available to Datalab users.\n\n\n\n\n\nSynchronization with a remote repository requires authentication with GitHub. This is done using a personal access token, which must be generated from the user’s GitHub account. The generation service is accessible at this address. The GitHub documentation provides illustrations to guide the process.\nTo generate a token, it is necessary to choose a token name, an expiration date, and access rights (scope). It is recommended to choose a short expiration date (30 days) and limited access (repo only) to reduce security risks in case the token is maliciously exposed.\n\n\n\nRecommended configuration for generating a GitHub access token\n\n\nOnce the token is generated, it will be displayed on the screen. A token can only be viewed once; if lost, a new one must be generated.\n\n\n\nIt is recommended to add access tokens to a password manager. Alternatively, the token can be added to the “External Services” configuration of the user account on Datalab, which allows the token to be directly accessible within the services offered on the platform.\n\n\n\nAdding a GitHub access token to a user account on Datalab\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to use the email address associated with your GitHub account in the “Account Information.” It is this address that effectively links the commits you make to your GitHub account."
  },
  {
    "objectID": "docs/en/version-control.html#using-git-with-datalab-services",
    "href": "docs/en/version-control.html#using-git-with-datalab-services",
    "title": "Version Control",
    "section": "",
    "text": "Git is pre-configured to work natively with the relevant Datalab services. When opening a service, it is possible to configure certain elements. If you have added a GitHub access token to your Datalab account, it will be pre-configured. It is also possible to indicate the complete URL of a Git repository (e.g., https://github.com/InseeFrLab/onyxia), which will then be cloned into the instance’s workspace during initialization.\n\n\n\nConfiguring Git when opening a service\n\n\n\n\nThe GitHub access token is available in the terminals of different services via the environment variable $GIT_PERSONAL_ACCESS_TOKEN. To avoid having to authenticate for every operation involving the remote repository (clone, push, and pull), it is recommended to clone it, including the access token in the HTTPS link, using the following command:\ngit clone https://${GIT_PERSONAL_ACCESS_TOKEN}@github.com/&lt;owner&gt;/&lt;repo&gt;.git\nwhere  and  should be replaced with the GitHub username and repository name, respectively.\n\n\n\nThe main code production services available on Datalab have a graphical interface to facilitate the use of Git:\n\nRStudio: RStudio provides a native and fairly comprehensive graphical interface for Git. The utilitR documentation presents its operation in detail.\nJupyter: the jupyterlab-git plugin provides a (fairly basic) interface between Jupyter and Git.\nVSCode: VSCode natively offers a well-integrated graphical interface with Git and GitHub. A detailed documentation presents the possibilities of the tool.\n\n\n\n\n\n\n\nWarning\n\n\n\nGraphical interfaces make it easier to get started with Git but never completely replace the use of the tool via a terminal due to necessarily imperfect integration. Therefore, it is useful to familiarize yourself with using Git via the terminal as early as possible."
  },
  {
    "objectID": "docs/en/secrets.html",
    "href": "docs/en/secrets.html",
    "title": "Secrets Management",
    "section": "",
    "text": "Secrets Management\n\nEnvironment Variables\nSometimes, certain pieces of information need to be made available to a large number of applications, or they should not be directly embedded in your code (access tokens, passwords, etc.). The use of environment variables allows accessing this information from any service.\nWhen a service is launched, several environment variables are automatically injected—such as access tokens for GitHub and MinIO.\n\n\n\nCreation and Management of Secrets\nOn the platform, environment variables are treated as secrets stored in Vault (the Datalab’s safe) and are encrypted. This enables you to store tokens, credentials, and passwords securely. The My Secrets page is designed like a file explorer, allowing you to sort and organize your variables into folders.\n\nGetting Started:\n\nCreate a new folder with + New folder.\nThen, within this folder, create a new secret with + New secret.\nOpen your secret.\n\n\nEach secret can contain multiple variables, consisting of key-value pairs.\n\n+ Add a variable\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe keys (variable names) always begin with $ and contain only letters, numbers, and the underscore character (_). By convention, keys are written in UPPERCASE.\n\n\nFill in the name of the key and its value.\n\n\n\nConverting Secrets into Environment Variables\nOnce your secret is edited, along with its different variables, you are ready to use it in your service.\n\nCopy the secret’s path by clicking on the Use in a service button.\nThen, during the configuration of your service, go to the Vault tab and paste the secret’s path in the dedicated field.\n\n\n\nCreate and open your service.\n\nTo verify that your environment variables have been successfully created, you can run the following commands in the service terminal:\n# List all available environment variables\nenv\n\n# Display the value of an environment variable\necho $MY_VARIABLE\n\n# Find all environment variables containing a given pattern\nenv | grep -i \"&lt;PATTERN&gt;\""
  },
  {
    "objectID": "docs/en/discover.html",
    "href": "docs/en/discover.html",
    "title": "First Use",
    "section": "",
    "text": "Welcome to Onyxia Datalab, a shared self-service platform for data processing, designed for statisticians and data scientists working for the government. This tutorial provides a guided tour of the Datalab to quickly get you up and running with its services.\n\n\n\n\n\n\nWarning\n\n\n\nThe terms of use for Datalab can be found at this address. We remind you that Datalab is intended exclusively for the processing of public and non-sensitive data. Projects involving non-open data for experimentation can be carried out in consultation with the Datalab team, subject to compliance with project-specific security rules.\n\n\n\n\nThe service catalog is at the core of Datalab’s usage. It offers a set of services for statistical data processing and complete management of data science projects.\n\n\n\nTo launch a service, simply click on the Launch button of the desired service.\nA page centered on the requested service will then open, offering several options:\n\nClick again on the Launch button to start the service with its default configuration.\nCustomize the name that the instance will have once the service is launched.\nUnfold a configuration menu to customize the service’s configuration before launching it.\nSave a customized configuration by clicking on the bookmark at the top right of the service.\n\n\nThe detailed configuration of Datalab services is an advanced usage and is not covered in this tutorial but in other pages of this documentation site.\n\n\n\n\nLaunching a service automatically takes you to the My Services page, where all active instances on the user’s account are listed.\n\nOnce the service is launched, an Open button appears, allowing access to the service. A password — and, depending on the services, a username — is generally required to use the service. This information is available in the service’s README, which can be accessed by clicking on the button of the same name.\n\n\n\nDeleting an instance of a service is done simply by clicking on the trash can icon below the instance.\n\n\n\n\n\n\nCaution\n\n\n\nFor some services, deleting an instance will also delete all associated data, and this action is irreversible. Therefore, it is essential to always read the README associated with the instance, which specifies the consequences of deleting the instance. In general, it is crucial to ensure that the data and code used are backed up before deleting the instance. Ideally, version your code with Git and regularly back up the data using MinIO (S3 storage system).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe resources made available for executing services are shared among different Datalab users. Please make sure not to leave active services that you no longer use. We sometimes systematically remove inactive instances after a certain period to free up resources.\n\n\n\n\n\n\nSupport and assistance for using Datalab are provided on a dedicated channel of the inter-ministerial instant messaging service Tchap. Any questions about using Datalab or suggestions for improvement are welcome there.\nFor users not using Tchap, it is also possible to contact us via email at innovation@insee.fr."
  },
  {
    "objectID": "docs/en/discover.html#service-catalog",
    "href": "docs/en/discover.html#service-catalog",
    "title": "First Use",
    "section": "",
    "text": "The service catalog is at the core of Datalab’s usage. It offers a set of services for statistical data processing and complete management of data science projects.\n\n\n\nTo launch a service, simply click on the Launch button of the desired service.\nA page centered on the requested service will then open, offering several options:\n\nClick again on the Launch button to start the service with its default configuration.\nCustomize the name that the instance will have once the service is launched.\nUnfold a configuration menu to customize the service’s configuration before launching it.\nSave a customized configuration by clicking on the bookmark at the top right of the service.\n\n\nThe detailed configuration of Datalab services is an advanced usage and is not covered in this tutorial but in other pages of this documentation site.\n\n\n\n\nLaunching a service automatically takes you to the My Services page, where all active instances on the user’s account are listed.\n\nOnce the service is launched, an Open button appears, allowing access to the service. A password — and, depending on the services, a username — is generally required to use the service. This information is available in the service’s README, which can be accessed by clicking on the button of the same name.\n\n\n\nDeleting an instance of a service is done simply by clicking on the trash can icon below the instance.\n\n\n\n\n\n\nCaution\n\n\n\nFor some services, deleting an instance will also delete all associated data, and this action is irreversible. Therefore, it is essential to always read the README associated with the instance, which specifies the consequences of deleting the instance. In general, it is crucial to ensure that the data and code used are backed up before deleting the instance. Ideally, version your code with Git and regularly back up the data using MinIO (S3 storage system).\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe resources made available for executing services are shared among different Datalab users. Please make sure not to leave active services that you no longer use. We sometimes systematically remove inactive instances after a certain period to free up resources."
  },
  {
    "objectID": "docs/en/discover.html#support",
    "href": "docs/en/discover.html#support",
    "title": "First Use",
    "section": "",
    "text": "Support and assistance for using Datalab are provided on a dedicated channel of the inter-ministerial instant messaging service Tchap. Any questions about using Datalab or suggestions for improvement are welcome there.\nFor users not using Tchap, it is also possible to contact us via email at innovation@insee.fr."
  },
  {
    "objectID": "docs/en/configure.html",
    "href": "docs/en/configure.html",
    "title": "Service Configuration",
    "section": "",
    "text": "After clicking on “New service” &gt; “RStudio/Jupyter-python/VScode-python” &gt; “Launch”\n\n\nTo recognize the service and/or the configuration if saved by clicking on the bookmark symbol at the top right. If the name already exists among the saved configurations, saving will overwrite the old configuration.\nConvenient for distinguishing different services of the same type (RStudio, Jupyter…).\n\n\n\nIt is possible to share a service with a group of people by checking the “Share the service” box when opening the service. Other members of the group will see the service and can use it. Creating groups can be done by writing to administrators on Tchap (privately) or by email at innovation@insee.fr, providing the group name, usernames of the members, and whether or not a associated storage space is needed on MinIO.\n\nFor occasional needs, it is also possible to share a service that you have created with another person. Simply provide them with the URL (e.g., https://user-aaaaaaaaaaaaaa-xxxxxxx-x.user.lab.sspcloud.fr/) and the service password. The username remains Onyxia. Please note, it is recommended to change the service password during its launch (in the Security tab) to avoid any leaks. You should also uncheck Enable IP protection and Enable network policy in the Security tab. Only one person at a time can connect to an RStudio service.\n\n\n\n\n\n\n\n\n\n\n\n\nA link to a shell script (sequence of Linux commands) that is executed right after the service is launched. Convenient for automating the setup of certain configurations.\nThis script link must be accessible on the internet, for example, on https://git.lab.sspcloud.fr/ with a public project or on S3 storage with a public file.\nExample of an initialization script that clones a project from a private Gitlab instance, configures global RStudio options, automatically opens the cloned RStudio project, installs and selects French spelling correction, and customizes code snippets (snippets).\n\n\n\n\n\n\nWarning\n\n\n\nThe script is executed as a superuser (Root), and the files it creates become the property of the superuser. This leads to errors when these files are called, for example, RStudio configuration files. To give normal user rights (named onyxia) to their personal folder\n\n\nchown -R ${USERNAME}:${GROUPNAME} ${HOME}\n:::\n\n\n\nOptions to pass to the initialization script, separated by spaces and can be subsequently called with $1, $2, etc.\nFor example, if you enter file1.txt file2.txt in the PersonalInitArgs field and use this initialization script:\n#!/bin/bash\ntouch $1\ntouch $2\nThe script will create two files, file1.txt and file2.txt, using the touch command.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is the password to enter when opening a service, which is provided by “Copy the password” on the service page. It is supplied by the general parameter “Password for your services,” which can be found in “My Account” &gt; “Account Information,” unless a specific one is defined at the service level.\n\n\n\nIf checked, the service is only accessible from a single IP. Uncheck it if you wish to work from different locations.\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page.\n\n\n\n\n\n\nWarning\n\n\n\nIt is not possible to automatically clone a private project from a private instance (i.e., other than gitlab.com and github.com). To do so, you will need to use a shell script as indicated here.\n\n\n\n\nIf checked, configure Git and attempt a clone when the service starts.\n\n\n\nThe name that will appear in the commits (not the Gitlab or Github account username).\n\n\n\nThe email address that will appear in the commits (not necessarily the email associated with the Gitlab or Github account).\n\n\n\n\n\n\nAccess token defined on the platform used (Gitlab, Github…).\n\n\n\nThe URL obtained from the platform used (Gitlab, Github…) by clicking on “Clone” &gt; HTTPS.\nIn the format:\nhttps://github.com/InseeFrLab/docs.sspcloud.fr.git\n\n\n\n\n\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page."
  },
  {
    "objectID": "docs/en/configure.html#custom-name",
    "href": "docs/en/configure.html#custom-name",
    "title": "Service Configuration",
    "section": "",
    "text": "To recognize the service and/or the configuration if saved by clicking on the bookmark symbol at the top right. If the name already exists among the saved configurations, saving will overwrite the old configuration.\nConvenient for distinguishing different services of the same type (RStudio, Jupyter…)."
  },
  {
    "objectID": "docs/en/configure.html#share-the-service",
    "href": "docs/en/configure.html#share-the-service",
    "title": "Service Configuration",
    "section": "",
    "text": "It is possible to share a service with a group of people by checking the “Share the service” box when opening the service. Other members of the group will see the service and can use it. Creating groups can be done by writing to administrators on Tchap (privately) or by email at innovation@insee.fr, providing the group name, usernames of the members, and whether or not a associated storage space is needed on MinIO.\n\nFor occasional needs, it is also possible to share a service that you have created with another person. Simply provide them with the URL (e.g., https://user-aaaaaaaaaaaaaa-xxxxxxx-x.user.lab.sspcloud.fr/) and the service password. The username remains Onyxia. Please note, it is recommended to change the service password during its launch (in the Security tab) to avoid any leaks. You should also uncheck Enable IP protection and Enable network policy in the Security tab. Only one person at a time can connect to an RStudio service."
  },
  {
    "objectID": "docs/en/configure.html#init",
    "href": "docs/en/configure.html#init",
    "title": "Service Configuration",
    "section": "",
    "text": "A link to a shell script (sequence of Linux commands) that is executed right after the service is launched. Convenient for automating the setup of certain configurations.\nThis script link must be accessible on the internet, for example, on https://git.lab.sspcloud.fr/ with a public project or on S3 storage with a public file.\nExample of an initialization script that clones a project from a private Gitlab instance, configures global RStudio options, automatically opens the cloned RStudio project, installs and selects French spelling correction, and customizes code snippets (snippets).\n\n\n\n\n\n\nWarning\n\n\n\nThe script is executed as a superuser (Root), and the files it creates become the property of the superuser. This leads to errors when these files are called, for example, RStudio configuration files. To give normal user rights (named onyxia) to their personal folder\n\n\nchown -R ${USERNAME}:${GROUPNAME} ${HOME}\n:::\n\n\n\nOptions to pass to the initialization script, separated by spaces and can be subsequently called with $1, $2, etc.\nFor example, if you enter file1.txt file2.txt in the PersonalInitArgs field and use this initialization script:\n#!/bin/bash\ntouch $1\ntouch $2\nThe script will create two files, file1.txt and file2.txt, using the touch command."
  },
  {
    "objectID": "docs/en/configure.html#security",
    "href": "docs/en/configure.html#security",
    "title": "Service Configuration",
    "section": "",
    "text": "This is the password to enter when opening a service, which is provided by “Copy the password” on the service page. It is supplied by the general parameter “Password for your services,” which can be found in “My Account” &gt; “Account Information,” unless a specific one is defined at the service level.\n\n\n\nIf checked, the service is only accessible from a single IP. Uncheck it if you wish to work from different locations."
  },
  {
    "objectID": "docs/en/configure.html#git",
    "href": "docs/en/configure.html#git",
    "title": "Service Configuration",
    "section": "",
    "text": "To learn how to use this tab, see the dedicated page.\n\n\n\n\n\n\nWarning\n\n\n\nIt is not possible to automatically clone a private project from a private instance (i.e., other than gitlab.com and github.com). To do so, you will need to use a shell script as indicated here.\n\n\n\n\nIf checked, configure Git and attempt a clone when the service starts.\n\n\n\nThe name that will appear in the commits (not the Gitlab or Github account username).\n\n\n\nThe email address that will appear in the commits (not necessarily the email associated with the Gitlab or Github account).\n\n\n\n\n\n\nAccess token defined on the platform used (Gitlab, Github…).\n\n\n\nThe URL obtained from the platform used (Gitlab, Github…) by clicking on “Clone” &gt; HTTPS.\nIn the format:\nhttps://github.com/InseeFrLab/docs.sspcloud.fr.git\n\n\n\n\n\n\n\n\n\n\n\n\nTo learn how to use this tab, see the dedicated page."
  },
  {
    "objectID": "docs/en/principles.html",
    "href": "docs/en/principles.html",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Onyxia project stems from the recognition of common difficulties faced by data scientists in the public sector:\n\nOften isolated agents, due to the relative scarcity of data skills in administration.\nInadequate infrastructures in terms of resources and technologies, which hinder innovation.\nDifficulty transitioning from experimentation to production due to multiple separations (physical separation, development languages, work methods) between business units and IT production.\n\nIn response to these challenges, Datalab SSPCloud was built to offer a collaborative platform at multiple levels:\n\nSharing a modern infrastructure centered around deploying services through containers, specifically designed for data science applications.\nSharing methods through the mutualization of data science services offered, to which everyone can contribute.\nSharing knowledge through Datalab-related training and the formation of user communities focused on its utilization.\n\n\n\n\n\n\n\nNote\n\n\n\nOnyxia, Datalab SSP Cloud: What are the differences?\nOnyxia is an open-source project that provides a web application for accessing data science services. The Datalab SSP Cloud is an instance of the Onyxia project hosted at Insee.\n\n\n\n\n\nThe architecture of Datalab is based on a set of fundamental principles:\n\nData science-oriented production by providing an infrastructure suitable for most use cases and a service catalog covering the entire data project lifecycle.\nChoices that promote user autonomy by avoiding proprietary lock-ins and enabling access to the lower layers of the infrastructure to address advanced and specific needs.\nA project that is 100% cloud-native and also cloud-agnostic, allowing for easy deployment on any infrastructure.\nA completely open-source project, both in terms of its constituent components and its distribution (MIT license).\n\n\n\n\nDatalab can be accessed through a modern and responsive user interface, focused on providing a great user experience. It serves as the technical connection between the different components of Onyxia:\n\nOpen-source technologies that represent the state of the art in container deployment and orchestration, storage, and security.\nA service and tool catalog to support data science projects.\nA training and documentation platform to facilitate onboarding onto the offered technologies.\n\n\n\n\nFundamental building blocks of Datalab Onyxia\n\n\nThe service catalog is designed to accommodate the essential needs of data scientists, from self-service development to production deployment of processes or applications. The entire data project lifecycle is covered, and the catalog of services is regularly expanded to meet users’ new requirements.\n\n\n\nA comprehensive service catalog for data science projects\n\n\n\n\n\nThe Datalab Onyxia project is resolutely open at multiple levels:\n\nThe Datalab is accessible through its web interface to all public service agents (via AgentConnect or a gouv.fr email address) as well as students from statistical schools linked to Insee (Cefil, Ensai, Ensae).\nThe open-source code and modularity of the project allow for the deployment of a customized Onyxia instance on any Kubernetes-based infrastructure cluster.\nThe project is open to external contributions, whether related to the service catalog, the graphical interface, or the arrangement of the software components it comprises."
  },
  {
    "objectID": "docs/en/principles.html#a-platform-for-collaboration",
    "href": "docs/en/principles.html#a-platform-for-collaboration",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Onyxia project stems from the recognition of common difficulties faced by data scientists in the public sector:\n\nOften isolated agents, due to the relative scarcity of data skills in administration.\nInadequate infrastructures in terms of resources and technologies, which hinder innovation.\nDifficulty transitioning from experimentation to production due to multiple separations (physical separation, development languages, work methods) between business units and IT production.\n\nIn response to these challenges, Datalab SSPCloud was built to offer a collaborative platform at multiple levels:\n\nSharing a modern infrastructure centered around deploying services through containers, specifically designed for data science applications.\nSharing methods through the mutualization of data science services offered, to which everyone can contribute.\nSharing knowledge through Datalab-related training and the formation of user communities focused on its utilization.\n\n\n\n\n\n\n\nNote\n\n\n\nOnyxia, Datalab SSP Cloud: What are the differences?\nOnyxia is an open-source project that provides a web application for accessing data science services. The Datalab SSP Cloud is an instance of the Onyxia project hosted at Insee."
  },
  {
    "objectID": "docs/en/principles.html#fundamental-principles",
    "href": "docs/en/principles.html#fundamental-principles",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The architecture of Datalab is based on a set of fundamental principles:\n\nData science-oriented production by providing an infrastructure suitable for most use cases and a service catalog covering the entire data project lifecycle.\nChoices that promote user autonomy by avoiding proprietary lock-ins and enabling access to the lower layers of the infrastructure to address advanced and specific needs.\nA project that is 100% cloud-native and also cloud-agnostic, allowing for easy deployment on any infrastructure.\nA completely open-source project, both in terms of its constituent components and its distribution (MIT license)."
  },
  {
    "objectID": "docs/en/principles.html#service-offering",
    "href": "docs/en/principles.html#service-offering",
    "title": "Principles of Datalab",
    "section": "",
    "text": "Datalab can be accessed through a modern and responsive user interface, focused on providing a great user experience. It serves as the technical connection between the different components of Onyxia:\n\nOpen-source technologies that represent the state of the art in container deployment and orchestration, storage, and security.\nA service and tool catalog to support data science projects.\nA training and documentation platform to facilitate onboarding onto the offered technologies.\n\n\n\n\nFundamental building blocks of Datalab Onyxia\n\n\nThe service catalog is designed to accommodate the essential needs of data scientists, from self-service development to production deployment of processes or applications. The entire data project lifecycle is covered, and the catalog of services is regularly expanded to meet users’ new requirements.\n\n\n\nA comprehensive service catalog for data science projects"
  },
  {
    "objectID": "docs/en/principles.html#an-open-project",
    "href": "docs/en/principles.html#an-open-project",
    "title": "Principles of Datalab",
    "section": "",
    "text": "The Datalab Onyxia project is resolutely open at multiple levels:\n\nThe Datalab is accessible through its web interface to all public service agents (via AgentConnect or a gouv.fr email address) as well as students from statistical schools linked to Insee (Cefil, Ensai, Ensae).\nThe open-source code and modularity of the project allow for the deployment of a customized Onyxia instance on any Kubernetes-based infrastructure cluster.\nThe project is open to external contributions, whether related to the service catalog, the graphical interface, or the arrangement of the software components it comprises."
  },
  {
    "objectID": "docs/en/storage.html",
    "href": "docs/en/storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "The file storage solution associated with Datalab is MinIO, an object storage system based on the cloud, compatible with Amazon’s S3 API. In practice, this has several advantages:\n\nStored files are easily accessible from anywhere: a file can be accessed directly via a simple URL, which can be shared.\nIt is possible to access the stored files directly within the data science services (R, Python, etc.) offered on Datalab, without the need to copy the files locally beforehand, greatly improving the reproducibility of analyses.\n\n\n\n\n\n\nThe My Files page in Datalab takes the form of a file explorer showing the different buckets (repositories) to which the user has access.\nEach user has a personal bucket by default to store their files. Within this bucket, two options are possible:\n\n“Create a directory”: Creates a directory in the current bucket/directory hierarchically, similar to a traditional file system.\n“Upload a file”: Uploads one or multiple files to the current directory.\n\n::: {.callout-note} The graphical interface for data storage in Datalab is still under construction. As such, it may experience responsiveness issues. For frequent operations on file storage, it may be preferable to interact with MinIO via the terminal. :::\n\n\n\nBy clicking on a file in their personal bucket, the user can access its characteristics page. On this page, it is possible to change the distribution status of the file. Changing the status of the file from “private” to “public” generates a distribution link, which can then be shared for downloading the file. The “public” status only grants read rights to other users, and modifying or deleting other users’ personal files is not possible.\nTo simplify the sharing of multiple files for, say, a training session, it is possible to create a “distribution” folder in the user’s personal bucket. By default, all files present in this folder have a public distribution status.\n::: {.callout-note} For collaborative projects, it can be beneficial for different participants to have access to a shared storage space. It is possible to create shared buckets on MinIO for this purpose. Feel free to contact us via the channels specified on the “First Use” page if you wish to work on open data projects on Datalab. :::\n::: {.callout-warning} In accordance with the terms of use, only open data-type or non-sensitive data can be stored on Datalab. Having a file with a “private” distribution status does not guarantee perfect confidentiality. :::\n\n\n\n\nThe access credentials needed to access data on MinIO are pre-configured in the various Datalab services, accessible in the form of environment variables. This greatly facilitates importing and exporting files from the services.\n\n\n\nRPythonmc\n\n\nIn R, interaction with an S3-compatible file system is made possible by the aws.s3 library.\nlibrary(aws.s3)\n\n\nIn Python, interaction with an S3-compatible file system is made possible by two libraries:\n\nBoto3, a library created and maintained by Amazon.\nS3Fs, a library that allows interaction with stored files similar to a classic filesystem.\n\nFor this reason, and because S3Fs is used by default by the pandas library to manage S3 connections, we will present how to manage storage on MinIO via Python using this library.\nimport os\nimport s3fs\n\n# Create filesystem object\nS3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n\n\nMinIO offers a command-line client (mc) that allows interaction with the storage system in a manner similar to a classic UNIX filesystem. This client is installed by default and accessible via a terminal in the various Datalab services.\nThe MinIO client offers basic UNIX commands such as ls, cat, cp, etc. The complete list is available in the client documentation.\n\n\n\nListing the files in a bucket\n\nRPythonmc\n\n\naws.s3::get_bucket(\"donnees-insee\", region = \"\")\n\n\nfs.ls(\"donnees-insee\")\n\n\nThe Datalab storage is accessible via the alias s3. For example, to list the files in the bucket donnees-insee:\nmc ls s3/donnees-insee\n\n\n\nImporting Data\n\nRPythonmc\n\n\nBUCKET &lt;- \"donnees-insee\"\nFILE_KEY_S3 &lt;- \"diffusion/BPE/2019/BPE_ENS.csv\"\n\ndf &lt;- \n  aws.s3::s3read_using(\n    FUN = readr::read_delim,\n    # Put FUN options here\n    delim = \";\",\n    object = FILE_KEY_S3,\n    bucket = BUCKET,\n    opts = list(\"region\" = \"\")\n  )\n\n\nThe S3Fs package allows you to interact with files stored on MinIO as if they were local files. The syntax is therefore very familiar to Python users. For example, to import/export tabular data via pandas:\nimport pandas as pd\n\nBUCKET = \"donnees-insee\"\nFILE_KEY_S3 = \"diffusion/BPE/2019/BPE_ENS.csv\"\nFILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n\nwith fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n    df_bpe = pd.read_csv(file_in, sep=\";\")\n\n\nTo copy data from a MinIO bucket to the local service:\nmc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv\n::: {.callout-warning} Copying files to the local service is generally not a good practice: it limits the reproducibility of analyses and becomes quickly impossible with large volumes of data. Therefore, it is preferable to get into the habit of importing data directly into R/Python. :::\n\n\n\nExporting Data to MinIO\n\nRPythonmc\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\n\naws.s3::s3write_using(\n    df,\n    FUN = readr::write_csv,\n    object = FILE_KEY_OUT_S3,\n    bucket = BUCKET_OUT,\n    opts = list(\"region\" = \"\")\n)\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\nFILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n\nwith fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n    df_bpe.to_csv(file_out)\n\n\nTo copy data from the local service to a bucket on MinIO:\nmc cp local/path/to/my/file.csv s3/&lt;my_bucket&gt;/remote/path/to/my/file.csv\n\n\n\nRenewing Expired Access Tokens\nAccess to MinIO storage is possible via a personal access token, which is valid for 24 hours and automatically regenerated at regular intervals on SSP Cloud. When a token has expired, services created before the expiration date (using the previous token) can no longer access storage, and the affected service will appear in red on the My Services page. In this case, there are two options:\n\nOpen a new service on Datalab, which will have a default, up-to-date token.\nManually replace expired tokens with new ones. Scripts indicating how to do this for different MinIO uses (R/Python/mc) are available here. Simply choose the relevant script and execute it in your current working environment."
  },
  {
    "objectID": "docs/en/storage.html#principles",
    "href": "docs/en/storage.html#principles",
    "title": "Data Storage",
    "section": "",
    "text": "The file storage solution associated with Datalab is MinIO, an object storage system based on the cloud, compatible with Amazon’s S3 API. In practice, this has several advantages:\n\nStored files are easily accessible from anywhere: a file can be accessed directly via a simple URL, which can be shared.\nIt is possible to access the stored files directly within the data science services (R, Python, etc.) offered on Datalab, without the need to copy the files locally beforehand, greatly improving the reproducibility of analyses."
  },
  {
    "objectID": "docs/en/storage.html#managing-your-data",
    "href": "docs/en/storage.html#managing-your-data",
    "title": "Data Storage",
    "section": "",
    "text": "The My Files page in Datalab takes the form of a file explorer showing the different buckets (repositories) to which the user has access.\nEach user has a personal bucket by default to store their files. Within this bucket, two options are possible:\n\n“Create a directory”: Creates a directory in the current bucket/directory hierarchically, similar to a traditional file system.\n“Upload a file”: Uploads one or multiple files to the current directory.\n\n::: {.callout-note} The graphical interface for data storage in Datalab is still under construction. As such, it may experience responsiveness issues. For frequent operations on file storage, it may be preferable to interact with MinIO via the terminal. :::\n\n\n\nBy clicking on a file in their personal bucket, the user can access its characteristics page. On this page, it is possible to change the distribution status of the file. Changing the status of the file from “private” to “public” generates a distribution link, which can then be shared for downloading the file. The “public” status only grants read rights to other users, and modifying or deleting other users’ personal files is not possible.\nTo simplify the sharing of multiple files for, say, a training session, it is possible to create a “distribution” folder in the user’s personal bucket. By default, all files present in this folder have a public distribution status.\n::: {.callout-note} For collaborative projects, it can be beneficial for different participants to have access to a shared storage space. It is possible to create shared buckets on MinIO for this purpose. Feel free to contact us via the channels specified on the “First Use” page if you wish to work on open data projects on Datalab. :::\n::: {.callout-warning} In accordance with the terms of use, only open data-type or non-sensitive data can be stored on Datalab. Having a file with a “private” distribution status does not guarantee perfect confidentiality. :::"
  },
  {
    "objectID": "docs/en/storage.html#using-data-stored-on-minio",
    "href": "docs/en/storage.html#using-data-stored-on-minio",
    "title": "Data Storage",
    "section": "",
    "text": "The access credentials needed to access data on MinIO are pre-configured in the various Datalab services, accessible in the form of environment variables. This greatly facilitates importing and exporting files from the services.\n\n\n\nRPythonmc\n\n\nIn R, interaction with an S3-compatible file system is made possible by the aws.s3 library.\nlibrary(aws.s3)\n\n\nIn Python, interaction with an S3-compatible file system is made possible by two libraries:\n\nBoto3, a library created and maintained by Amazon.\nS3Fs, a library that allows interaction with stored files similar to a classic filesystem.\n\nFor this reason, and because S3Fs is used by default by the pandas library to manage S3 connections, we will present how to manage storage on MinIO via Python using this library.\nimport os\nimport s3fs\n\n# Create filesystem object\nS3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n\n\nMinIO offers a command-line client (mc) that allows interaction with the storage system in a manner similar to a classic UNIX filesystem. This client is installed by default and accessible via a terminal in the various Datalab services.\nThe MinIO client offers basic UNIX commands such as ls, cat, cp, etc. The complete list is available in the client documentation.\n\n\n\nListing the files in a bucket\n\nRPythonmc\n\n\naws.s3::get_bucket(\"donnees-insee\", region = \"\")\n\n\nfs.ls(\"donnees-insee\")\n\n\nThe Datalab storage is accessible via the alias s3. For example, to list the files in the bucket donnees-insee:\nmc ls s3/donnees-insee\n\n\n\nImporting Data\n\nRPythonmc\n\n\nBUCKET &lt;- \"donnees-insee\"\nFILE_KEY_S3 &lt;- \"diffusion/BPE/2019/BPE_ENS.csv\"\n\ndf &lt;- \n  aws.s3::s3read_using(\n    FUN = readr::read_delim,\n    # Put FUN options here\n    delim = \";\",\n    object = FILE_KEY_S3,\n    bucket = BUCKET,\n    opts = list(\"region\" = \"\")\n  )\n\n\nThe S3Fs package allows you to interact with files stored on MinIO as if they were local files. The syntax is therefore very familiar to Python users. For example, to import/export tabular data via pandas:\nimport pandas as pd\n\nBUCKET = \"donnees-insee\"\nFILE_KEY_S3 = \"diffusion/BPE/2019/BPE_ENS.csv\"\nFILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n\nwith fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n    df_bpe = pd.read_csv(file_in, sep=\";\")\n\n\nTo copy data from a MinIO bucket to the local service:\nmc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv\n::: {.callout-warning} Copying files to the local service is generally not a good practice: it limits the reproducibility of analyses and becomes quickly impossible with large volumes of data. Therefore, it is preferable to get into the habit of importing data directly into R/Python. :::\n\n\n\nExporting Data to MinIO\n\nRPythonmc\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\n\naws.s3::s3write_using(\n    df,\n    FUN = readr::write_csv,\n    object = FILE_KEY_OUT_S3,\n    bucket = BUCKET_OUT,\n    opts = list(\"region\" = \"\")\n)\n\n\nBUCKET_OUT = \"&lt;my_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"my_folder/BPE_ENS.csv\"\nFILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n\nwith fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n    df_bpe.to_csv(file_out)\n\n\nTo copy data from the local service to a bucket on MinIO:\nmc cp local/path/to/my/file.csv s3/&lt;my_bucket&gt;/remote/path/to/my/file.csv\n\n\n\nRenewing Expired Access Tokens\nAccess to MinIO storage is possible via a personal access token, which is valid for 24 hours and automatically regenerated at regular intervals on SSP Cloud. When a token has expired, services created before the expiration date (using the previous token) can no longer access storage, and the affected service will appear in red on the My Services page. In this case, there are two options:\n\nOpen a new service on Datalab, which will have a default, up-to-date token.\nManually replace expired tokens with new ones. Scripts indicating how to do this for different MinIO uses (R/Python/mc) are available here. Simply choose the relevant script and execute it in your current working environment."
  },
  {
    "objectID": "docs/fr/configure.html",
    "href": "docs/fr/configure.html",
    "title": "Configuration des services",
    "section": "",
    "text": "Après avoir cliqué sur “Nouveau service” &gt; “RStudio/Jupyter-python/VScode-python” &gt; “Lancer”\n\n\nPour reconnaître le service et/ou la configuration si on l’enregistre en cliquant sur le symbole de marque page en haut à droite. Si le nom existe déjà parmi les configurations enregistrées, l’enregistrement écrasera l’ancienne configuration.\nPratique pour distinguer différents services d’un même type (RStudio, Jupyter…).\n\n\n\nIl est possible de partager un service à un groupe de personnes en cochant la case “Partager le service” à l’ouverture du service. Les autres membres du groupe verront le service et pourront l’utiliser. La création de groupes se fait en écrivant aux administrateurs sur Tchap (en privé) ou par mail à l’adresse innovation@insee.fr, en communiquant le nom de groupe, les noms d’utilisateurs des membres, le besoin ou non d’un espace de stockage associé sur MinIO.\n\nPour un besoin ponctuel, il est aussi possible de partager un service que l’on a créé à une autre personne. Il suffit de lui communiquer l’URL (type https://user-aaaaaaaaaaaaaa-xxxxxxx-x.user.lab.sspcloud.fr/), ainsi que le mot de passe du service. Le nom d’utilisateur reste Onyxia. Attention, il est recommandé de changer le mot de passe du service lors de son lancement (onglet Security) pour ne pas le faire fuiter. Il faudra aussi décocher Enable IP protection et Enable network policy dans l’onglet Security. Une seule personne à la fois peut se connecter à un service RStudio.\n\n\n\n\n\n\n\n\n\n\n\n\nUn lien vers un script shell (enchaînement de commandes linux) qui est exécuté juste après le lancement du service. Pratique pour automatiser la mise en place de certaines configurations.\nCe lien du script doit être accessible sur internet, par exemple sur https://git.lab.sspcloud.fr/ avec un projet public ou sur le stockage S3 avec un fichier public.\nExemple de script d’initialisation qui clone un projet à partir d’une instance Gitlab privée, configure les options globales de RStudio, ouvre automatiquement le projet RStudio cloné, installe et sélectionne la correction orthographique française, personnalise les bribes de codes (snippets).\n\n\n\n\n\n\nWarning\n\n\n\nLe script est exécuté en tant que superutilisateur (Root) et les fichiers qu’il crée sont ainsi la propriété du superutilisateur. Ceci génère des erreurs ensuite quand ces fichiers sont appelés, par exemple des fichiers de configuration de RStudio. Pour rendre à l’utilisateur normal (qui s’appelle onyxia) les droit sur son dossier personnel :\nchown -R ${USERNAME}:${GROUPNAME} ${HOME}\n\n\n\n\n\nDes options à passer au script d’initialisation, séparées par des espaces et que l’on peut ensuite appeler avec $1, $2…\nPar exemple si on inscrit dans le champ PersonalInitArgs fichier1.txt fichier2.txt, et qu’on utilise ce script d’initialisation :\n#!/bin/bash\ntouch $1\ntouch $2\nLe script créera via la commande touch deux fichiers fichier1.txt et fichier2.txt.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC’est le mot de passe à saisir lorsqu’on ouvre un service, celui donné par “Copier le mot de passage” sur la page des services. Il est fourni par le paramètre général “Mot de passe pour vos services” que l’on trouve dans “Mon Compte” &gt; “Informations du compte”, sauf si on en a défini un particulier au niveau du service.\n\n\n\nSi coché, le service n’est accessible que par une seule IP, à décocher si l’on souhaite travailler de deux endroits différents.\n\n\n\n\n\n\n\nPour apprendre à utiliser cet onglet, voir la page dédiée.\n\n\n\n\n\n\nWarning\n\n\n\nIl n’est pas possible de cloner automatiquement un projet privé d’une instance privée (c’est-à-dire autre que gitlab.com et github.com). Pour le faire, il faudra recourir à un script shell comme indiqué ici.\n\n\n\n\nSi coché, configure Git et tente un clone au démarrage du service.\n\n\n\nLe nom qui apparaîtra dans les commits (pas le nom d’utilisateur du compte Gitlab ou Github).\n\n\n\nL’adresse email qui apparaîtra dans les commits (pas forcément le mail associé au compte Gitlab ou Github).\n\n\n\n\n\n\nJeton d’accès défini sur la plateforme utilisée (Gitlab, Github…).\n\n\n\nL’URL obtenue sur la plateforme utilisée (Gitlab, Github…) en cliquant sur “Cloner” &gt; HTTPS.\nDe type :\nhttps://github.com/InseeFrLab/docs.sspcloud.fr.git\n\n\n\n\n\n\n\n\n\n\n\n\nPour apprendre à utiliser cet onglet, voir la page dédiée."
  },
  {
    "objectID": "docs/fr/configure.html#nom-personnalisé",
    "href": "docs/fr/configure.html#nom-personnalisé",
    "title": "Configuration des services",
    "section": "",
    "text": "Pour reconnaître le service et/ou la configuration si on l’enregistre en cliquant sur le symbole de marque page en haut à droite. Si le nom existe déjà parmi les configurations enregistrées, l’enregistrement écrasera l’ancienne configuration.\nPratique pour distinguer différents services d’un même type (RStudio, Jupyter…)."
  },
  {
    "objectID": "docs/fr/configure.html#partager-le-service",
    "href": "docs/fr/configure.html#partager-le-service",
    "title": "Configuration des services",
    "section": "",
    "text": "Il est possible de partager un service à un groupe de personnes en cochant la case “Partager le service” à l’ouverture du service. Les autres membres du groupe verront le service et pourront l’utiliser. La création de groupes se fait en écrivant aux administrateurs sur Tchap (en privé) ou par mail à l’adresse innovation@insee.fr, en communiquant le nom de groupe, les noms d’utilisateurs des membres, le besoin ou non d’un espace de stockage associé sur MinIO.\n\nPour un besoin ponctuel, il est aussi possible de partager un service que l’on a créé à une autre personne. Il suffit de lui communiquer l’URL (type https://user-aaaaaaaaaaaaaa-xxxxxxx-x.user.lab.sspcloud.fr/), ainsi que le mot de passe du service. Le nom d’utilisateur reste Onyxia. Attention, il est recommandé de changer le mot de passe du service lors de son lancement (onglet Security) pour ne pas le faire fuiter. Il faudra aussi décocher Enable IP protection et Enable network policy dans l’onglet Security. Une seule personne à la fois peut se connecter à un service RStudio."
  },
  {
    "objectID": "docs/fr/configure.html#init",
    "href": "docs/fr/configure.html#init",
    "title": "Configuration des services",
    "section": "",
    "text": "Un lien vers un script shell (enchaînement de commandes linux) qui est exécuté juste après le lancement du service. Pratique pour automatiser la mise en place de certaines configurations.\nCe lien du script doit être accessible sur internet, par exemple sur https://git.lab.sspcloud.fr/ avec un projet public ou sur le stockage S3 avec un fichier public.\nExemple de script d’initialisation qui clone un projet à partir d’une instance Gitlab privée, configure les options globales de RStudio, ouvre automatiquement le projet RStudio cloné, installe et sélectionne la correction orthographique française, personnalise les bribes de codes (snippets).\n\n\n\n\n\n\nWarning\n\n\n\nLe script est exécuté en tant que superutilisateur (Root) et les fichiers qu’il crée sont ainsi la propriété du superutilisateur. Ceci génère des erreurs ensuite quand ces fichiers sont appelés, par exemple des fichiers de configuration de RStudio. Pour rendre à l’utilisateur normal (qui s’appelle onyxia) les droit sur son dossier personnel :\nchown -R ${USERNAME}:${GROUPNAME} ${HOME}\n\n\n\n\n\nDes options à passer au script d’initialisation, séparées par des espaces et que l’on peut ensuite appeler avec $1, $2…\nPar exemple si on inscrit dans le champ PersonalInitArgs fichier1.txt fichier2.txt, et qu’on utilise ce script d’initialisation :\n#!/bin/bash\ntouch $1\ntouch $2\nLe script créera via la commande touch deux fichiers fichier1.txt et fichier2.txt."
  },
  {
    "objectID": "docs/fr/configure.html#security",
    "href": "docs/fr/configure.html#security",
    "title": "Configuration des services",
    "section": "",
    "text": "C’est le mot de passe à saisir lorsqu’on ouvre un service, celui donné par “Copier le mot de passage” sur la page des services. Il est fourni par le paramètre général “Mot de passe pour vos services” que l’on trouve dans “Mon Compte” &gt; “Informations du compte”, sauf si on en a défini un particulier au niveau du service.\n\n\n\nSi coché, le service n’est accessible que par une seule IP, à décocher si l’on souhaite travailler de deux endroits différents."
  },
  {
    "objectID": "docs/fr/configure.html#git",
    "href": "docs/fr/configure.html#git",
    "title": "Configuration des services",
    "section": "",
    "text": "Pour apprendre à utiliser cet onglet, voir la page dédiée.\n\n\n\n\n\n\nWarning\n\n\n\nIl n’est pas possible de cloner automatiquement un projet privé d’une instance privée (c’est-à-dire autre que gitlab.com et github.com). Pour le faire, il faudra recourir à un script shell comme indiqué ici.\n\n\n\n\nSi coché, configure Git et tente un clone au démarrage du service.\n\n\n\nLe nom qui apparaîtra dans les commits (pas le nom d’utilisateur du compte Gitlab ou Github).\n\n\n\nL’adresse email qui apparaîtra dans les commits (pas forcément le mail associé au compte Gitlab ou Github).\n\n\n\n\n\n\nJeton d’accès défini sur la plateforme utilisée (Gitlab, Github…).\n\n\n\nL’URL obtenue sur la plateforme utilisée (Gitlab, Github…) en cliquant sur “Cloner” &gt; HTTPS.\nDe type :\nhttps://github.com/InseeFrLab/docs.sspcloud.fr.git\n\n\n\n\n\n\n\n\n\n\n\n\nPour apprendre à utiliser cet onglet, voir la page dédiée."
  },
  {
    "objectID": "docs/fr/principles.html",
    "href": "docs/fr/principles.html",
    "title": "Principes du Datalab",
    "section": "",
    "text": "Le projet Onyxia part du constat de difficultés communes rencontrées par les datascientists du secteur public :\n\ndes agents souvent isolés, du fait de la relative rareté des compétences data dans l’administration ;\ndes infrastructures inadaptées, aussi bien en matière de ressources que de technologies, qui constituent un frein à l’innovation ;\nune difficulté à passer de l’expérimentation à la mise en production, du fait de multiples séparations (séparation physique, langage de développement, modes de travail) entre les directions métier et la production informatique.\n\nFace à ce constat, le Datalab SSPCloud a été construit pour proposer une plateforme de mutualisation à plusieurs niveaux :\n\npartage d’une infrastructure moderne, centrée autour du déploiement de services via des conteneurs, et dimensionnée pour les usages de data science ;\npartage de méthodes, via une mutualisation des services de data science proposés, auxquels chacun peut contribuer ;\npartage de connaissances, via des formations associées au Datalab ainsi que la constitution de commaunautés d’entraide centrées sur son utilisation.\n\n\n\n\n\n\n\nNote\n\n\n\nOnyxia, Datalab SSP Cloud : quelles différences ?\nOnyxiaest un projet open-source qui propose une plateforme de services de data science, accessible via une application Web. Le Datalab SSP Cloudest une instance du projet Onyxia, hébergée à l’Insee.\n\n\n\n\n\nL’architecture du Datalab est basée sur un ensemble de principes fondamentaux :\n\nune production orientée data science, en proposant une infrastructure dimensionnée à la plupart des usages et un catalogue de services couvrant l’ensemble du cycle de vie des projets data ;\ndes choix qui favorisent l’autonomie des usagers, en évitant tout enfermement propriétaire et en permettant l’accès aux couches basses de l’infrastructure pour couvrir les besoins avancés et spécifiques ;\nun projet 100% cloud-natif, mais également cloud-agnostique, permettant un déploiement simple sur n’importe quelle infrastructure ;\nun projet complètement open-source, à la fois du point de vue de ses briques constitutives que de sa diffusion (licence MIT).\n\n\n\n\nLe Datalab est accessible via une interface utilisateur moderne et réactive, centrée sur l’expérience utilisateur. Celle-ci constitue le liant technique entre les différentes composantes d’Onyxia :\n\ndes technologies open-source qui constituent l’état de l’art du déploiement et de l’orchestration de conteneurs, du stockage et de la sécurité ;\nun catalogue de services et d’outils pour accompagner les projets de data science ;\nune plateforme de formation et de documentation pour faciliter l’onboarding sur les technologies proposées.\n\n\n\n\nBriques fondamentales du Datalab Onyxia\n\n\nLe catalogue de services est pensé de manière à accomoder l’essentiel des usages des data scientists, du développement en self-service à la mise en production de traitements ou d’application. L’ensemble du cycle de vie d’un projet data est ainsi couvert, et le catalogue des services est régulièrement étendu pour répondre aux nouveaux besoins des utilisateurs.\n\n\n\nUn catalogue de services complet pour les projets de data science\n\n\n\n\n\nLe projet du Datalab Onyxia est résolument ouvert, à de multiples niveaux :\n\nle Datalab est accessible via son interface Web à tous les agents du service public (via AgentConnect ou une adresse mail en gouv.fr) ainsi qu’aux élèves des écoles de statistique liées à l’Insee (Cefil, Ensai, Ensae) ;\nle code source ouvert et la modularité du projet rendent possible le déploiement d’une instance Onyxia personnalisée sur n’importe quelle infrastructure basée sur un cluster Kubernetes ;\nle projet est ouvert aux contributions extérieures, qu’elles concernent le catalogue des services, l’interface graphique ou l’agencement des briques logicielles qui le constituent."
  },
  {
    "objectID": "docs/fr/principles.html#une-plateforme-de-mutualisation",
    "href": "docs/fr/principles.html#une-plateforme-de-mutualisation",
    "title": "Principes du Datalab",
    "section": "",
    "text": "Le projet Onyxia part du constat de difficultés communes rencontrées par les datascientists du secteur public :\n\ndes agents souvent isolés, du fait de la relative rareté des compétences data dans l’administration ;\ndes infrastructures inadaptées, aussi bien en matière de ressources que de technologies, qui constituent un frein à l’innovation ;\nune difficulté à passer de l’expérimentation à la mise en production, du fait de multiples séparations (séparation physique, langage de développement, modes de travail) entre les directions métier et la production informatique.\n\nFace à ce constat, le Datalab SSPCloud a été construit pour proposer une plateforme de mutualisation à plusieurs niveaux :\n\npartage d’une infrastructure moderne, centrée autour du déploiement de services via des conteneurs, et dimensionnée pour les usages de data science ;\npartage de méthodes, via une mutualisation des services de data science proposés, auxquels chacun peut contribuer ;\npartage de connaissances, via des formations associées au Datalab ainsi que la constitution de commaunautés d’entraide centrées sur son utilisation.\n\n\n\n\n\n\n\nNote\n\n\n\nOnyxia, Datalab SSP Cloud : quelles différences ?\nOnyxiaest un projet open-source qui propose une plateforme de services de data science, accessible via une application Web. Le Datalab SSP Cloudest une instance du projet Onyxia, hébergée à l’Insee."
  },
  {
    "objectID": "docs/fr/principles.html#principes-fondamentaux",
    "href": "docs/fr/principles.html#principes-fondamentaux",
    "title": "Principes du Datalab",
    "section": "",
    "text": "L’architecture du Datalab est basée sur un ensemble de principes fondamentaux :\n\nune production orientée data science, en proposant une infrastructure dimensionnée à la plupart des usages et un catalogue de services couvrant l’ensemble du cycle de vie des projets data ;\ndes choix qui favorisent l’autonomie des usagers, en évitant tout enfermement propriétaire et en permettant l’accès aux couches basses de l’infrastructure pour couvrir les besoins avancés et spécifiques ;\nun projet 100% cloud-natif, mais également cloud-agnostique, permettant un déploiement simple sur n’importe quelle infrastructure ;\nun projet complètement open-source, à la fois du point de vue de ses briques constitutives que de sa diffusion (licence MIT)."
  },
  {
    "objectID": "docs/fr/principles.html#offre-de-services",
    "href": "docs/fr/principles.html#offre-de-services",
    "title": "Principes du Datalab",
    "section": "",
    "text": "Le Datalab est accessible via une interface utilisateur moderne et réactive, centrée sur l’expérience utilisateur. Celle-ci constitue le liant technique entre les différentes composantes d’Onyxia :\n\ndes technologies open-source qui constituent l’état de l’art du déploiement et de l’orchestration de conteneurs, du stockage et de la sécurité ;\nun catalogue de services et d’outils pour accompagner les projets de data science ;\nune plateforme de formation et de documentation pour faciliter l’onboarding sur les technologies proposées.\n\n\n\n\nBriques fondamentales du Datalab Onyxia\n\n\nLe catalogue de services est pensé de manière à accomoder l’essentiel des usages des data scientists, du développement en self-service à la mise en production de traitements ou d’application. L’ensemble du cycle de vie d’un projet data est ainsi couvert, et le catalogue des services est régulièrement étendu pour répondre aux nouveaux besoins des utilisateurs.\n\n\n\nUn catalogue de services complet pour les projets de data science"
  },
  {
    "objectID": "docs/fr/principles.html#un-projet-ouvert",
    "href": "docs/fr/principles.html#un-projet-ouvert",
    "title": "Principes du Datalab",
    "section": "",
    "text": "Le projet du Datalab Onyxia est résolument ouvert, à de multiples niveaux :\n\nle Datalab est accessible via son interface Web à tous les agents du service public (via AgentConnect ou une adresse mail en gouv.fr) ainsi qu’aux élèves des écoles de statistique liées à l’Insee (Cefil, Ensai, Ensae) ;\nle code source ouvert et la modularité du projet rendent possible le déploiement d’une instance Onyxia personnalisée sur n’importe quelle infrastructure basée sur un cluster Kubernetes ;\nle projet est ouvert aux contributions extérieures, qu’elles concernent le catalogue des services, l’interface graphique ou l’agencement des briques logicielles qui le constituent."
  },
  {
    "objectID": "docs/fr/storage.html",
    "href": "docs/fr/storage.html",
    "title": "Stockage de données",
    "section": "",
    "text": "La solution de stockage de fichiers associée au Datalab est MinIO, un système de stockage d’objets basé sur le cloud, compatible avec l’API S3 d’Amazon. Concrètement, cela a plusieurs avantages :\n\nles fichiers stockés sont accessibles facilement et à n’importe quel endroit : un fichier est accessible directement via une simple URL, qui peut être partagée ;\nil est possible d’accéder aux fichiers stockés directement dans les services de data science (R, Python…) proposés sur le Datalab, sans avoir besoin de copier les fichiers localement au préalable, ce qui améliore fortement la reproductibilité des analyses.\n\n\n\n\n\n\nLa page Mes fichiers du Datalab prend la forme d’un explorateur de fichiers présentant les différents buckets (dépôts) auxquels l’utilisateur a accès.\nChaque utilisateur dispose par défaut d’un bucket personnel pour stocker ses fichiers. Au sein de ce bucket, deux options sont possibles :\n\n“créer un répertoire” : crée un répertoire dans le bucket/répertoire courant, de manière hiérarchique, comme dans un système de fichiers traditionnel ;\n“uploader** un fichier**” : upload un ou plusieurs fichiers dans le répertoire courant.\n\n\n\n\n\n\n\nNote\n\n\n\nL’interface graphique du stockage de données sur le Datalab est encore en cours de construction. Elle peut à ce titre présenter des problèmes de réactivité. Pour des opérations fréquentes sur le stockage de fichiers, il peut être préférable d’interagir avec MinIO via le terminal.\n\n\n\n\n\nEn cliquant sur un fichier dans son bucket personnel, on accède à sa page de caractéristiques. Sur celle-ci, il est notamment possible de changer le statut de diffusion du fichier. Changer le statut du fichier de “privé” à “public” permet d’obtenir un lien de diffusion, qui peut alors être transmis pour téléchargement du fichier. Le statut “public” ne donne aux autres utilisateurs que des droits en lecture, la modification ou la suppression de fichiers personnels par d’autres utilisateurs est impossible.\nPour simplifier la mise à disposition en lecture de plusieurs fichiers — dans le cadre d’une formation par exemple — il est possible de créer un dossier “diffusion” dans son bucket personnel. Par défaut, tous les fichiers présents dans ce dossier ont un statut de diffusion public.\n\n\n\n\n\n\nNote\n\n\n\nDans le cadre de projets collaboratifs, il peut être intéressant pour les différents participants d’avoir accès à un espace de stockage commun. Il est possible pour cet usage de créer des buckets partagés sur MinIO. N’hésitez pas à nous contacter via les canaux précisés sur la page “Première utilisation” si vous souhaitez porter des projets open-data sur le Datalab.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nConformément aux conditions d’utilisation, seuls des données de type open data ou ne présentant aucune sensibilité peuvent être stockées sur le Datalab. Le fait qu’un fichier ait un statut de diffusion “privé” ne suffit pas à garantir une parfaite confidentialité.\n\n\n\n\n\n\nLes identifiants d’accès nécessaires pour accéder à des données sur MinIO sont pré-configurés dans les différents services du Datalab, accessibles sous la forme de variables d’environnement. Ainsi, l’import et l’export de fichiers à partir des services est grandement facilité.\n\n\n\nRPythonmc\n\n\nEn R, l’interaction avec un système de fichiers compatible S3 est rendu possible par la librairie aws.s3.\nlibrary(aws.s3)\n\n\nEn Python, l’interaction avec un système de fichiers compatible S3 est rendu possible par deux librairies :\n\nBoto3, une librairie créée et maintenue par Amazon ;\nS3Fs, une librairie qui permet d’interagir avec les fichiers stockés à l’instar d’un filesystem classique.\n\nPour cette raison et parce que S3Fs est utilisée par défaut par la librairie pandas pour gérer les connections S3, nous allons présenter la gestion du stockage sur MinIO via Python à travers cette librairie.\nimport os\nimport s3fs\n\n# Create filesystem object\nS3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n\n\nMinIO propose un client en ligne de commande (ùc) qui permet d’interagir avec le système de stockage à la manière d’un filesystem UNIX classique. Ce client est installé par défaut et accessible via un terminal dans les différents services du Datalab.\nLe client MinIO propose les commandes UNIX de base, telles que ls, cat, cp, etc. La liste complète est disponible dans la documentation du client.\n\n\n\nLister les fichiers d’un bucket\n\nRPythonmc\n\n\naws.s3::get_bucket(\"donnees-insee\", region = \"\")\n\n\nfs.ls(\"donnees-insee\")\n\n\nLe stockage du Datalab est accessible via l’alias s3. Par exemple, pour lister les fichiers du bucket donnees-insee :\nmc ls s3/donnees-insee\n\n\n\nImporter des données\n\nRPythonmc\n\n\nBUCKET &lt;- \"donnees-insee\"\nFILE_KEY_S3 &lt;- \"diffusion/BPE/2019/BPE_ENS.csv\"\n\ndf &lt;- \n  aws.s3::s3read_using(\n    FUN = readr::read_delim,\n    # Mettre les options de FUN ici\n    delim = \";\",\n    object = FILE_KEY_S3,\n    bucket = BUCKET,\n    opts = list(\"region\" = \"\")\n  )\n\n\nLe package S3Fs permet d’interagir avec les fichiers stockés sur MinIO comme s’il s’agissait de fichiers locaux. La syntaxe est donc très familière pour les utilisateurs de Python. Par exemple, pour importer/exporter des données tabulaires via pandas :\nimport pandas as pd\n\nBUCKET = \"donnees-insee\"\nFILE_KEY_S3 = \"diffusion/BPE/2019/BPE_ENS.csv\"\nFILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n\nwith fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n    df_bpe = pd.read_csv(file_in, sep=\";\")\n\n\nPour copier les données d’un bucket sur MinIO vers le service local :\nmc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv\n\n\n\n\n\n\nWarning\n\n\n\nCopier les fichiers dans le service local n’est généralement pas une bonne pratique : cela limite la reproductibilité des analyses, et devient rapidement impossible avec des volumes importants de données. Il est donc préférable de prendre l’habitude d’importer les données comme des fichiers directement dans R/Python.\n\n\n\n\n\nExporter des données vers MinIO\n\nRPythonmc\n\n\nBUCKET_OUT = \"&lt;mon_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"mon_dossier/BPE_ENS.csv\"\n\naws.s3::s3write_using(\n    df,\n    FUN = readr::write_csv,\n    object = FILE_KEY_OUT_S3,\n    bucket = BUCKET_OUT,\n    opts = list(\"region\" = \"\")\n)\n\n\nBUCKET_OUT = \"&lt;mon_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"mon_dossier/BPE_ENS.csv\"\nFILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n\nwith fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n    df_bpe.to_csv(file_out)\n\n\nPour copier les données du service local vers un bucket sur MinIO:\nmc cp chemin/local/vers/mon/fichier.csv s3/&lt;mon_bucket&gt;/chemin/distant/vers/mon/fichier.csv\n\n\n\n**Renouveler des jetons d’accès (*tokens) périmés**\nL’accès au stockage MinIO est possible via un token (jeton d’accès) personnel, valide 24h, et automatiquement régénéré à échéances régulières sur le SSP Cloud. Lorsqu’un token a expiré, les services créés avant la date d’expiration (avec le précédent token) ne peuvent plus accéder au stockage ; le service concerné apparaît alors marqué en rouge dans la page Mes Services. Dans ce cas, deux possibilités : - ouvrir un nouveau service sur le Datalab, qui aura par défaut un nouveau token à jour - remplacer manuellement les jetons périmés par des nouveaux. Des scripts indiquant la manière de faire pour les différentes utilisations de MinIO (R/Python/mc) sont disponibles ici. Il suffit de choisir le script pertinent et de l’exécuter dans son environnement de travail courant."
  },
  {
    "objectID": "docs/fr/storage.html#principes",
    "href": "docs/fr/storage.html#principes",
    "title": "Stockage de données",
    "section": "",
    "text": "La solution de stockage de fichiers associée au Datalab est MinIO, un système de stockage d’objets basé sur le cloud, compatible avec l’API S3 d’Amazon. Concrètement, cela a plusieurs avantages :\n\nles fichiers stockés sont accessibles facilement et à n’importe quel endroit : un fichier est accessible directement via une simple URL, qui peut être partagée ;\nil est possible d’accéder aux fichiers stockés directement dans les services de data science (R, Python…) proposés sur le Datalab, sans avoir besoin de copier les fichiers localement au préalable, ce qui améliore fortement la reproductibilité des analyses."
  },
  {
    "objectID": "docs/fr/storage.html#gérer-ses-données",
    "href": "docs/fr/storage.html#gérer-ses-données",
    "title": "Stockage de données",
    "section": "",
    "text": "La page Mes fichiers du Datalab prend la forme d’un explorateur de fichiers présentant les différents buckets (dépôts) auxquels l’utilisateur a accès.\nChaque utilisateur dispose par défaut d’un bucket personnel pour stocker ses fichiers. Au sein de ce bucket, deux options sont possibles :\n\n“créer un répertoire” : crée un répertoire dans le bucket/répertoire courant, de manière hiérarchique, comme dans un système de fichiers traditionnel ;\n“uploader** un fichier**” : upload un ou plusieurs fichiers dans le répertoire courant.\n\n\n\n\n\n\n\nNote\n\n\n\nL’interface graphique du stockage de données sur le Datalab est encore en cours de construction. Elle peut à ce titre présenter des problèmes de réactivité. Pour des opérations fréquentes sur le stockage de fichiers, il peut être préférable d’interagir avec MinIO via le terminal.\n\n\n\n\n\nEn cliquant sur un fichier dans son bucket personnel, on accède à sa page de caractéristiques. Sur celle-ci, il est notamment possible de changer le statut de diffusion du fichier. Changer le statut du fichier de “privé” à “public” permet d’obtenir un lien de diffusion, qui peut alors être transmis pour téléchargement du fichier. Le statut “public” ne donne aux autres utilisateurs que des droits en lecture, la modification ou la suppression de fichiers personnels par d’autres utilisateurs est impossible.\nPour simplifier la mise à disposition en lecture de plusieurs fichiers — dans le cadre d’une formation par exemple — il est possible de créer un dossier “diffusion” dans son bucket personnel. Par défaut, tous les fichiers présents dans ce dossier ont un statut de diffusion public.\n\n\n\n\n\n\nNote\n\n\n\nDans le cadre de projets collaboratifs, il peut être intéressant pour les différents participants d’avoir accès à un espace de stockage commun. Il est possible pour cet usage de créer des buckets partagés sur MinIO. N’hésitez pas à nous contacter via les canaux précisés sur la page “Première utilisation” si vous souhaitez porter des projets open-data sur le Datalab.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nConformément aux conditions d’utilisation, seuls des données de type open data ou ne présentant aucune sensibilité peuvent être stockées sur le Datalab. Le fait qu’un fichier ait un statut de diffusion “privé” ne suffit pas à garantir une parfaite confidentialité."
  },
  {
    "objectID": "docs/fr/storage.html#utiliser-des-données-stockées-sur-minio",
    "href": "docs/fr/storage.html#utiliser-des-données-stockées-sur-minio",
    "title": "Stockage de données",
    "section": "",
    "text": "Les identifiants d’accès nécessaires pour accéder à des données sur MinIO sont pré-configurés dans les différents services du Datalab, accessibles sous la forme de variables d’environnement. Ainsi, l’import et l’export de fichiers à partir des services est grandement facilité.\n\n\n\nRPythonmc\n\n\nEn R, l’interaction avec un système de fichiers compatible S3 est rendu possible par la librairie aws.s3.\nlibrary(aws.s3)\n\n\nEn Python, l’interaction avec un système de fichiers compatible S3 est rendu possible par deux librairies :\n\nBoto3, une librairie créée et maintenue par Amazon ;\nS3Fs, une librairie qui permet d’interagir avec les fichiers stockés à l’instar d’un filesystem classique.\n\nPour cette raison et parce que S3Fs est utilisée par défaut par la librairie pandas pour gérer les connections S3, nous allons présenter la gestion du stockage sur MinIO via Python à travers cette librairie.\nimport os\nimport s3fs\n\n# Create filesystem object\nS3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\nfs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n\n\nMinIO propose un client en ligne de commande (ùc) qui permet d’interagir avec le système de stockage à la manière d’un filesystem UNIX classique. Ce client est installé par défaut et accessible via un terminal dans les différents services du Datalab.\nLe client MinIO propose les commandes UNIX de base, telles que ls, cat, cp, etc. La liste complète est disponible dans la documentation du client.\n\n\n\nLister les fichiers d’un bucket\n\nRPythonmc\n\n\naws.s3::get_bucket(\"donnees-insee\", region = \"\")\n\n\nfs.ls(\"donnees-insee\")\n\n\nLe stockage du Datalab est accessible via l’alias s3. Par exemple, pour lister les fichiers du bucket donnees-insee :\nmc ls s3/donnees-insee\n\n\n\nImporter des données\n\nRPythonmc\n\n\nBUCKET &lt;- \"donnees-insee\"\nFILE_KEY_S3 &lt;- \"diffusion/BPE/2019/BPE_ENS.csv\"\n\ndf &lt;- \n  aws.s3::s3read_using(\n    FUN = readr::read_delim,\n    # Mettre les options de FUN ici\n    delim = \";\",\n    object = FILE_KEY_S3,\n    bucket = BUCKET,\n    opts = list(\"region\" = \"\")\n  )\n\n\nLe package S3Fs permet d’interagir avec les fichiers stockés sur MinIO comme s’il s’agissait de fichiers locaux. La syntaxe est donc très familière pour les utilisateurs de Python. Par exemple, pour importer/exporter des données tabulaires via pandas :\nimport pandas as pd\n\nBUCKET = \"donnees-insee\"\nFILE_KEY_S3 = \"diffusion/BPE/2019/BPE_ENS.csv\"\nFILE_PATH_S3 = BUCKET + \"/\" + FILE_KEY_S3\n\nwith fs.open(FILE_PATH_S3, mode=\"rb\") as file_in:\n    df_bpe = pd.read_csv(file_in, sep=\";\")\n\n\nPour copier les données d’un bucket sur MinIO vers le service local :\nmc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv\n\n\n\n\n\n\nWarning\n\n\n\nCopier les fichiers dans le service local n’est généralement pas une bonne pratique : cela limite la reproductibilité des analyses, et devient rapidement impossible avec des volumes importants de données. Il est donc préférable de prendre l’habitude d’importer les données comme des fichiers directement dans R/Python.\n\n\n\n\n\nExporter des données vers MinIO\n\nRPythonmc\n\n\nBUCKET_OUT = \"&lt;mon_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"mon_dossier/BPE_ENS.csv\"\n\naws.s3::s3write_using(\n    df,\n    FUN = readr::write_csv,\n    object = FILE_KEY_OUT_S3,\n    bucket = BUCKET_OUT,\n    opts = list(\"region\" = \"\")\n)\n\n\nBUCKET_OUT = \"&lt;mon_bucket&gt;\"\nFILE_KEY_OUT_S3 = \"mon_dossier/BPE_ENS.csv\"\nFILE_PATH_OUT_S3 = BUCKET_OUT + \"/\" + FILE_KEY_OUT_S3\n\nwith fs.open(FILE_PATH_OUT_S3, 'w') as file_out:\n    df_bpe.to_csv(file_out)\n\n\nPour copier les données du service local vers un bucket sur MinIO:\nmc cp chemin/local/vers/mon/fichier.csv s3/&lt;mon_bucket&gt;/chemin/distant/vers/mon/fichier.csv\n\n\n\n**Renouveler des jetons d’accès (*tokens) périmés**\nL’accès au stockage MinIO est possible via un token (jeton d’accès) personnel, valide 24h, et automatiquement régénéré à échéances régulières sur le SSP Cloud. Lorsqu’un token a expiré, les services créés avant la date d’expiration (avec le précédent token) ne peuvent plus accéder au stockage ; le service concerné apparaît alors marqué en rouge dans la page Mes Services. Dans ce cas, deux possibilités : - ouvrir un nouveau service sur le Datalab, qui aura par défaut un nouveau token à jour - remplacer manuellement les jetons périmés par des nouveaux. Des scripts indiquant la manière de faire pour les différentes utilisations de MinIO (R/Python/mc) sont disponibles ici. Il suffit de choisir le script pertinent et de l’exécuter dans son environnement de travail courant."
  }
]