::: {.content-visible when-profile="fr"}

# Stockage de données

## Principes

La solution de stockage de fichiers associée au Datalab est [MinIO](https://min.io), un système de stockage d'objets basé sur le cloud, compatible avec l'API S3 d'Amazon. Concrètement, cela a plusieurs avantages :

* les fichiers stockés sont accessibles facilement et à n'importe quel endroit : un fichier est accessible directement via une simple URL, qui peut être partagée ;
* il est possible d'accéder aux fichiers stockés directement dans les services de _data science_ (R, Python...) proposés sur le Datalab, sans avoir besoin de copier les fichiers localement au préalable, ce qui améliore fortement la reproductibilité des analyses.

![MinIO Schema](./img/minio.svg)

## Gérer ses données

### Importer des données

La page [Mes fichiers](https://datalab.sspcloud.fr/my-files/) du Datalab prend la forme d'un explorateur de fichiers présentant les différents _buckets_ (dépôts) auxquels l’utilisateur a accès.

Chaque utilisateur dispose par défaut d'un _bucket_ personnel pour stocker ses fichiers. Au sein de ce _bucket_, deux options sont possibles :

* "**créer un répertoire**" : crée un répertoire dans le _bucket_/répertoire courant, de manière hiérarchique, comme dans un système de fichiers traditionnel ;
* "_**uploader**_** un fichier**" : _upload_ un ou plusieurs fichiers dans le répertoire courant.

::: {.callout-note}
L'interface graphique du stockage de données sur le Datalab est encore en cours de construction. Elle peut à ce titre présenter des problèmes de réactivité. Pour des opérations fréquentes sur le stockage de fichiers, il peut être préférable d'interagir avec MinIO via le terminal.
:::

### Partager des données

La politique de contrôle d'accès au stockage S3 **interdit par défaut** l'accès aux buckets des autres utilisateurs du SSPCloud,
**à l'exception du dossier `diffusion`** directement à la racine de chaque bucket qui est accessible en lecture seule à l'ensemble des utilisateurs.

Pour partager des données, il vous suffit ainsi de créer un dossier `diffusion` directement à la racine de votre bucket personnel
et d'y déposer les éléments que vous souhaitez rendre accessibles aux autres utilisateurs de la plateforme.

Il est également possible de configurer manuellement la politique de partage afin d'ajuster plus finement les permissions.
Pour ce faire il est actuellement nécessaire d'interagir avec **MinIO** via un terminal qui vous permet d'accéder à des fonctionnalités avancées.

En utilisant [MinIO Client](https://min.io/docs/minio/linux/reference/minio-mc.html), vous pouvez rendre un dossier publiquement accessible avec la commande suivante :
```bash
mc anonymous set download s3/<votre nom d'utilisateur>/<le dossier que vous voulez rendre publique>
```
Cette commande permet d’accorder des droits de téléchargement publics pour le dossier spécifié.

Pour des besoins plus spécifiques de contrôle d'accès, comme la gestion de droits d'accès anonymes ou restreints,
consultez la [documentation officielle de MinIO Client](https://min.io/docs/minio/linux/reference/minio-mc/mc-anonymous.html).

Un tutoriel interactif expliquant comment utiliser VSCode au sein du datalab pour partager un dossier S3
est disponible [ici](https://app.tango.us/app/workflow/Mettre-un-dossier-S3-en-publique-90b131c8ebff4a71904d1b0bdf3e108b)

::: {.callout-note}
Dans le cadre de projets collaboratifs, il peut être intéressant pour les différents participants d'avoir accès à un espace de stockage commun.
Il est possible pour cet usage de créer des _buckets_ partagés sur MinIO.
N'hésitez pas à nous contacter via les canaux précisés sur la page "[Première utilisation](discover.qmd)" si vous souhaitez porter des projets _open-data_ sur le Datalab.
:::

::: {.callout-warning}
Conformément aux [conditions d'utilisation](https://datalab.sspcloud.fr/custom-resources/tos\_en.md), seules des données de type _open data_ ou ne présentant aucune sensibilité peuvent être stockées sur le Datalab.
Le fait qu'un fichier ait un statut de diffusion "privé" ne suffit pas à garantir sa confidentialité.
:::

## Utiliser des données stockées sur MinIO

Les identifiants d'accès nécessaires pour accéder à des données sur MinIO sont pré-configurés dans les différents services du Datalab, accessibles sous la forme de [variables d'environnement](secrets.qmd). Ainsi, l'import et l'export de fichiers à partir des services est grandement facilité.

### Configuration

::: {.panel-tabset}

#### R

En R, l'interaction avec un système de fichiers compatible S3 est rendu possible par la librairie `aws.s3`.

```r
library(aws.s3)
```

#### Python

En Python, l'interaction avec un système de fichiers compatible S3 est rendu possible par deux librairies :

* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), une librairie créée et maintenue par Amazon ;
* [S3Fs](https://s3fs.readthedocs.io/en/latest/), une librairie qui permet d'interagir avec les fichiers stockés à l'instar d'un _filesystem_ classique.

Pour cette raison et parce que S3Fs est utilisée par défaut par la librairie [pandas](https://pandas.pydata.org) pour gérer les connections S3, nous allons présenter la gestion du stockage sur MinIO via Python à travers cette librairie.

```python
import os
import s3fs

# Create filesystem object
S3_ENDPOINT_URL = "https://" + os.environ["AWS_S3_ENDPOINT"]
fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})
```

#### mc (terminal)

MinIO propose un client en ligne de commande (`ùc`) qui permet d’interagir avec le système de stockage à la manière d'un _filesystem_ UNIX classique. Ce client est installé par défaut et accessible via un terminal dans les différents services du Datalab.

Le client MinIO propose les commandes UNIX de base, telles que ls, cat, cp, etc. La liste complète est disponible dans la [documentation du client](https://docs.min.io/docs/minio-client-complete-guide.html).

:::

### Lister les fichiers d'un *bucket*

::: {.panel-tabset}

#### R

```r
aws.s3::get_bucket("donnees-insee", region = "")
```

#### Python

```python
fs.ls("donnees-insee")
```

#### mc (terminal)

Le stockage du Datalab est accessible via l'alias `s3`. Par exemple, pour lister les fichiers du bucket `donnees-insee` :

```bash
mc ls s3/donnees-insee
```

:::

### Importer des données

::: {.panel-tabset}

#### R

```r
BUCKET <- "donnees-insee"
FILE_KEY_S3 <- "diffusion/BPE/2019/BPE_ENS.csv"

df <-
  aws.s3::s3read_using(
    FUN = readr::read_delim,
    # Mettre les options de FUN ici
    delim = ";",
    object = FILE_KEY_S3,
    bucket = BUCKET,
    opts = list("region" = "")
  )
```

#### Python

Le package S3Fs permet d'interagir avec les fichiers stockés sur MinIO comme s'il s'agissait de fichiers locaux. La syntaxe est donc très familière pour les utilisateurs de Python. Par exemple, pour importer/exporter des données tabulaires via `pandas` :

```python
import pandas as pd

BUCKET = "donnees-insee"
FILE_KEY_S3 = "diffusion/BPE/2019/BPE_ENS.csv"
FILE_PATH_S3 = BUCKET + "/" + FILE_KEY_S3

with fs.open(FILE_PATH_S3, mode="rb") as file_in:
    df_bpe = pd.read_csv(file_in, sep=";")
```

#### mc (terminal)

Pour copier les données d'un bucket sur MinIO vers le service local :

```bash
mc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv
```

::: {.callout-warning}
**Copier les fichiers dans le service local n'est généralement pas une bonne pratique** : cela limite la reproductibilité des analyses, et devient rapidement impossible avec des volumes importants de données. Il est donc préférable de prendre l'habitude d'importer les données comme des fichiers directement dans `R`/`Python`.
:::

:::

### Exporter des données vers MinIO

::: {.panel-tabset}

#### R

```r
BUCKET_OUT = "<mon_bucket>"
FILE_KEY_OUT_S3 = "mon_dossier/BPE_ENS.csv"

aws.s3::s3write_using(
    df,
    FUN = readr::write_csv,
    object = FILE_KEY_OUT_S3,
    bucket = BUCKET_OUT,
    opts = list("region" = "")
)
```

#### Python

```python
BUCKET_OUT = "<mon_bucket>"
FILE_KEY_OUT_S3 = "mon_dossier/BPE_ENS.csv"
FILE_PATH_OUT_S3 = BUCKET_OUT + "/" + FILE_KEY_OUT_S3

with fs.open(FILE_PATH_OUT_S3, 'w') as file_out:
    df_bpe.to_csv(file_out)
```

#### mc (terminal)

Pour copier les données du service local vers un bucket sur MinIO:

```bash
mc cp chemin/local/vers/mon/fichier.csv s3/<mon_bucket>/chemin/distant/vers/mon/fichier.csv
```

:::

## Renouveler des jetons d'accès (*tokens*) périmés

L'accès au stockage MinIO est possible via un _token_ (jeton d'accès) personnel, valide 7 jours, et automatiquement régénéré à échéances régulières sur le SSP Cloud. Lorsqu'un token a expiré, les services créés avant la date d'expiration (avec le précédent token) ne peuvent plus accéder au stockage ; le service concerné apparaît alors marqué en rouge dans la page [Mes Services](https://datalab.sspcloud.fr/my-services). Dans ce cas, deux possibilités :

- ouvrir un nouveau service sur le Datalab, qui aura par défaut un nouveau token à jour ;

- remplacer manuellement les jetons périmés par des nouveaux. Des scripts indiquant la manière de faire pour les différentes utilisations de MinIO (`R`/`Python`/`mc`) sont disponibles [ici](https://datalab.sspcloud.fr/account/storage). Il suffit de choisir le script pertinent et de l'exécuter dans son environnement de travail courant.

## Usages avancés

### Créer un compte de service

Pour des raisons de sécurité, l'authentification à MinIO utilisée par défault dans les services interactifs du SSP Cloud repose un sur jeton d'accès temporaire.
Dans le cadre de projets impliquant des traitements périodiques ou le déploiement d'applications, on peut avoir besoin d'un accès plus pérenne à des données sur MinIO.

Dans ce cas, on utilise un compte de service, c'est à dire un compte qui est rattaché à un certain projet ou une certaine application plutôt qu'à une personne. En termes techniques, au lieu de s'authentifier à MinIO via un triplet (access key id, secret access key et session token), on va utiliser un couple (access key id, secret access key) qui donne des permissions en lecture/écriture à un certain bucket de projet.

La procédure de création d'un compte de service est décrite ci-dessous.

::: {.panel-tabset}

#### Interface graphique

- Ouvrir la [console MinIO](https://minio-console.lab.sspcloud.fr)

- Ouvrir l'onglet `Access Keys`

- Les informations du compte de service sont pré-générées. Il est possible de modifier l'access-key pour lui donner un nom plus simple.

- La `policy` précisant les droits est également pré-générée. Idéalement, on restreint la policy pour qu'elle ne concerne que le/les bucket(s) du projet.

- Une fois le compte de service généré, l'access-key et la secret-access-key peuvent être utilisées pour authentifier les services / applications au bucket spécifié

#### Terminal (mc)

- Créer un service sur le SSP Cloud avec des accès MinIO à jour. Confirmer que la connection fonctionne avec :

```bash
mc ls s3/<nom_utilisateur>
```

- Générer un fichier `policy.json` avec le contenu suivant, en remplaçant (deux fois) `projet-<mon_projet>` par le nom du bucket concerné :

```json
{
    "Version": "2012-10-17",
    "Statement": [
     {
      "Effect": "Allow",
      "Action": [
       "s3:*"
      ],
      "Resource": [
       "arn:aws:s3:::projet-<mon_projet>",
       "arn:aws:s3:::projet-<mon_projet>/*"
      ]
     }
    ]
  }
```

- Dans un terminal, générer le compte de service avec la commande suivante :

```bash
mc admin user svcacct add s3 $AWS_ACCESS_KEY_ID --access-key="<access-key>" --secret-key="<secret-access-key>" --policy="policy.json"
```

en remplaçant `<access-key>` et `<secret-access-key>` par des noms de votre choix. Idéalement, on donnera un nom simple comme access-key (ex : `sa-projet-nomduprojet`) mais une clé complexe comme secret-access-key, générable par exemple avec le client `gpg` :

```bash
gpg --gen-random --armor 1 16
```

- Vous pouvez désormais utiliser l'access-key et la secret-access-key pour authentifier les services / applications au bucket spécifié.

:::

::: {.callout-warning}
Attention, les informations d'authentification générées n'apparaissent qu'une seule fois. Elles peuvent ensuite être stockées dans un gestionnaire de mot de passe, un service de stockage de secrets comme [Vault](https://datalab.sspcloud.fr/my-secrets), ou bien via la feature d'[options de projet](https://datalab.sspcloud.fr/project-settings) d'Onyxia qui permet d'importer le compte de service directement dans les services au moment de leur configuration.
:::

:::










::: {.content-visible when-profile="en"}

# Data Storage

## Principles

The file storage solution associated with Datalab is [MinIO](https://min.io), an object storage system based on the cloud, compatible with Amazon's S3 API. In practice, this has several advantages:

* Stored files are easily accessible from anywhere: a file can be accessed directly via a simple URL, which can be shared.
* It is possible to access the stored files directly within the data science services (R, Python, etc.) offered on Datalab, without the need to copy the files locally beforehand, greatly improving the reproducibility of analyses.

![MinIO Schema](./img/minio.svg)

## Managing your data

### Importing data

The [My Files](https://datalab.sspcloud.fr/mes-fichiers/) page in Datalab takes the form of a file explorer showing the different buckets (repositories) to which the user has access.

Each user has a personal bucket by default to store their files. Within this bucket, two options are possible:

* "**Create a directory**": Creates a directory in the current bucket/directory hierarchically, similar to a traditional file system.
* "**Upload a file**": Uploads one or multiple files to the current directory.

::: {.callout-note}
The graphical interface for data storage in Datalab is still under construction. As such, it may experience responsiveness issues. For frequent operations on file storage, it may be preferable to interact with MinIO via the terminal.
:::

### Sharing data

The default access policy of the S3 storage **forbids all access** to a bucket by any third party users of the **SSPCLoud**.
The **only exception** is the `diffusion` folder located directly at the root of each bucket for which other users have read-only access by default.

Therefore a straightforward way to share files for, say, a training session, is to create a `diffusion` folder in the user's personal bucket
and use it to store all resources meant to be shared with other users of the plateform.

By clicking on a file in their personal bucket, the user can access its characteristics page.
On this page, it is also possible to **manually change the diffusion status of the file**.
Changing the status of the file from "private" to "public" generates a **diffusion link**, which can then be shared for downloading the file.
The "public" status only grants read-only access rights to other users, and modifying or deleting other users' personal files is not possible.

::: {.callout-note}
For collaborative projects, it can be beneficial for different participants to have access to a shared storage space.
It is possible to create shared buckets on MinIO for this purpose.
Feel free to contact us via the channels specified on the "[First Use](discover.qmd)" page if you wish to work on open data projects on the SSPCloud Datalab.
:::

::: {.callout-warning}
In accordance with the [terms of use](https://datalab.sspcloud.fr/custom-resources/tos\_fr.md), only non-sensitive data (e.g. *open data*) may be stored on the SSPCloud Datalab.
Setting a file's diffusion status to "private" does not guarantee its confidentiality.
:::

## Using data stored on MinIO

The access credentials needed to access data on MinIO are pre-configured in the various Datalab services, accessible in the form of [environment variables](secrets.qmd). This greatly facilitates importing and exporting files from the services.

### Configuration

::: {.panel-tabset}

#### R

In R, interaction with an S3-compatible file system is made possible by the `aws.s3` library.

```r
library(aws.s3)
```

#### Python

In Python, interaction with an S3-compatible file system is made possible by two libraries:

* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), a library created and maintained by Amazon.
* [S3Fs](https://s3fs.readthedocs.io/en/latest/), a library that allows interaction with stored files similar to a classic filesystem.

For this reason, and because S3Fs is used by default by the [pandas](https://pandas.pydata.org) library to manage S3 connections, we will present how to manage storage on MinIO via Python using this library.

```python
import os
import s3fs

# Create filesystem object
S3_ENDPOINT_URL = "https://" + os.environ["AWS_S3_ENDPOINT"]
fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})
```

#### mc

MinIO offers a command-line client (`mc`) that allows interaction with the storage system in a manner similar to a classic UNIX filesystem. This client is installed by default and accessible via a terminal in the various Datalab services.

The MinIO client offers basic UNIX commands such as ls, cat, cp, etc. The complete list is available in the [client documentation](https://docs.min.io/docs/minio-client-complete-guide.html).

:::

### Listing the files in a bucket

::: {.panel-tabset}

#### R

```r
aws.s3::get_bucket("donnees-insee", region = "")
```

#### Python

```python
fs.ls("donnees-insee")
```

#### mc

The Datalab storage is accessible via the alias `s3`. For example, to list the files in the bucket `donnees-insee`:

```bash
mc ls s3/donnees-insee
```

:::

### Importing data in a service

::: {.panel-tabset}

#### R

```r
BUCKET <- "donnees-insee"
FILE_KEY_S3 <- "diffusion/BPE/2019/BPE_ENS.csv"

df <-
  aws.s3::s3read_using(
    FUN = readr::read_delim,
    # Put FUN options here
    delim = ";",
    object = FILE_KEY_S3,
    bucket = BUCKET,
    opts = list("region" = "")
  )
```

#### Python

The S3Fs package allows you to interact with files stored on MinIO as if they were local files. The syntax is therefore very familiar to Python users. For example, to import/export tabular data via `pandas`:

```python
import pandas as pd

BUCKET = "donnees-insee"
FILE_KEY_S3 = "diffusion/BPE/2019/BPE_ENS.csv"
FILE_PATH_S3 = BUCKET + "/" + FILE_KEY_S3

with fs.open(FILE_PATH_S3, mode="rb") as file_in:
    df_bpe = pd.read_csv(file_in, sep=";")
```

#### mc

To copy data from a MinIO bucket to the local service:

```bash
mc cp s3/donnees-insee/diffusion/BPE/2019/BPE_ENS.csv ./BPE_ENS.csv
```

::: {.callout-warning}
**Copying files to the local service is generally not a good practice**: it limits the reproducibility of analyses and becomes quickly impossible with large volumes of data. Therefore, it is preferable to get into the habit of importing data directly into `R`/`Python`.
:::

:::

### Exporting data to MinIO

::: {.panel-tabset}

#### R

```r
BUCKET_OUT = "<my_bucket>"
FILE_KEY_OUT_S3 = "my_folder/BPE_ENS.csv"

aws.s3::s3write_using(
    df,
    FUN = readr::write_csv,
    object = FILE_KEY_OUT_S3,
    bucket = BUCKET_OUT,
    opts = list("region" = "")
)
```

#### Python

```python
BUCKET_OUT = "<my_bucket>"
FILE_KEY_OUT_S3 = "my_folder/BPE_ENS.csv"
FILE_PATH_OUT_S3 = BUCKET_OUT + "/" + FILE_KEY_OUT_S3

with fs.open(FILE_PATH_OUT_S3, 'w') as file_out:
    df_bpe.to_csv(file_out)
```

#### mc

To copy data from the local service to a bucket on MinIO:

```bash
mc cp local/path/to/my/file.csv s3/<my_bucket>/remote/path/to/my/file.csv
```

:::

### Renewing expired access tokens

Access to MinIO storage is possible via a personal access token, which is valid for 7 days and automatically regenerated at regular intervals on SSP Cloud. When a token has expired, services created before the expiration date (using the previous token) can no longer access storage, and the affected service will appear in red on the [My Services](https://datalab.sspcloud.fr/my-services) page. In this case, there are two options:

- Open a new service on Datalab, which will have a default, up-to-date token.

- Manually replace expired tokens with new ones. Scripts indicating how to do this for different MinIO uses (`R`/`Python`/`mc`) are available [here](https://datalab.sspcloud.fr/account/storage). Simply choose the relevant script and execute it in your current working environment.

## Advanced Usage

### Creating a Service Account

For security reasons, the authentication to MinIO used by default in the interactive services of the SSP Cloud relies on a temporary access token. In the context of projects involving periodic processing or the deployment of applications, a more permanent access to MinIO data may be required.

In this case, a service account is used, which is an account tied to a specific project or application rather than an individual. Technically, instead of authenticating to MinIO via a triplet (access key id, secret access key, and session token), a pair (access key id, secret access key) will be used, granting read/write permissions to a specific project bucket.

The procedure for creating a service account is described below.

::: {.panel-tabset}

#### Graphical Interface

- Open the [MinIO console](https://minio-console.lab.sspcloud.fr)

- Open the `Access Keys` tab

- The service account information is pre-generated. It is possible to modify the access key to give it a simpler name.

- The `policy` specifying the rights is also pre-generated. Ideally, the policy should be restricted to only cover the project bucket(s).

- Once the service account is generated, the access key and secret access key can be used to authenticate the services/applications to the specified bucket.

#### Terminal (mc)

- Create a service on the SSP Cloud with up-to-date MinIO access. Confirm that the connection works with:

```bash
mc ls s3/<username>
```

- Generate a `policy.json` file with the following content, replacing `project-<my_project>` with the name of the relevant bucket (twice):

```json
{
    "Version": "2012-10-17",
    "Statement": [
     {
      "Effect": "Allow",
      "Action": [
       "s3:*"
      ],
      "Resource": [
       "arn:aws:s3:::projet-<my_project>",
       "arn:aws:s3:::projet-<my_project>/*"
      ]
     }
    ]
  }
```

- In a terminal, generate the service account with the following command:

```bash
mc admin user svcacct add s3 $AWS_ACCESS_KEY_ID --access-key="<access-key>" --secret-key="<secret-key>" --policy="policy.json"
```

replacing `<access-key>` and `<secret-key>` with names of your choice. Ideally, give a simple name for the access key (e.g., `sa-project-projectname`) but a complex key for the secret access key, which can be generated, for example, with the `gpg` client:

```bash
gpg --gen-random --armor 1 16
```

- You can now use the access key and secret access key to authenticate the services/applications to the specified bucket

:::

::: {.callout-warning}
Note that the generated authentication information appears only once. They can then be stored in a password manager, a secret storage service like Vault, or via the Onyxia project settings feature, which allows importing the service account directly into services at the time of their configuration.
:::

:::
